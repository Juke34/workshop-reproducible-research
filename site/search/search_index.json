{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"home/","text":"Tools for Reproducible Research # Course overview # GitHub repository 6 - 9 Juin, 2023 One of the key principles of proper scientific procedure is the act of repeating an experiment or analysis and being able to reach similar conclusions. Published research based on computational analysis ( e.g. bioinformatics or computational biology) have often suffered from incomplete method descriptions ( e.g. list of used software versions); unavailable raw data; and incomplete, undocumented and/or unavailable code. This essentially prevents any possibility of reproducing the results of such studies. The term \u201creproducible research\u201d has been used to describe the idea that a scientific publication should be distributed along with all the raw data and metadata used in the study, all the code and/or computational notebooks needed to produce results from the raw data, and the computational environment or a complete description thereof. Reproducible research not only leads to proper scientific conduct, but also enables other researchers to build upon previous work. Most importantly, the person who organizes their work with reproducibility in mind will quickly realize the immediate personal benefits: an organized and structured way of working. The person that most often has to reproduce your own analysis is your future self! Course content and learning outcomes # The following topics and tools are covered in the course: Data management Project organisation Git Conda Snakemake Nextflow R Markdown Jupyter Docker Singularity At the end of the course, students should be able to: Use good practices for data analysis and management Clearly organise their bioinformatic projects Use the version control system Git to track and collaborate on code Use the package and environment manager Conda Use and develop workflows with Snakemake and Nextflow Use R Markdown and Jupyter Notebooks to document and generate automated reports for their analyses Use Docker and Singularity to distribute containerized computational environments Application # This is an NBIS / Elixir course. The course is open for PhD students, postdocs, group leaders and core facility staff. International applications are welcome, but we will give approximately half of the participant slots to applicants from Swedish universities, due to the national role NBIS plays in Sweden. This online training event has no fee. However, if you confirm your participation but do not do so (no-show) you will be invoiced 2,000 SEK. Please note that NBIS cannot invoice individuals. The only entry requirements for this course is a basic knowledge of Unix systems ( i.e. being able to work on the command line) as well as at least a basic knowledge of either R or Python. Due to limited space the course can accommodate maximum of 20 participants. If we receive more applications, participants will be selected based on several criteria. Selection criteria include correct entry requirements, motivation to attend the course as well as gender and geographical balance. Please note that NBIS training events do not provide any formal university credits. The training content is estimated to correspond to a certain number of credits, however the estimated credits are just guidelines. If formal credits are crucial, the student needs to confer with the home department before submitting a course application in order to establish whether the course is valid for formal credits or not. By accepting to participate in the course, you agree to follow the NBIS Training Code of Conduct . Schedule # You can find the course schedule at this page . Location # This course round is given on site. Course material # The pre-course setup page lists all the information you need before the course starts. The most important part is the installation and setup of all the tools used in the course, so make sure you've gone through it all for the course start. You can find the tutorials themselves ( i.e. the content we will go through during the course) in the modules page. All of the lectures used in this course is available under the lectures/ directory on GitHub. Teachers # Jacques Dainat (course responsible) Thomas Denecker (course responsible) Aurore Comte (teacher) Julie ORJUELA (teacher) Contact # To contact us, please send a mail to the follow address: xxx@xxx.fr .","title":"Home"},{"location":"home/#tools-for-reproducible-research","text":"","title":"Tools for Reproducible Research"},{"location":"home/#course-overview","text":"GitHub repository 6 - 9 Juin, 2023 One of the key principles of proper scientific procedure is the act of repeating an experiment or analysis and being able to reach similar conclusions. Published research based on computational analysis ( e.g. bioinformatics or computational biology) have often suffered from incomplete method descriptions ( e.g. list of used software versions); unavailable raw data; and incomplete, undocumented and/or unavailable code. This essentially prevents any possibility of reproducing the results of such studies. The term \u201creproducible research\u201d has been used to describe the idea that a scientific publication should be distributed along with all the raw data and metadata used in the study, all the code and/or computational notebooks needed to produce results from the raw data, and the computational environment or a complete description thereof. Reproducible research not only leads to proper scientific conduct, but also enables other researchers to build upon previous work. Most importantly, the person who organizes their work with reproducibility in mind will quickly realize the immediate personal benefits: an organized and structured way of working. The person that most often has to reproduce your own analysis is your future self!","title":"Course overview"},{"location":"home/#course-content-and-learning-outcomes","text":"The following topics and tools are covered in the course: Data management Project organisation Git Conda Snakemake Nextflow R Markdown Jupyter Docker Singularity At the end of the course, students should be able to: Use good practices for data analysis and management Clearly organise their bioinformatic projects Use the version control system Git to track and collaborate on code Use the package and environment manager Conda Use and develop workflows with Snakemake and Nextflow Use R Markdown and Jupyter Notebooks to document and generate automated reports for their analyses Use Docker and Singularity to distribute containerized computational environments","title":"Course content and learning outcomes"},{"location":"home/#application","text":"This is an NBIS / Elixir course. The course is open for PhD students, postdocs, group leaders and core facility staff. International applications are welcome, but we will give approximately half of the participant slots to applicants from Swedish universities, due to the national role NBIS plays in Sweden. This online training event has no fee. However, if you confirm your participation but do not do so (no-show) you will be invoiced 2,000 SEK. Please note that NBIS cannot invoice individuals. The only entry requirements for this course is a basic knowledge of Unix systems ( i.e. being able to work on the command line) as well as at least a basic knowledge of either R or Python. Due to limited space the course can accommodate maximum of 20 participants. If we receive more applications, participants will be selected based on several criteria. Selection criteria include correct entry requirements, motivation to attend the course as well as gender and geographical balance. Please note that NBIS training events do not provide any formal university credits. The training content is estimated to correspond to a certain number of credits, however the estimated credits are just guidelines. If formal credits are crucial, the student needs to confer with the home department before submitting a course application in order to establish whether the course is valid for formal credits or not. By accepting to participate in the course, you agree to follow the NBIS Training Code of Conduct .","title":"Application"},{"location":"home/#schedule","text":"You can find the course schedule at this page .","title":"Schedule"},{"location":"home/#location","text":"This course round is given on site.","title":"Location"},{"location":"home/#course-material","text":"The pre-course setup page lists all the information you need before the course starts. The most important part is the installation and setup of all the tools used in the course, so make sure you've gone through it all for the course start. You can find the tutorials themselves ( i.e. the content we will go through during the course) in the modules page. All of the lectures used in this course is available under the lectures/ directory on GitHub.","title":"Course material"},{"location":"home/#teachers","text":"Jacques Dainat (course responsible) Thomas Denecker (course responsible) Aurore Comte (teacher) Julie ORJUELA (teacher)","title":"Teachers"},{"location":"home/#contact","text":"To contact us, please send a mail to the follow address: xxx@xxx.fr .","title":"Contact"},{"location":"take_down/","text":"Take down # Depending on which of the tutorials you have taken, there might be quite a lot of files stored on your computer. Here are instructions for how to remove them. All the tutorials depend on you cloning the workshop-reproducible-research GitHub repo. This can be removed like any other directory; via Finder, Explorer or rm -rf workshop-reproducible-research . Note that this will also delete the hidden directories .git , which contains the history of the repo, and .snakemake , which contains the history of any Snakemake runs. Conda # Several of the tutorials use Conda for installing packages. This amounts to about 2.6 GB if you've done all the tutorials. If you plan on using Conda in the future you can remove just the packages, or you can remove everything including Conda itself. Note that this is not needed if you've done the tutorials on Windows using Docker (see the section on Docker below instead). In order to remove all your Conda environments, you first need to list them: conda env list For each of the environments except \"base\" run the following: conda remove -n envname --all And, finally: conda clean --all If you also want to remove Conda itself ( i.e. removing all traces of Conda), you need to check where Conda is installed. Look for the row \"base environment\". conda info This should say something like /Users/<user>/miniconda3 . Then remove the entire Conda directory: rm -rf /Users/<user>/miniconda3 Lastly, open your ~/.bashrc file (or ~/.bash_profile if on Mac) in a text editor and remove the path to Conda from PATH. Snakemake # Snakemake is installed via Conda and will be removed if you follow the instructions in the Conda section above. Note that Snakemake also generates a hidden .snakemake directory in the directory where it's run. You can remove this with the following: rm -rf workshop-reproducible-research/snakemake/.snakemake Jupyter # Jupyter is installed via Conda and will be removed if you follow the instructions in the Conda section above. Docker # If you've done the Docker tutorial or if you've been running Docker for Windows you have some cleaning up to do. Docker is infamous for quickly taking up huge amounts of space, and some maintenance is necessary every now and then. Here is how to uninstall Docker completely. For instructions for how to remove individual images or containers, see the Docker tutorial . macOS # Click the Docker icon in the menu bar (upper right part of the screen) and select \"Preferences\". In the upper right corner, you should find a little bug icon. Click on that icon and select \"Reset to factory defaults\". You may have to fill in your password. Then select \"Uninstall\". Once it's done uninstalling, drag the Docker app from Applications to Trash. Linux # If you've installed Docker with apt-get , uninstall it like this: apt-get purge docker-ce Images, containers, and volumes are not automatically removed. To delete all of them: rm -rf /var/lib/docker Windows # Uninstall Docker for Windows (on Windows 10) or Docker Toolbox (on Windows 7) via Control Panel > Programs > Programs and Features. Docker Toolbox will also have installed Oracle VM VirtualBox, so uninstall that as well if you're not using it for other purposes. Singularity # Singularity images are files that can simply be deleted. Singularity also creates a hidden directory .singularity in your home directory that contains its cache, which you may delete. Windows # On Windows, you will additionally need to uninstall Git for Windows, VirtualBox, Vagrant and Vagrant Manager (see the Singularity installation guide ).","title":"Take down"},{"location":"take_down/#take-down","text":"Depending on which of the tutorials you have taken, there might be quite a lot of files stored on your computer. Here are instructions for how to remove them. All the tutorials depend on you cloning the workshop-reproducible-research GitHub repo. This can be removed like any other directory; via Finder, Explorer or rm -rf workshop-reproducible-research . Note that this will also delete the hidden directories .git , which contains the history of the repo, and .snakemake , which contains the history of any Snakemake runs.","title":"Take down"},{"location":"take_down/#conda","text":"Several of the tutorials use Conda for installing packages. This amounts to about 2.6 GB if you've done all the tutorials. If you plan on using Conda in the future you can remove just the packages, or you can remove everything including Conda itself. Note that this is not needed if you've done the tutorials on Windows using Docker (see the section on Docker below instead). In order to remove all your Conda environments, you first need to list them: conda env list For each of the environments except \"base\" run the following: conda remove -n envname --all And, finally: conda clean --all If you also want to remove Conda itself ( i.e. removing all traces of Conda), you need to check where Conda is installed. Look for the row \"base environment\". conda info This should say something like /Users/<user>/miniconda3 . Then remove the entire Conda directory: rm -rf /Users/<user>/miniconda3 Lastly, open your ~/.bashrc file (or ~/.bash_profile if on Mac) in a text editor and remove the path to Conda from PATH.","title":"Conda"},{"location":"take_down/#snakemake","text":"Snakemake is installed via Conda and will be removed if you follow the instructions in the Conda section above. Note that Snakemake also generates a hidden .snakemake directory in the directory where it's run. You can remove this with the following: rm -rf workshop-reproducible-research/snakemake/.snakemake","title":"Snakemake"},{"location":"take_down/#jupyter","text":"Jupyter is installed via Conda and will be removed if you follow the instructions in the Conda section above.","title":"Jupyter"},{"location":"take_down/#docker","text":"If you've done the Docker tutorial or if you've been running Docker for Windows you have some cleaning up to do. Docker is infamous for quickly taking up huge amounts of space, and some maintenance is necessary every now and then. Here is how to uninstall Docker completely. For instructions for how to remove individual images or containers, see the Docker tutorial .","title":"Docker"},{"location":"take_down/#macos","text":"Click the Docker icon in the menu bar (upper right part of the screen) and select \"Preferences\". In the upper right corner, you should find a little bug icon. Click on that icon and select \"Reset to factory defaults\". You may have to fill in your password. Then select \"Uninstall\". Once it's done uninstalling, drag the Docker app from Applications to Trash.","title":"macOS"},{"location":"take_down/#linux","text":"If you've installed Docker with apt-get , uninstall it like this: apt-get purge docker-ce Images, containers, and volumes are not automatically removed. To delete all of them: rm -rf /var/lib/docker","title":"Linux"},{"location":"take_down/#windows","text":"Uninstall Docker for Windows (on Windows 10) or Docker Toolbox (on Windows 7) via Control Panel > Programs > Programs and Features. Docker Toolbox will also have installed Oracle VM VirtualBox, so uninstall that as well if you're not using it for other purposes.","title":"Windows"},{"location":"take_down/#singularity","text":"Singularity images are files that can simply be deleted. Singularity also creates a hidden directory .singularity in your home directory that contains its cache, which you may delete.","title":"Singularity"},{"location":"take_down/#windows_1","text":"On Windows, you will additionally need to uninstall Git for Windows, VirtualBox, Vagrant and Vagrant Manager (see the Singularity installation guide ).","title":"Windows"},{"location":"conda/conda-1-introduction/","text":"Conda is a package and environment manager. As a package manager it enables you to install a wide range of software and tools using one simple command: conda install . As an environment manager it allows you to create and manage multiple different environments, each with their own set of packages. What are the benefits of using an environment manager? Some examples include the ability to easily run different versions of the same package, have different cross-package dependencies that are otherwise incompatible with each other and, last but not least, easy installation of all the software needed for an analysis. Environments are of particular relevance when making bioinformatics projects reproducible. Full reproducibility requires the ability to recreate the system that was originally used to generate the results. This can, to a large extent, be accomplished by using Conda to make a project environment with specific versions of the packages that are needed in the project. You can read more about Conda here . A Conda package is a compressed tarball (system-level libraries, Python or other modules, executable programs or other components). Conda keeps track of the dependencies between packages and platforms - this means that when installing a given package, all necessary dependencies will also be installed. Conda packages are typically hosted and downloaded from remote so-called channels . Some widely used channels for general-purpose and bioinformatics packages are conda-forge and Bioconda , respectively. Both of these are community-driven projects, so if you're missing some package you can contribute to the channel by adding the package to it. When installing a Conda package you specify the package name, version (optional) and channel to download from. A Conda environment is essentially a directory that is added to your PATH and that contains a specific collection of packages that you have installed. Packages are symlinked between environments to avoid unnecessary duplication. This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if, you haven't done so already. Then open up a terminal and go to workshop-reproducible-research/tutorials/conda . Instructions below assume that you are standing in workshop-reproducible-research/tutorials/conda/ unless otherwise specified ( e.g. if it says \"create a file\", it means save it in workshop-reproducible-research/tutorials/conda/ ).","title":"Introduction"},{"location":"conda/conda-2-the-basics/","text":"Let's assume that you are just about to start a new exciting research project called Project A . Creating Conda environments # Let's make our first Conda environment: conda create -n project_a -c bioconda fastqc This will create an environment called project_a , containing FastQC from the Bioconda channel. Conda will list the packages that will be installed and ask for your confirmation. Once it is done, you can activate the environment: conda activate project_a By default, Conda will add information to your prompt telling you which environment that is active. To see all your environments you can run: conda info --envs The active environment will be marked with an asterisk. To see the installed packages and their versions in the active environment, run: conda list To save the installed packages to a file, run: conda env export --from-history > environment.yml where --from-history only reports the packages requested to be installed and not additional dependancies. A caveat is that if no version was originally specified, then it is not included in the export file either. Now, deactivate the environment by running conda deactivate . List all environments again. Which environment is now marked as active? Try to run FastQC: fastqc --version Did it work? Activate your project_a environment and run the fastqc --version command again. Does it work now? Hopefully the FastQC software was not found in your base environment (unless you had installed it previously), but worked once your environment was activated. Adding more packages # Now, let's add another package ( SRA-Tools ) to our environment using conda install . Make sure that project_a is the active environment first. conda install -c bioconda sra-tools If we don't specify the package version, the latest available version will be installed. What version of SRA-Tools got installed? Run the following to see what versions are available: conda search -c bioconda sra-tools Now try to install a different version of SRA-Tools, e.g. : conda install -c bioconda sra-tools=2.7.0 Read the information that Conda displays in the terminal. It probably asks if you want to downgrade the initial SRA-Tools installation to the one specified here ( 2.7.0 in the example). You can only have one version of a given package in a given environment. Let's assume that you will have sequencing data in your Project A, and want to use the latest Bowtie2 software to align your reads. Find out what versions of Bowtie2 are available in the Bioconda channel using conda search -c bioconda . Now install the latest available version of Bowtie2 in your project_a environment. Let's further assume that you have an old project (called Project Old ) where you know you used Bowtie2 2.2.5 . You just got back reviewer comments and they want you to include some alignment statistics. Unfortunately, you haven't saved that information so you will have to rerun the alignment. Now, it is essential that you use the same version of Bowtie that your results are based on, otherwise the alignment statistics will be misleading. Using Conda environments this becomes simple. You can just have a separate environment for your old project where you have an old version of Bowtie2 without interfering with your new Project A where you want the latest version. Make a new environment for your old project: conda create -n project_old -c bioconda bowtie2=2.2.5 List your environments (do you remember the command?). Activate project_old and check the Bowtie2 version ( bowtie2 --version ). Activate project_a again and check the Bowtie2 version. Removing packages # Now let's try to remove an installed package from the active environment: conda remove sra-tools Run conda deactivate to exit your active environment. Now, let's remove an environment: conda env remove -n project_old After making a few different environments and installing a bunch of packages, Conda can take up some disk space. You can remove unnecessary files with the command: conda clean -a This will remove package tar-balls that are left from package installations, unused packages ( i.e. those not present in any environments), and cached data. Quick recap In this section we've learned: How to use conda install for installing packages on the fly. How to create, activate and change between environments. How to remove packages or environments and clean up.","title":"The basics"},{"location":"conda/conda-2-the-basics/#creating-conda-environments","text":"Let's make our first Conda environment: conda create -n project_a -c bioconda fastqc This will create an environment called project_a , containing FastQC from the Bioconda channel. Conda will list the packages that will be installed and ask for your confirmation. Once it is done, you can activate the environment: conda activate project_a By default, Conda will add information to your prompt telling you which environment that is active. To see all your environments you can run: conda info --envs The active environment will be marked with an asterisk. To see the installed packages and their versions in the active environment, run: conda list To save the installed packages to a file, run: conda env export --from-history > environment.yml where --from-history only reports the packages requested to be installed and not additional dependancies. A caveat is that if no version was originally specified, then it is not included in the export file either. Now, deactivate the environment by running conda deactivate . List all environments again. Which environment is now marked as active? Try to run FastQC: fastqc --version Did it work? Activate your project_a environment and run the fastqc --version command again. Does it work now? Hopefully the FastQC software was not found in your base environment (unless you had installed it previously), but worked once your environment was activated.","title":"Creating Conda environments"},{"location":"conda/conda-2-the-basics/#adding-more-packages","text":"Now, let's add another package ( SRA-Tools ) to our environment using conda install . Make sure that project_a is the active environment first. conda install -c bioconda sra-tools If we don't specify the package version, the latest available version will be installed. What version of SRA-Tools got installed? Run the following to see what versions are available: conda search -c bioconda sra-tools Now try to install a different version of SRA-Tools, e.g. : conda install -c bioconda sra-tools=2.7.0 Read the information that Conda displays in the terminal. It probably asks if you want to downgrade the initial SRA-Tools installation to the one specified here ( 2.7.0 in the example). You can only have one version of a given package in a given environment. Let's assume that you will have sequencing data in your Project A, and want to use the latest Bowtie2 software to align your reads. Find out what versions of Bowtie2 are available in the Bioconda channel using conda search -c bioconda . Now install the latest available version of Bowtie2 in your project_a environment. Let's further assume that you have an old project (called Project Old ) where you know you used Bowtie2 2.2.5 . You just got back reviewer comments and they want you to include some alignment statistics. Unfortunately, you haven't saved that information so you will have to rerun the alignment. Now, it is essential that you use the same version of Bowtie that your results are based on, otherwise the alignment statistics will be misleading. Using Conda environments this becomes simple. You can just have a separate environment for your old project where you have an old version of Bowtie2 without interfering with your new Project A where you want the latest version. Make a new environment for your old project: conda create -n project_old -c bioconda bowtie2=2.2.5 List your environments (do you remember the command?). Activate project_old and check the Bowtie2 version ( bowtie2 --version ). Activate project_a again and check the Bowtie2 version.","title":"Adding more packages"},{"location":"conda/conda-2-the-basics/#removing-packages","text":"Now let's try to remove an installed package from the active environment: conda remove sra-tools Run conda deactivate to exit your active environment. Now, let's remove an environment: conda env remove -n project_old After making a few different environments and installing a bunch of packages, Conda can take up some disk space. You can remove unnecessary files with the command: conda clean -a This will remove package tar-balls that are left from package installations, unused packages ( i.e. those not present in any environments), and cached data. Quick recap In this section we've learned: How to use conda install for installing packages on the fly. How to create, activate and change between environments. How to remove packages or environments and clean up.","title":"Removing packages"},{"location":"conda/conda-3-projects/","text":"We have up until now specified which Conda packages to install directly on the command line using the conda create and conda install commands. For working in projects this is not the recommended way. Instead, for increased control and reproducibility, it is better to use an environment file (in YAML format ) that specifies the packages, versions and channels needed to create the environment for a project. Throughout these tutorials we will use a case study where we analyze an RNA-seq experiment with the multiresistant bacteria MRSA (see intro ). You will now start to make a Conda YAML file for this MRSA project. The file will contain a list of the software and versions needed to execute the analysis code. In this Conda tutorial, all code for the analysis is available in the script code/run_qc.sh . This code will download the raw FASTQ-files and subsequently run quality control on these using the FastQC software. Working with environments # We will start by making a Conda YAML-file that contains the required packages to perform these two steps. Later in the course, you will update the Conda YAML-file with more packages, as the analysis workflow is expanded. Let's get going! Make a YAML file called environment.yml looking like this, and save it in the current directory (which should be workshop-reproducible-research/tutorials/conda ): channels: - conda-forge - bioconda dependencies: - fastqc=0.11.9 - sra-tools=2.11.0 Now, make a new Conda environment from the YAML file (note that here the command is conda env create as opposed to conda create that we used above): conda env create -n project_mrsa -f environment.yml Tip You can also specify exactly which channel a package should come from inside the environment file, using the channel::package=version syntax. Tip Instead of the -n flag you can use the -p flag to set the full path to where the Conda environment should be installed. In that way you can contain the Conda environment inside the project directory, which does make sense from a reproducibility perspective, and makes it easier to keep track of what environment belongs to what project. If you don't specify -p the environment will be installed in the default miniconda3/envs/ directory. Activate the environment! Now we can run the code for the MRSA project found in code/run_qc.sh , either by running bash code/run_qc.sh or by opening the run_qc.sh file and executing each line in the terminal one by one. Do this! This should download the project FASTQ files and run FastQC on them (as mentioned above). Check your directory contents ( ls -Rlh , or in your file browser). It should now have the following structure: conda/ | |- code/ | |- run_qc.sh | |- data/ | |- raw_internal/ | |- SRR935090.fastq.gz | |- SRR935091.fastq.gz | |- SRR935092.fastq.gz | |- intermediate/ | |- fastqc/ | |- SRR935090_fastqc.zip | |- SRR935091_fastqc.zip | |- SRR935092_fastqc.zip | |- results/ | |- fastqc/ | |- SRR935090_fastqc.html | |- SRR935091_fastqc.html | |- SRR935092_fastqc.html | |- environment.yml Note that all that was needed to carry out the analysis and generate these files and results was environment.yml (that we used to create a Conda environment with the required packages) and the analysis code in code/run_qc.sh . Keeping track of dependencies # Projects can often be quite large and require lots of dependencies; it can feel daunting to try to capture all of that in a single Conda environment, especially when you consider potential incompatibilities that may arise. It can therefore be a good idea to start new projects with an environment file with each package you know that you will need to use, but without specifying exact versions (except for those packages where you know you need a specific version). Conda will then try to get the latest compatible versions of all the specified software, making the start-up and installation part of new projects easier. You can then add the versions that were installed to your environment file afterwards, ensuring future reproducibility. There is one command that can make this easier: conda env export . This allows you to export a list of the packages you've already installed, including their specific versions, meaning you can easily add them after the fact to your environment file. If you use the --no-builds flag, you'll get a list of the packages minus their OS-specific build specifications, which is more useful for making the environment portable across systems. This way, you can start with an environment file with just the packages you need (without version), allow Conda to solve the dependency tree and install the most up-to-date version possible, and then add the resulting version back in to the environment file using the export command! Optimising for speed # One of the greatest strengths of Conda is, unfortunately, also its greatest weakness in its current implementation: the availability of a frankly enormous number of packages and versions. This means that the search space for the dependency hierarchy of any given Conda environment can become equally enormous, leading to a (at times) ridiculous execution time for the dependency solver. It is not uncommon to find yourself waiting for minutes for Conda to solve a dependency hierarchy, sometimes even into the double digits. How can this be circumvented? Firstly, it is useful to specify as many of the major.minor.patch version numbers as possible when defining your environment: this drastically reduces the search space that Conda needs to go through. This is not always possible, though. For example, we mentioned in the end of the Environments in projects section that you might want to start out new projects without version specifications for most packages, which means that the search space is going to be large. Here is where another software comes into play: Mamba . The Mamba package manager is built on-top of Conda with some changes and additions that greatly speed up the execution time. First of all, core parts of Mamba are written in C++ instead of Python, like the original Conda. Secondly, it uses a different dependency solver algorithm which is much faster than the one Conda uses. Lastly, it allows for parallel downloading of repository data and package files with multi-threading. All in all, these changes mean that Mamba is (currently) simply a better version of Conda. Hopefully these changes will be incorporated into the Conda core at some point in the future! So, how do you get Mamba? Funnily enough, the easiest way to install it is (of course) using Conda! Just run conda install -n base -c conda-forge mamba , which will install Mamba in your base Conda environment. Mamba works almost exactly the same as Conda, meaning that all you need to do is to stop using conda command and instead use mamba command - simple! Be aware though that in order to use mamba activate and mamba deactivate you first need to run mamba init . So transitioning into using Mamba is actually quite easy - enjoy your shorter execution times! Quick recap In this section we've learned: How to define our Conda environment using a YAML-file. How to use conda env create to make a new environment from a YAML-file. How to use conda env export to get a list of installed packages. How to work with Conda in a project-like setting. How to optimise Conda for speed.","title":"Projects"},{"location":"conda/conda-3-projects/#working-with-environments","text":"We will start by making a Conda YAML-file that contains the required packages to perform these two steps. Later in the course, you will update the Conda YAML-file with more packages, as the analysis workflow is expanded. Let's get going! Make a YAML file called environment.yml looking like this, and save it in the current directory (which should be workshop-reproducible-research/tutorials/conda ): channels: - conda-forge - bioconda dependencies: - fastqc=0.11.9 - sra-tools=2.11.0 Now, make a new Conda environment from the YAML file (note that here the command is conda env create as opposed to conda create that we used above): conda env create -n project_mrsa -f environment.yml Tip You can also specify exactly which channel a package should come from inside the environment file, using the channel::package=version syntax. Tip Instead of the -n flag you can use the -p flag to set the full path to where the Conda environment should be installed. In that way you can contain the Conda environment inside the project directory, which does make sense from a reproducibility perspective, and makes it easier to keep track of what environment belongs to what project. If you don't specify -p the environment will be installed in the default miniconda3/envs/ directory. Activate the environment! Now we can run the code for the MRSA project found in code/run_qc.sh , either by running bash code/run_qc.sh or by opening the run_qc.sh file and executing each line in the terminal one by one. Do this! This should download the project FASTQ files and run FastQC on them (as mentioned above). Check your directory contents ( ls -Rlh , or in your file browser). It should now have the following structure: conda/ | |- code/ | |- run_qc.sh | |- data/ | |- raw_internal/ | |- SRR935090.fastq.gz | |- SRR935091.fastq.gz | |- SRR935092.fastq.gz | |- intermediate/ | |- fastqc/ | |- SRR935090_fastqc.zip | |- SRR935091_fastqc.zip | |- SRR935092_fastqc.zip | |- results/ | |- fastqc/ | |- SRR935090_fastqc.html | |- SRR935091_fastqc.html | |- SRR935092_fastqc.html | |- environment.yml Note that all that was needed to carry out the analysis and generate these files and results was environment.yml (that we used to create a Conda environment with the required packages) and the analysis code in code/run_qc.sh .","title":"Working with environments"},{"location":"conda/conda-3-projects/#keeping-track-of-dependencies","text":"Projects can often be quite large and require lots of dependencies; it can feel daunting to try to capture all of that in a single Conda environment, especially when you consider potential incompatibilities that may arise. It can therefore be a good idea to start new projects with an environment file with each package you know that you will need to use, but without specifying exact versions (except for those packages where you know you need a specific version). Conda will then try to get the latest compatible versions of all the specified software, making the start-up and installation part of new projects easier. You can then add the versions that were installed to your environment file afterwards, ensuring future reproducibility. There is one command that can make this easier: conda env export . This allows you to export a list of the packages you've already installed, including their specific versions, meaning you can easily add them after the fact to your environment file. If you use the --no-builds flag, you'll get a list of the packages minus their OS-specific build specifications, which is more useful for making the environment portable across systems. This way, you can start with an environment file with just the packages you need (without version), allow Conda to solve the dependency tree and install the most up-to-date version possible, and then add the resulting version back in to the environment file using the export command!","title":"Keeping track of dependencies"},{"location":"conda/conda-3-projects/#optimising-for-speed","text":"One of the greatest strengths of Conda is, unfortunately, also its greatest weakness in its current implementation: the availability of a frankly enormous number of packages and versions. This means that the search space for the dependency hierarchy of any given Conda environment can become equally enormous, leading to a (at times) ridiculous execution time for the dependency solver. It is not uncommon to find yourself waiting for minutes for Conda to solve a dependency hierarchy, sometimes even into the double digits. How can this be circumvented? Firstly, it is useful to specify as many of the major.minor.patch version numbers as possible when defining your environment: this drastically reduces the search space that Conda needs to go through. This is not always possible, though. For example, we mentioned in the end of the Environments in projects section that you might want to start out new projects without version specifications for most packages, which means that the search space is going to be large. Here is where another software comes into play: Mamba . The Mamba package manager is built on-top of Conda with some changes and additions that greatly speed up the execution time. First of all, core parts of Mamba are written in C++ instead of Python, like the original Conda. Secondly, it uses a different dependency solver algorithm which is much faster than the one Conda uses. Lastly, it allows for parallel downloading of repository data and package files with multi-threading. All in all, these changes mean that Mamba is (currently) simply a better version of Conda. Hopefully these changes will be incorporated into the Conda core at some point in the future! So, how do you get Mamba? Funnily enough, the easiest way to install it is (of course) using Conda! Just run conda install -n base -c conda-forge mamba , which will install Mamba in your base Conda environment. Mamba works almost exactly the same as Conda, meaning that all you need to do is to stop using conda command and instead use mamba command - simple! Be aware though that in order to use mamba activate and mamba deactivate you first need to run mamba init . So transitioning into using Mamba is actually quite easy - enjoy your shorter execution times! Quick recap In this section we've learned: How to define our Conda environment using a YAML-file. How to use conda env create to make a new environment from a YAML-file. How to use conda env export to get a list of installed packages. How to work with Conda in a project-like setting. How to optimise Conda for speed.","title":"Optimising for speed"},{"location":"conda/conda-4-extra-material/","text":"The following extra material contains some more advanced things you can do with Conda and the command line in general, which is not part of the main course materials. All the essential skills of Conda are covered by the previous section: the material here should be considered tips and tricks from people who use Conda as part of their daily work. You thus don't need to use these things unless you want to, and you can even skip this part of the lesson if you like! Configuring Conda # The behaviour of your Conda installation can be changed using an optional configuration file .condarc . On a fresh Conda install no such file is included but it's created in your home directory as ~/.condarc the first time you run conda config . You can edit the .condarc file either using a text editor or by way of the conda config command. To list all config parameters and their settings run: conda config --show Similar to Conda environment files, the configuration file is in YAML syntax. This means that the config file is structured in the form of key:value pairs where the key is the name of the config parameter ( e.g. auto_update_conda ) and the value is the parameter setting ( e.g. True ). Adding the name of a config parameter to conda config --show will show only that parameter, e.g. conda config --show channels . You can change parameters with the --set , --add , --append and --remove flags to conda config . If you for example want to enable the 'Always yes' behaviour which makes Conda automatically choose the yes option, such as when installing, you can run: conda config --set always_yes True To see details about a config parameter you can run conda config --describe parameter . Try running it on the channels parameter: conda config --describe channels In the beginning of this tutorial we added Conda channels to the .condarc file using conda config --add channels . To remove one of the channels from the configuration file you can run: conda config --remove channels conda-forge Check your .condarc file to see the change. To add the conda-forge channel back to the top of the channels simply run: conda config --add channels conda-forge To completely remove a parameter and all its values run: conda config --remove-key parameter For a list of Conda configuration parameters see the Conda configuration page. Managing Python versions # With Conda it's possible to keep several different versions of Python on your computer at the same time, and switching between these versions is very easy. However, a single Conda environment can only contain one version of Python. Your current Python installation # The Conda base environment has its own version of Python installed. When you open a terminal (after having installed Conda on your system) this base environment is activated by default (as evidenced by (base) prepended to your prompt). You can check what Python version is installed in this environment by running python --version . To see the exact path to the Python executable type which python . In addition to this your computer may already have Python installed in a separate (system-wide) location outside of the Conda installation. To see if that is the case type conda deactivate until your prompt is not prepended with a Conda environment name. Then type which python . If a path was printed to the terminal ( e.g. /usr/bin/python ) that means some Python version is already installed in that location. Check what version it is by typing python --version . Now activate the base Conda environment again by typing conda activate (or the equivalent conda activate base ) then check the Python installation path and version using which and python --version as above. See the difference? When you activate a Conda environment your $PATH variable is updated so that when you call python (or any other program) the system first searches the directory of the currently active environment. Different Python versions # When you create a new Conda environment you can choose to install a specific version of Python in that environment as well. As an example, create an environment containing Python version 3.5 by running: conda create -n py35 python=3.5 Here we name the environment py35 but you can choose whatever name you want. To activate the environment run: conda activate py35 You now have a completely separate environment with its own Python version. Let's say you instead want an environment with Python version 2.7 installed. You may for instance want to run scripts or packages that were written for Python 2.x and are thus incompatible with Python 3.x. Simply create the new Conda environment with: conda create -n py27 python=2.7 Activate this environment with: conda activate py27 Now, switching between Python versions is as easy as typing conda activate py35 / conda activate py27 . Note If you create an environment where none of the packages require Python, and you don't explicitly install the python package then that new environment will use the Python version installed in your base Conda environment. Decorating your prompt # By default, Conda adds the name of the currently activated environment to the end of your command line prompt. This is a good thing, as it makes it easier to keep track of what environment and packages you have access to. The way this is done in the default implementation becomes an issue when using absolute paths for environments (specifying conda env create -p path/to/environment , though, as the entire path will be added to the prompt. This can take up a lot of unnecessary space, but can be solved in a number of ways. The most straightforward way to solve this is to change the Conda configuration file, specifically the settings of the env_prompt configuration value which determines how Conda modifies your command line prompt. For more information about this setting you can run conda config --describe env_prompt and to see your current setting you can run conda config --show env_prompt . By default env_prompt is set to ({default_env}) which modifies your prompt with the active environment name if it was installed using the -n flag or if the environment folder has a parent folder named envs/ . Otherwise the full environment path ( i.e. the 'prefix') is displayed. If you instead set env_prompt to ({name}) Conda will modify your prompt with the folder name of the active environment. You can change the setting by running conda config --set env_prompt '({name}) ' If you wish to keep the ({default_env}) behaviour, or just don't want to change your Conda config, an alternative is to keep Conda environment folders within a parent folder called envs/ . This will make Conda only add the folder name of the Conda environment to your prompt when you activate it. As an example, say you have a project called project_a with the project path ~/myprojects/project_a . You could then install the environment for project_a into a folder ~/myprojects/project_a/envs/project_a_environment . Activating the environment by pointing Conda to it ( e.g. conda activate ~/myprojects/project_a/envs/project_a_environment ) will only cause your prompt to be modified with project_a_environment . Bash aliases for conda # Some programmers like to have aliases ( i.e. shortcuts) for common commands. Two aliases that might be usefol for you are alias coac='conda activate' and alias code='conda deactivate' . Don't forget to add them to your ~/.bash_profile if you want to use them! Rolling back to an earlier version of the environment # Conda keeps a history of the changes to an environment. You can see revisions to an environment by using: conda list --revisions which shows each revision (numbered) and what's installed. You can revert back to particular revision using: conda install --revision 5","title":"Extra material"},{"location":"conda/conda-4-extra-material/#configuring-conda","text":"The behaviour of your Conda installation can be changed using an optional configuration file .condarc . On a fresh Conda install no such file is included but it's created in your home directory as ~/.condarc the first time you run conda config . You can edit the .condarc file either using a text editor or by way of the conda config command. To list all config parameters and their settings run: conda config --show Similar to Conda environment files, the configuration file is in YAML syntax. This means that the config file is structured in the form of key:value pairs where the key is the name of the config parameter ( e.g. auto_update_conda ) and the value is the parameter setting ( e.g. True ). Adding the name of a config parameter to conda config --show will show only that parameter, e.g. conda config --show channels . You can change parameters with the --set , --add , --append and --remove flags to conda config . If you for example want to enable the 'Always yes' behaviour which makes Conda automatically choose the yes option, such as when installing, you can run: conda config --set always_yes True To see details about a config parameter you can run conda config --describe parameter . Try running it on the channels parameter: conda config --describe channels In the beginning of this tutorial we added Conda channels to the .condarc file using conda config --add channels . To remove one of the channels from the configuration file you can run: conda config --remove channels conda-forge Check your .condarc file to see the change. To add the conda-forge channel back to the top of the channels simply run: conda config --add channels conda-forge To completely remove a parameter and all its values run: conda config --remove-key parameter For a list of Conda configuration parameters see the Conda configuration page.","title":"Configuring Conda"},{"location":"conda/conda-4-extra-material/#managing-python-versions","text":"With Conda it's possible to keep several different versions of Python on your computer at the same time, and switching between these versions is very easy. However, a single Conda environment can only contain one version of Python.","title":"Managing Python versions"},{"location":"conda/conda-4-extra-material/#your-current-python-installation","text":"The Conda base environment has its own version of Python installed. When you open a terminal (after having installed Conda on your system) this base environment is activated by default (as evidenced by (base) prepended to your prompt). You can check what Python version is installed in this environment by running python --version . To see the exact path to the Python executable type which python . In addition to this your computer may already have Python installed in a separate (system-wide) location outside of the Conda installation. To see if that is the case type conda deactivate until your prompt is not prepended with a Conda environment name. Then type which python . If a path was printed to the terminal ( e.g. /usr/bin/python ) that means some Python version is already installed in that location. Check what version it is by typing python --version . Now activate the base Conda environment again by typing conda activate (or the equivalent conda activate base ) then check the Python installation path and version using which and python --version as above. See the difference? When you activate a Conda environment your $PATH variable is updated so that when you call python (or any other program) the system first searches the directory of the currently active environment.","title":"Your current Python installation"},{"location":"conda/conda-4-extra-material/#different-python-versions","text":"When you create a new Conda environment you can choose to install a specific version of Python in that environment as well. As an example, create an environment containing Python version 3.5 by running: conda create -n py35 python=3.5 Here we name the environment py35 but you can choose whatever name you want. To activate the environment run: conda activate py35 You now have a completely separate environment with its own Python version. Let's say you instead want an environment with Python version 2.7 installed. You may for instance want to run scripts or packages that were written for Python 2.x and are thus incompatible with Python 3.x. Simply create the new Conda environment with: conda create -n py27 python=2.7 Activate this environment with: conda activate py27 Now, switching between Python versions is as easy as typing conda activate py35 / conda activate py27 . Note If you create an environment where none of the packages require Python, and you don't explicitly install the python package then that new environment will use the Python version installed in your base Conda environment.","title":"Different Python versions"},{"location":"conda/conda-4-extra-material/#decorating-your-prompt","text":"By default, Conda adds the name of the currently activated environment to the end of your command line prompt. This is a good thing, as it makes it easier to keep track of what environment and packages you have access to. The way this is done in the default implementation becomes an issue when using absolute paths for environments (specifying conda env create -p path/to/environment , though, as the entire path will be added to the prompt. This can take up a lot of unnecessary space, but can be solved in a number of ways. The most straightforward way to solve this is to change the Conda configuration file, specifically the settings of the env_prompt configuration value which determines how Conda modifies your command line prompt. For more information about this setting you can run conda config --describe env_prompt and to see your current setting you can run conda config --show env_prompt . By default env_prompt is set to ({default_env}) which modifies your prompt with the active environment name if it was installed using the -n flag or if the environment folder has a parent folder named envs/ . Otherwise the full environment path ( i.e. the 'prefix') is displayed. If you instead set env_prompt to ({name}) Conda will modify your prompt with the folder name of the active environment. You can change the setting by running conda config --set env_prompt '({name}) ' If you wish to keep the ({default_env}) behaviour, or just don't want to change your Conda config, an alternative is to keep Conda environment folders within a parent folder called envs/ . This will make Conda only add the folder name of the Conda environment to your prompt when you activate it. As an example, say you have a project called project_a with the project path ~/myprojects/project_a . You could then install the environment for project_a into a folder ~/myprojects/project_a/envs/project_a_environment . Activating the environment by pointing Conda to it ( e.g. conda activate ~/myprojects/project_a/envs/project_a_environment ) will only cause your prompt to be modified with project_a_environment .","title":"Decorating your prompt"},{"location":"conda/conda-4-extra-material/#bash-aliases-for-conda","text":"Some programmers like to have aliases ( i.e. shortcuts) for common commands. Two aliases that might be usefol for you are alias coac='conda activate' and alias code='conda deactivate' . Don't forget to add them to your ~/.bash_profile if you want to use them!","title":"Bash aliases for conda"},{"location":"conda/conda-4-extra-material/#rolling-back-to-an-earlier-version-of-the-environment","text":"Conda keeps a history of the changes to an environment. You can see revisions to an environment by using: conda list --revisions which shows each revision (numbered) and what's installed. You can revert back to particular revision using: conda install --revision 5","title":"Rolling back to an earlier version of the environment"},{"location":"containers/containers-1-introduction/","text":"Container-based technologies are designed to make it easier to create, deploy, and run applications by isolating them in self-contained software units (hence their name). The idea is to package software and/or code together with everything it needs (other packages it depends, various environment settings, etc. ) into one unit, i.e. a container. This way we can ensure that the software or code functions in exactly the same way regardless of where it's executed. Containers are in many ways similar to virtual machines but more lightweight. Rather than starting up a whole new operating system, containers can use the same kernel (usually Linux) as the system that they're running on. This makes them much faster and smaller compared to virtual machines. While this might sound a bit technical, actually using containers is quite smooth and very powerful. Containers have also proven to be a very good solution for packaging, running and distributing scientific data analyses. Some applications of containers relevant for reproducible research are: When publishing, package your analyses in a container image and let it accompany the article. This way interested readers can reproduce your analysis at the push of a button. Packaging your analysis in a container enables you to develop on e.g. your laptop and seamlessly move to cluster or cloud to run the actual analysis. Say that you are collaborating on a project and you are using Mac while your collaborator is using Windows. You can then set up a container image specific for your project to ensure that you are working in an identical environment. One of the largest and most widely used container-based technologies is Docker . Just as with Git, Docker was designed for software development but is rapidly becoming widely used in scientific research. Another container-based technology is Singularity , which was developed to work well in computer cluster environments, such as Uppmax. We will cover both Docker and Singularity in this course, but the focus will be be on the former (since that is the most widely used and runs on all three operating systems). This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to install Docker if you haven't done so already, then open up a terminal and go to workshop-reproducible-research/tutorials/containers . Attention! Docker images tend to take up quite a lot of space. In order to do all the exercises in this tutorial you need to have ~10 GB available.","title":"Introduction"},{"location":"containers/containers-2-the-basics/","text":"We're almost ready to start, just one last note on nomenclature. You might have noticed that we sometimes refer to \"Docker images\" and sometimes to \"Docker containers\". A container is simply an instance of an image. To use a programming metaphor, if an image is a class, then a container is an instance of that class \u2014 a runtime object. You can have an image containing, say, a certain Linux distribution, and then start multiple containers running that same OS. Attention! If you don't have root privileges you have to prepend all Docker commands with sudo . Downloading containers # Docker containers typically run Linux, so let's start by downloading an image containing Ubuntu (a popular Linux distribution that is based on only open-source tools) through the command line. docker pull ubuntu:latest You will notice that it downloads different layers with weird hashes as names. This represents a very fundamental property of Docker images that we'll get back to in just a little while. The process should end with something along the lines of: Status: Downloaded newer image for ubuntu:latest docker.io/library/ubuntu:latest Let's take a look at our new and growing collection of Docker images: docker image ls The Ubuntu image show show up in this list, with something looking like this: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest d70eaf7277ea 3 weeks ago 72.9MB Running containers # We can now start a container running our image. We can refer to the image either by \"REPOSITORY:TAG\" (\"latest\" is the default so we can omit it) or \"IMAGE ID\". The syntax for docker run is docker run [OPTIONS] IMAGE [COMMAND] [ARG...] . Let's run the command uname -a to get some info about the operating system. First run on your own system (use systeminfo if you are on Windows): uname -a This should print something like this to your command line: Darwin liv433l.lan 15.6.0 Darwin Kernel Version 15.6.0: Mon Oct 2 22:20:08 PDT 2017; root:xnu-3248.71.4~1/RELEASE_X86_64 x86_64 Seems like I'm running the Darwin version of macOS. Then run it in the Ubuntu Docker container: docker run ubuntu uname -a Here I get the following result: Linux 24d063b5d877 5.4.39-linuxkit #1 SMP Fri May 8 23:03:06 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux And now I'm running on Linux! Try the same thing with whoami . Running interactively # So, seems we can execute arbitrary commands on Linux. Seems useful, but maybe a bit limited. We can also get an interactive terminal with the flags -it . docker run -it ubuntu This should put at a terminal prompt inside a container running Ubuntu. Your prompt should now look similar to: root@1f339e929fa9:/# Here you can do whatever; install, run, remove stuff. It will still be within the container and never affect your host system. Now exit the container with exit . Containers inside scripts # Okay, so Docker lets us work in any OS in a quite convenient way. That would probably be useful on its own, but Docker is much more powerful than that. For example, let's look at the shell part of the index_genome rule in the Snakemake workflow for the MRSA case study: shell: \"\"\" bowtie2-build tempfile intermediate/{wildcards.genome_id} > {log} \"\"\" You may have seen that one can use containers through both Snakemake and Nextflow if you've gone through their tutorial's extra material, but we can also use containers directly inside scripts in a very simple way. Let's imagine we want to run the above command using containers instead. How would that look? It's quite simple, really: first we find a container image that has bowtie2 installed, and then prepend the command with docker run <image> . First of all we need to download the genome to index though, so run: curl -o NCTC8325.fa.gz ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz gunzip -c NCTC8325.fa.gz > tempfile to download and prepare the input for bowtie2. Now try running the following Bash code: docker run -v $(pwd):/analysis quay.io/biocontainers/bowtie2:2.5.0--py310h8d7afc0_0 bowtie2-build /analysis/tempfile /analysis/NCTC8325 Docker will automatically download the container image and subsequently run the command! Here we're using -v $(pwd):/analysis to mount the current directory inside the container in order to make the tempfile input available to bowtie2. More on these so called \"Bind mounts\" in Section 4 of this tutorial. Quick recap In this section we've learned: How to use docker pull for downloading images from a central registry. How to use docker image ls for getting information about the images we have on our system. How to use docker run for starting a container from an image. How to use the -it flag for running in interactive mode. How to use Docker inside scripts.","title":"The basics"},{"location":"containers/containers-2-the-basics/#downloading-containers","text":"Docker containers typically run Linux, so let's start by downloading an image containing Ubuntu (a popular Linux distribution that is based on only open-source tools) through the command line. docker pull ubuntu:latest You will notice that it downloads different layers with weird hashes as names. This represents a very fundamental property of Docker images that we'll get back to in just a little while. The process should end with something along the lines of: Status: Downloaded newer image for ubuntu:latest docker.io/library/ubuntu:latest Let's take a look at our new and growing collection of Docker images: docker image ls The Ubuntu image show show up in this list, with something looking like this: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest d70eaf7277ea 3 weeks ago 72.9MB","title":"Downloading containers"},{"location":"containers/containers-2-the-basics/#running-containers","text":"We can now start a container running our image. We can refer to the image either by \"REPOSITORY:TAG\" (\"latest\" is the default so we can omit it) or \"IMAGE ID\". The syntax for docker run is docker run [OPTIONS] IMAGE [COMMAND] [ARG...] . Let's run the command uname -a to get some info about the operating system. First run on your own system (use systeminfo if you are on Windows): uname -a This should print something like this to your command line: Darwin liv433l.lan 15.6.0 Darwin Kernel Version 15.6.0: Mon Oct 2 22:20:08 PDT 2017; root:xnu-3248.71.4~1/RELEASE_X86_64 x86_64 Seems like I'm running the Darwin version of macOS. Then run it in the Ubuntu Docker container: docker run ubuntu uname -a Here I get the following result: Linux 24d063b5d877 5.4.39-linuxkit #1 SMP Fri May 8 23:03:06 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux And now I'm running on Linux! Try the same thing with whoami .","title":"Running containers"},{"location":"containers/containers-2-the-basics/#running-interactively","text":"So, seems we can execute arbitrary commands on Linux. Seems useful, but maybe a bit limited. We can also get an interactive terminal with the flags -it . docker run -it ubuntu This should put at a terminal prompt inside a container running Ubuntu. Your prompt should now look similar to: root@1f339e929fa9:/# Here you can do whatever; install, run, remove stuff. It will still be within the container and never affect your host system. Now exit the container with exit .","title":"Running interactively"},{"location":"containers/containers-2-the-basics/#containers-inside-scripts","text":"Okay, so Docker lets us work in any OS in a quite convenient way. That would probably be useful on its own, but Docker is much more powerful than that. For example, let's look at the shell part of the index_genome rule in the Snakemake workflow for the MRSA case study: shell: \"\"\" bowtie2-build tempfile intermediate/{wildcards.genome_id} > {log} \"\"\" You may have seen that one can use containers through both Snakemake and Nextflow if you've gone through their tutorial's extra material, but we can also use containers directly inside scripts in a very simple way. Let's imagine we want to run the above command using containers instead. How would that look? It's quite simple, really: first we find a container image that has bowtie2 installed, and then prepend the command with docker run <image> . First of all we need to download the genome to index though, so run: curl -o NCTC8325.fa.gz ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz gunzip -c NCTC8325.fa.gz > tempfile to download and prepare the input for bowtie2. Now try running the following Bash code: docker run -v $(pwd):/analysis quay.io/biocontainers/bowtie2:2.5.0--py310h8d7afc0_0 bowtie2-build /analysis/tempfile /analysis/NCTC8325 Docker will automatically download the container image and subsequently run the command! Here we're using -v $(pwd):/analysis to mount the current directory inside the container in order to make the tempfile input available to bowtie2. More on these so called \"Bind mounts\" in Section 4 of this tutorial. Quick recap In this section we've learned: How to use docker pull for downloading images from a central registry. How to use docker image ls for getting information about the images we have on our system. How to use docker run for starting a container from an image. How to use the -it flag for running in interactive mode. How to use Docker inside scripts.","title":"Containers inside scripts"},{"location":"containers/containers-3-building-images/","text":"In the previous section we downloaded a Docker image of Ubuntu and noticed that it was based on layers, each with a unique hash as id. An image in Docker is based on a number of read-only layers, where each layer contains the differences to the previous layers. If you've done the Git tutorial this might remind you of how a Git commit contains the difference to the previous commit. The great thing about this is that we can start from one base layer, say containing an operating system and some utility programs, and then generate many new images based on this, say 10 different project-specific images. The total space requirements would then only be $base+\\sum_{i=1}^{10}(specific_{i})$ rather than $\\sum_{i=1}^{10}(base+specific_{i})$. For example, Bioconda (see the Conda tutorial ) has one base image and then one individual layer for each of the more than 3000 packages available in Bioconda. Docker provides a convenient way to describe how to go from a base image to the image we want by using a \"Dockerfile\". This is a simple text file containing the instructions for how to generate each layer. Docker images are typically quite large, often several GBs, while Dockerfiles are small and serve as blueprints for the images. It is therefore good practice to have your Dockerfile in your project Git repository, since it allows other users to exactly replicate your project environment. We will be looking at a Dockerfile called Dockerfile_slim that is located in your containers directory (where you should hopefully be standing already). We will now go through that file and discuss the different steps and what they do. After that we'll build the image and test it out. Lastly, we'll start from that image and make a new one to reproduce the results from the Conda tutorial . Understanding Dockerfiles # Here are the first few lines of Dockerfile_slim . Each line in the Dockerfile will typically result in one layer in the resulting image. The format for Dockerfiles is INSTRUCTION arguments . A full specification of the format, together with best practices, can be found here . FROM ubuntu:16.04 LABEL description = \"Minimal image for the NBIS reproducible research course.\" MAINTAINER \"John Sundh\" john.sundh@scilifelab.se Here we use the instructions FROM , LABEL and MAINTAINER . The important one is FROM , which specifies the base image our image should start from. In this case we want it to be Ubuntu 16.04, which is one of the official repositories . There are many roads to Rome when it comes to choosing the best image to start from. Say you want to run RStudio in a Conda environment through a Jupyter notebook. You could then start from one of the rocker images for R, a Miniconda image , or a Jupyter image . Or you just start from one of the low-level official images and set up everything from scratch. LABEL and MAINTAINER is just meta-data that can be used for organizing your various Docker components. Let's take a look at the next section of the Dockerfile. # Use bash as shell SHELL [\"/bin/bash\", \"-c\"] # Set workdir WORKDIR /course SHELL simply sets which shell to use. WORKDIR determines the directory the container should start in. The next few lines introduce the important RUN instruction, which is used for executing shell commands: # Install necessary tools RUN apt-get update && \\ apt-get install -y --no-install-recommends bzip2 \\ ca-certificates \\ curl \\ fontconfig \\ git \\ language-pack-en \\ tzdata \\ vim \\ unzip \\ wget \\ && apt-get clean # Install Miniconda and add to PATH RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O && \\ bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/ && \\ rm Miniconda3-4.7.12.1-Linux-x86_64.sh && \\ /usr/miniconda3/bin/conda clean -tipsy && \\ ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \\ echo \". /usr/miniconda3/etc/profile.d/conda.sh\" >> ~/.bashrc && \\ echo \"conda activate base\" >> ~/.bashrc As a general rule, you want each layer in an image to be a \"logical unit\". For example, if you want to install a program the RUN command should both retrieve the program, install it and perform any necessary clean up. This is due to how layers work and how Docker decides what needs to be rerun between builds. The first command uses Ubuntu's package manager APT to install some packages (similar to how we've previously used Conda). Say that the first command was split into two instead: # Update apt-get RUN apt-get update # Install packages RUN apt-get install -y --no-install-recommends bzip2 \\ ca-certificates \\ curl \\ fontconfig \\ git \\ language-pack-en \\ tzdata \\ vim \\ unzip \\ wget # Clear the local repository of retrieved package files RUN apt-get clean The first command will update the apt-get package lists and the second will install the packages bzip2 , ca-certificates , curl , fontconfig , git , language-pack-en , tzdata , vim , unzip and wget . Say that you build this image now, and then in a month's time you realize that you would have liked a Swedish language pack instead of an English. You change to language-pack-sv and rebuild the image. Docker detects that there is no layer with the new list of packages and reruns the second RUN command. However, there is no way for Docker to know that it should also update the apt-get package lists . You therefore risk to end up with old versions of packages and, even worse, the versions would depend on when the previous version of the image was first built. The next RUN command retrieves and installs Miniconda3. Let's see what would happen if we had that as separate commands instead. # Download Miniconda3 RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O # Install it RUN bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/ # Remove the downloaded installation file RUN rm Miniconda3-4.7.12.1-Linux-x86_64.sh # Remove unused packages and caches RUN /usr/miniconda3/bin/conda clean -tipsy # Permanently enable the Conda command RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh RUN echo \". /usr/miniconda3/etc/profile.d/conda.sh\" >> ~/.bashrc # Add the base environment permanently to PATH RUN echo \"conda activate base\" >> ~/.bashrc Remember that each layer contains the difference compared to the previous layer? What will happen here is that the first command adds the installation file and the second will unpack the file and install the software. The third layer will say \"the installation file should no longer exist on the file system\". However, the file will still remain in the image since the image is constructed layer-by-layer bottom-up. This results in unnecessarily many layers and bloated images. Line four is cleaning up conda to free up space, and the next two lines are there to make the Conda command available in the shell. The last command adds a code snippet to the bash startup file which automatically activates the Conda base environment in the container. # Add conda to PATH and set locale ENV PATH=\"/usr/miniconda3/bin:${PATH}\" ENV LC_ALL en_US.UTF-8 ENV LC_LANG en_US.UTF-8 Here we use the new instruction ENV . The first command adds conda to the path, so we can write conda install instead of /usr/miniconda3/bin/conda install . The next two commands set an UTF-8 character encoding so that we can use weird characters (and a bunch of other things). # Configure Conda channels and install Mamba RUN conda config --add channels bioconda \\ && conda config --add channels conda-forge \\ && conda config --set channel_priority strict \\ && conda install mamba \\ && mamba clean --all Here we just configure Conda and install Mamba, for quicker installations of any subsequent Conda packages we might want to do. # Open port for running Jupyter Notebook EXPOSE 8888 # Start Bash shell by default CMD /bin/bash EXPOSE opens up the port 8888, so that we can later run a Jupyter Notebook server on that port. CMD is an interesting instruction. It sets what a container should run when nothing else is specified. It can be used for example for printing some information on how to use the image or, as here, start a shell for the user. If the purpose of your image is to accompany a publication then CMD could be to run the workflow that generates the paper figures from raw data. Building from Dockerfiles # Ok, so now we understand how a Dockerfile works. Constructing the image from the Dockerfile is really simple. Try it out now: docker build -f Dockerfile_slim -t my_docker_image . This should result in something similar to this: => [internal] load build definition from Dockerfile_slim => => transferring dockerfile: 1.88kB => [internal] load .dockerignore => => transferring context: 2B => [internal] load metadata for docker.io/library/ubuntu:16.04 => [auth] library/ubuntu:pull token for registry-1.docker.io => [1/5] FROM docker.io/library/ubuntu:16.04@sha256:bb84bbf2ff36d46acaf0bb0c6bcb33dae64cd93cba8652d74c9aaf438fada438 => CACHED [2/5] WORKDIR /course => CACHED [3/5] RUN apt-get update && apt-get install -y --no-install-recommends bzip2 ca-certificates => CACHED [4/5] RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O && h Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3 => CACHED [5/5] RUN conda config --add channels bioconda && conda config --add channels conda-forge && conda config --set channel_priority strict && conda instal => exporting to image => => exporting layers => => writing image sha256:d14301f829d4554816df54ace927ec0aaad4a994e028371455f7a18a370f6af9 => => naming to docker.io/library/my_docker_image Exactly how the output looks depends on which version of Docker you are using. The -f flag sets which Dockerfile to use and -t tags the image with a name. This name is how you will refer to the image later. Lastly, the . is the path to where the image should be build ( . means the current directory). This had no real impact in this case, but matters if you want to import files. Validate with docker image ls that you can see your new image. Creating your own Dockerfile # Now it's time to make our own Dockerfile to reproduce the results from the Conda tutorial . If you haven't done the tutorial, it boils down to creating a Conda environment file, setting up that environment, downloading three RNA-seq data files, and running FastQC on those files. We will later package and run the whole RNA-seq workflow in a Docker container, but for now we keep it simple to reduce the size and time required. The Conda tutorial uses a shell script, run_qc.sh , for downloading and running the analysis. A copy of this file should also be available in your current directory. If we want to use the same script we need to include it in the image. So, this is what we need to do: Create the file Dockerfile_conda . Set FROM to the image we just built. Install the required packages with Conda. We could do this by adding environment.yml from the Conda tutorial, but here we do it directly as RUN commands. We need to add the conda-forge and bioconda channels with conda config --add channels <channel_name> and install fastqc=0.11.9 and sra-tools=2.10.1 with conda install . The packages will be installed to the default environment named base inside the container. Add run_qc.sh to the image by using the COPY instruction. The syntax is COPY source target , so in our case simply COPY run_qc.sh . to copy to the work directory in the image. Set the default command for the image to bash run_qc.sh , which will execute the shell script. Try to add required lines to Dockerfile_conda . If it seems overwhelming you can take a look at an example below: Click to show FROM my_docker_image:latest RUN conda config --add channels bioconda && \\ conda config --add channels conda-forge && \\ mamba install -n base fastqc=0.11.9 sra-tools=2.10.1 COPY run_qc.sh . CMD bash run_qc.sh Build the image and tag it my_docker_conda : docker build -t my_docker_conda -f Dockerfile_conda . Verify that the image was built using docker image ls . Quick recap In this section we've learned: How the keywords FROM , LABEL , MAINTAINER , RUN , ENV , SHELL , WORKDIR , and CMD can be used when writing a Dockerfile. The importance of letting each layer in the Dockerfile be a \"logical unit\". How to use docker build to construct and tag an image from a Dockerfile. How to create your own Dockerfile.","title":"Building a Docker image"},{"location":"containers/containers-3-building-images/#understanding-dockerfiles","text":"Here are the first few lines of Dockerfile_slim . Each line in the Dockerfile will typically result in one layer in the resulting image. The format for Dockerfiles is INSTRUCTION arguments . A full specification of the format, together with best practices, can be found here . FROM ubuntu:16.04 LABEL description = \"Minimal image for the NBIS reproducible research course.\" MAINTAINER \"John Sundh\" john.sundh@scilifelab.se Here we use the instructions FROM , LABEL and MAINTAINER . The important one is FROM , which specifies the base image our image should start from. In this case we want it to be Ubuntu 16.04, which is one of the official repositories . There are many roads to Rome when it comes to choosing the best image to start from. Say you want to run RStudio in a Conda environment through a Jupyter notebook. You could then start from one of the rocker images for R, a Miniconda image , or a Jupyter image . Or you just start from one of the low-level official images and set up everything from scratch. LABEL and MAINTAINER is just meta-data that can be used for organizing your various Docker components. Let's take a look at the next section of the Dockerfile. # Use bash as shell SHELL [\"/bin/bash\", \"-c\"] # Set workdir WORKDIR /course SHELL simply sets which shell to use. WORKDIR determines the directory the container should start in. The next few lines introduce the important RUN instruction, which is used for executing shell commands: # Install necessary tools RUN apt-get update && \\ apt-get install -y --no-install-recommends bzip2 \\ ca-certificates \\ curl \\ fontconfig \\ git \\ language-pack-en \\ tzdata \\ vim \\ unzip \\ wget \\ && apt-get clean # Install Miniconda and add to PATH RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O && \\ bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/ && \\ rm Miniconda3-4.7.12.1-Linux-x86_64.sh && \\ /usr/miniconda3/bin/conda clean -tipsy && \\ ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \\ echo \". /usr/miniconda3/etc/profile.d/conda.sh\" >> ~/.bashrc && \\ echo \"conda activate base\" >> ~/.bashrc As a general rule, you want each layer in an image to be a \"logical unit\". For example, if you want to install a program the RUN command should both retrieve the program, install it and perform any necessary clean up. This is due to how layers work and how Docker decides what needs to be rerun between builds. The first command uses Ubuntu's package manager APT to install some packages (similar to how we've previously used Conda). Say that the first command was split into two instead: # Update apt-get RUN apt-get update # Install packages RUN apt-get install -y --no-install-recommends bzip2 \\ ca-certificates \\ curl \\ fontconfig \\ git \\ language-pack-en \\ tzdata \\ vim \\ unzip \\ wget # Clear the local repository of retrieved package files RUN apt-get clean The first command will update the apt-get package lists and the second will install the packages bzip2 , ca-certificates , curl , fontconfig , git , language-pack-en , tzdata , vim , unzip and wget . Say that you build this image now, and then in a month's time you realize that you would have liked a Swedish language pack instead of an English. You change to language-pack-sv and rebuild the image. Docker detects that there is no layer with the new list of packages and reruns the second RUN command. However, there is no way for Docker to know that it should also update the apt-get package lists . You therefore risk to end up with old versions of packages and, even worse, the versions would depend on when the previous version of the image was first built. The next RUN command retrieves and installs Miniconda3. Let's see what would happen if we had that as separate commands instead. # Download Miniconda3 RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O # Install it RUN bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/ # Remove the downloaded installation file RUN rm Miniconda3-4.7.12.1-Linux-x86_64.sh # Remove unused packages and caches RUN /usr/miniconda3/bin/conda clean -tipsy # Permanently enable the Conda command RUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh RUN echo \". /usr/miniconda3/etc/profile.d/conda.sh\" >> ~/.bashrc # Add the base environment permanently to PATH RUN echo \"conda activate base\" >> ~/.bashrc Remember that each layer contains the difference compared to the previous layer? What will happen here is that the first command adds the installation file and the second will unpack the file and install the software. The third layer will say \"the installation file should no longer exist on the file system\". However, the file will still remain in the image since the image is constructed layer-by-layer bottom-up. This results in unnecessarily many layers and bloated images. Line four is cleaning up conda to free up space, and the next two lines are there to make the Conda command available in the shell. The last command adds a code snippet to the bash startup file which automatically activates the Conda base environment in the container. # Add conda to PATH and set locale ENV PATH=\"/usr/miniconda3/bin:${PATH}\" ENV LC_ALL en_US.UTF-8 ENV LC_LANG en_US.UTF-8 Here we use the new instruction ENV . The first command adds conda to the path, so we can write conda install instead of /usr/miniconda3/bin/conda install . The next two commands set an UTF-8 character encoding so that we can use weird characters (and a bunch of other things). # Configure Conda channels and install Mamba RUN conda config --add channels bioconda \\ && conda config --add channels conda-forge \\ && conda config --set channel_priority strict \\ && conda install mamba \\ && mamba clean --all Here we just configure Conda and install Mamba, for quicker installations of any subsequent Conda packages we might want to do. # Open port for running Jupyter Notebook EXPOSE 8888 # Start Bash shell by default CMD /bin/bash EXPOSE opens up the port 8888, so that we can later run a Jupyter Notebook server on that port. CMD is an interesting instruction. It sets what a container should run when nothing else is specified. It can be used for example for printing some information on how to use the image or, as here, start a shell for the user. If the purpose of your image is to accompany a publication then CMD could be to run the workflow that generates the paper figures from raw data.","title":"Understanding Dockerfiles"},{"location":"containers/containers-3-building-images/#building-from-dockerfiles","text":"Ok, so now we understand how a Dockerfile works. Constructing the image from the Dockerfile is really simple. Try it out now: docker build -f Dockerfile_slim -t my_docker_image . This should result in something similar to this: => [internal] load build definition from Dockerfile_slim => => transferring dockerfile: 1.88kB => [internal] load .dockerignore => => transferring context: 2B => [internal] load metadata for docker.io/library/ubuntu:16.04 => [auth] library/ubuntu:pull token for registry-1.docker.io => [1/5] FROM docker.io/library/ubuntu:16.04@sha256:bb84bbf2ff36d46acaf0bb0c6bcb33dae64cd93cba8652d74c9aaf438fada438 => CACHED [2/5] WORKDIR /course => CACHED [3/5] RUN apt-get update && apt-get install -y --no-install-recommends bzip2 ca-certificates => CACHED [4/5] RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O && h Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3 => CACHED [5/5] RUN conda config --add channels bioconda && conda config --add channels conda-forge && conda config --set channel_priority strict && conda instal => exporting to image => => exporting layers => => writing image sha256:d14301f829d4554816df54ace927ec0aaad4a994e028371455f7a18a370f6af9 => => naming to docker.io/library/my_docker_image Exactly how the output looks depends on which version of Docker you are using. The -f flag sets which Dockerfile to use and -t tags the image with a name. This name is how you will refer to the image later. Lastly, the . is the path to where the image should be build ( . means the current directory). This had no real impact in this case, but matters if you want to import files. Validate with docker image ls that you can see your new image.","title":"Building from Dockerfiles"},{"location":"containers/containers-3-building-images/#creating-your-own-dockerfile","text":"Now it's time to make our own Dockerfile to reproduce the results from the Conda tutorial . If you haven't done the tutorial, it boils down to creating a Conda environment file, setting up that environment, downloading three RNA-seq data files, and running FastQC on those files. We will later package and run the whole RNA-seq workflow in a Docker container, but for now we keep it simple to reduce the size and time required. The Conda tutorial uses a shell script, run_qc.sh , for downloading and running the analysis. A copy of this file should also be available in your current directory. If we want to use the same script we need to include it in the image. So, this is what we need to do: Create the file Dockerfile_conda . Set FROM to the image we just built. Install the required packages with Conda. We could do this by adding environment.yml from the Conda tutorial, but here we do it directly as RUN commands. We need to add the conda-forge and bioconda channels with conda config --add channels <channel_name> and install fastqc=0.11.9 and sra-tools=2.10.1 with conda install . The packages will be installed to the default environment named base inside the container. Add run_qc.sh to the image by using the COPY instruction. The syntax is COPY source target , so in our case simply COPY run_qc.sh . to copy to the work directory in the image. Set the default command for the image to bash run_qc.sh , which will execute the shell script. Try to add required lines to Dockerfile_conda . If it seems overwhelming you can take a look at an example below: Click to show FROM my_docker_image:latest RUN conda config --add channels bioconda && \\ conda config --add channels conda-forge && \\ mamba install -n base fastqc=0.11.9 sra-tools=2.10.1 COPY run_qc.sh . CMD bash run_qc.sh Build the image and tag it my_docker_conda : docker build -t my_docker_conda -f Dockerfile_conda . Verify that the image was built using docker image ls . Quick recap In this section we've learned: How the keywords FROM , LABEL , MAINTAINER , RUN , ENV , SHELL , WORKDIR , and CMD can be used when writing a Dockerfile. The importance of letting each layer in the Dockerfile be a \"logical unit\". How to use docker build to construct and tag an image from a Dockerfile. How to create your own Dockerfile.","title":"Creating your own Dockerfile"},{"location":"containers/containers-4-managing-containers/","text":"When you start a container with docker run it is given an unique id that you can use for interacting with the container. Let's try to run a container from the image we just created: docker run my_docker_conda If everything worked run_qc.sh is executed and will first download and then analyse the three samples. Once it's finished you can list all containers, including those that have exited. docker container ls --all This should show information about the container that we just ran. Similar to: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 39548f30ce45 my_docker_conda \"/bin/bash -c 'bas...\" 3 minutes ago Exited (0) 3 minutes ago el If we run docker run without any flags, your local terminal is attached to the container. This enables you to see the output of run_qc.sh , but also disables you from doing anything else in the meantime. We can start a container in detached mode with the -d flag. Try this out and run docker container ls to validate that the container is running. By default, Docker keeps containers after they have exited. This can be convenient for debugging or if you want to look at logs, but it also consumes huge amounts of disk space. It's therefore a good idea to always run with --rm , which will remove the container once it has exited. If we want to enter a running container, there are two related commands we can use, docker attach and docker exec . docker attach will attach local standard input, output, and error streams to a running container. This can be useful if your terminal closed down for some reason or if you started a terminal in detached mode and changed your mind. docker exec can be used to execute any command in a running container. It's typically used to peak in at what is happening by opening up a new shell. Here we start the container in detached mode and then start a new interactive shell so that we can see what happens. If you use ls inside the container you can see how the script generates file in the data , intermediate and results directories. Note that you will be thrown out when the container exits, so you have to be quick. docker run -d --rm --name my_container my_docker_conda docker exec -it my_container /bin/bash Bind mounts # There are obviously some advantages to isolating and running your data analysis in containers, but at some point you need to be able to interact with the host system to actually deliver the results. This is done via bind mounts. When you use a bind mount, a file or directory on the host machine is mounted into a container. That way, when the container generates a file in such a directory it will appear in the mounted directory on your host system. Tip Docker also has a more advanced way of data storage called volumes . Volumes provide added flexibility and are independent of the host machine's filesystem having a specific directory structure available. They are particularly useful when you want to share data between containers. Say that we are interested in getting the resulting html reports from FastQC in our container. We can do this by mounting a directory called, say, fastqc_results in your current directory to the /course/results/fastqc directory in the container. Try this out by running: docker run --rm -v $(pwd)/fastqc_results:/course/results/fastqc my_docker_conda Here the -v flag to docker run specifies the bind mount in the form of directory/on/your/computer:/directory/inside/container . $(pwd) simply evaluates to the working directory on your computer. Once the container finishes validate that it worked by opening one of the html reports under fastqc_results/ . We can also use bind mounts for getting files into the container rather than out. We've mainly been discussing Docker in the context of packaging an analysis pipeline to allow someone else to reproduce its outcome. Another application is as a kind of very powerful environment manager, similarly to how we've used Conda before. If you've organized your work into projects, then you can mount the whole project directory in a container and use the container as the terminal for running stuff while still using your normal OS for editing files and so on. Let's try this out by mounting our current directory and start an interactive terminal. Note that this will override the CMD command, so we won't start the analysis automatically when we start the container. docker run -it --rm -v $(pwd):/course/ my_docker_conda /bin/bash If you run ls you will see that all the files in the docker directory are there. Now edit run_qc.sh on your host system to download, say, 12000 reads instead of 15000. Then rerun the analysis with bash run_qc.sh . Tada! Validate that the resulting html reports look fine and then exit the container with exit . Quick recap In this section we've learned: How to use docker run for starting a container and how the flags -d and --rm work. How to use docker container ls for displaying information about the containers. How to use docker attach and docker exec to interact with running containers. How to use bind mounts to share data between the container and the host system.","title":"Managing containers"},{"location":"containers/containers-4-managing-containers/#bind-mounts","text":"There are obviously some advantages to isolating and running your data analysis in containers, but at some point you need to be able to interact with the host system to actually deliver the results. This is done via bind mounts. When you use a bind mount, a file or directory on the host machine is mounted into a container. That way, when the container generates a file in such a directory it will appear in the mounted directory on your host system. Tip Docker also has a more advanced way of data storage called volumes . Volumes provide added flexibility and are independent of the host machine's filesystem having a specific directory structure available. They are particularly useful when you want to share data between containers. Say that we are interested in getting the resulting html reports from FastQC in our container. We can do this by mounting a directory called, say, fastqc_results in your current directory to the /course/results/fastqc directory in the container. Try this out by running: docker run --rm -v $(pwd)/fastqc_results:/course/results/fastqc my_docker_conda Here the -v flag to docker run specifies the bind mount in the form of directory/on/your/computer:/directory/inside/container . $(pwd) simply evaluates to the working directory on your computer. Once the container finishes validate that it worked by opening one of the html reports under fastqc_results/ . We can also use bind mounts for getting files into the container rather than out. We've mainly been discussing Docker in the context of packaging an analysis pipeline to allow someone else to reproduce its outcome. Another application is as a kind of very powerful environment manager, similarly to how we've used Conda before. If you've organized your work into projects, then you can mount the whole project directory in a container and use the container as the terminal for running stuff while still using your normal OS for editing files and so on. Let's try this out by mounting our current directory and start an interactive terminal. Note that this will override the CMD command, so we won't start the analysis automatically when we start the container. docker run -it --rm -v $(pwd):/course/ my_docker_conda /bin/bash If you run ls you will see that all the files in the docker directory are there. Now edit run_qc.sh on your host system to download, say, 12000 reads instead of 15000. Then rerun the analysis with bash run_qc.sh . Tada! Validate that the resulting html reports look fine and then exit the container with exit . Quick recap In this section we've learned: How to use docker run for starting a container and how the flags -d and --rm work. How to use docker container ls for displaying information about the containers. How to use docker attach and docker exec to interact with running containers. How to use bind mounts to share data between the container and the host system.","title":"Bind mounts"},{"location":"containers/containers-5-sharing-images/","text":"There would be little point in going through all the trouble of making your analyses reproducible if you can't distribute them to others. Luckily, sharing Docker containers is extremely easy, and can be done in several ways. One of the more common ways to share Docker images is through container registries and repositories . For example, a Docker registry is a service that stores Docker images, which could be hosted by a third party, publicly or privately. One of the most common registries is Docker Hub , which is a registry hosted by Docker itself. A repository, on the other hand, is a collection of container images with the same name but different tags, i.e. versions. For example, ubuntu:latest or ubuntu:20.04 . Repositories are stored in registries. Note Remember that we now have some clashing nomenclature between Git repositories (which we covered in the Git tutorial) and container repositories, so be aware of which one you're talking about! There are many registries out there, but here are some that might be of interest to you who are taking this course: Docker Hub Quay Biocontainers Rocker Jupyter containers The most common registry is probably Docker Hub, which lets you host unlimited public images and one private image for free (after which they charge a small fee). Let's see how it's done! Register for an account on Docker Hub . Use docker login -u your_dockerhub_id to login to the Docker Hub registry. When you build an image, tag it with -t your_dockerhub_id/image_name , rather than just image_name . Once the image has been built, upload it to Docker Hub with docker push your_dockerhub_id/image_name . If another user runs docker run your_dockerhub_id/image_name the image will automatically be retrieved from Docker Hub. You can use docker pull for downloading without running. If you want to refer to a Docker image in for example a publication, it's very important that it's the correct version of the image. You can do this by adding a tag to the name like this docker build -t your_dockerhub_id/image_name:tag_name . Tip On Docker Hub it is also possible to link to your Bitbucket or GitHub account and select repositories from which you want to automatically build and distribute Docker images. The Docker Hub servers will then build an image from the Dockerfile in your Git repository and make it available for download using docker pull . That way, you don't have to bother manually building and pushing using docker push . The GitHub repository for this course is linked to Docker Hub and the Docker images are built automatically from Dockerfile and Dockerfile_slim , triggered by changes made to the GitHub repository. You can take a look at the course on Docker Hub here . Quick recap In this section we've learned: How container registries and repositories work How to use Docker Hub to share Docker images","title":"Distributing your images"},{"location":"containers/containers-6-packaging-the-case-study/","text":"During these tutorials we have been working on a case study about the multiresistant bacteria MRSA. Here we will build and run a Docker container that contains all the work we've done so far. We've set up a GitHub repository for version control and for hosting our project. We've defined a Conda environment that specifies the packages we're depending on in the project. We've constructed a Snakemake workflow that performs the data analysis and keeps track of files and parameters. We've written a R Markdown document that takes the results from the Snakemake workflow and summarizes them in a report. The workshop-reproducible-research/tutorials/containers directory contains the final versions of all the files we've generated in the other tutorials: environment.yml , Snakefile , config.yml , code/header.tex , and code/supplementary_material.Rmd . The only difference compared to the other tutorials is that we have also included the rendering of the Supplementary Material HTML file into the Snakemake workflow as the rule make_supplementary . Running all of these steps will take some time to execute (around 20 minutes or so), in particular if you're on a slow internet connection. Now take a look at Dockerfile . Everything should look quite familiar to you, since it's basically the same steps as in the image we constructed in the previous section, although some sections have been moved around. The main difference is that we add the project files needed for executing the workflow (mentioned in the previous paragraph), and install the conda packages listed in environment.yml . If you look at the CMD command you can see that it will run the whole Snakemake workflow by default. Now run docker build as before, tag the image with my_docker_project : docker build -t my_docker_project -f Dockerfile . Go get a coffee while the image builds (or you could use docker pull nbisweden/workshop-reproducible-research which will download the same image). Validate with docker image ls . Now all that remains is to run the whole thing with docker run . We just want to get the results, so mount the directory /course/results/ to, say, mrsa_results in your current directory. Well done! You now have an image that allows anyone to exactly reproduce your analysis workflow (if you first docker push to Dockerhub that is). Tip If you've done the Jupyter tutorial , you know that Jupyter Notebook runs as a web server. This makes it very well suited for running in a Docker container, since we can just expose the port Jupyter Notebook uses and redirect it to one of our own. You can then work with the notebooks in your browser just as you've done before, while it's actually running in the container. This means you could package your data, scripts and environment in a Docker image that also runs a Jupyter Notebook server. If you make this image available, say on Dockerhub, other researchers could then download it and interact with your data/code via the fancy interactive Jupyter notebooks that you have prepared for them. We haven't made any fancy notebooks for you, but we have set up a Jupyter Notebook server. Try it out if you want to (replace the image name with your version if you've built it yourself): bash docker run -it -p 8888:8888 nbisweden/workshop-reproducible-research \\ jupyter notebook --ip=0.0.0.0 --allow-root","title":"Packaging the case study"},{"location":"containers/containers-7-singularity/","text":"Singularity as an alternative container tool # Singularity is a container software alternative to Docker. It was originally developed by researchers at Lawrence Berkeley National Laboratory with focus on security, scientific software, and HPC clusters. One of the ways in which Singularity is more suitable for HPC is that it very actively restricts permissions so that you do not gain access to additional resources while inside the container. Here we give a brief introduction to Singularity and specifically how it can be used on HPC clusters such as Uppmax. If you want to read more, here are some additional resources: Singularity docs Uppmax Singularity user guide Singularity and Apptainer Singularity has very recently been renamed to Apptainer , but we have opted to stick with the original name in the material for now, while the change is still being adopted by the community and various documentation online. Converting Docker images to Singularity files # Singularity, unlike Docker, stores images as single files. A Singularity image file is self-contained (no shared layers) and can be moved around and shared like any other file. While it is possible to define and build Singularity images from scratch, in a manner similar to what you've already learned for Docker, this is not something we will cover here (but feel free to read more about this in e.g. the Singularity docs ). Instead, we will take advantage of the fact that Singularity can convert Docker images to the Singularity Image Format (SIF). This is great if there's a Docker image that you want to use on an HPC cluster such as Uppmax where you cannot use Docker. Tip! If you are running singularity through Vagrant VirtualBox you may have to set the temporary directory that Singularity uses during pull/build commands to something with more disk space. First run mkdir ~/tmp to create a tmp directory inside the home folder of the VirtualBox, then export SINGULARITY_TMPDIR=\"~/tmp\" . Let's try to convert the Docker image for this course directly from DockerHub using singularity pull : singularity pull mrsa_proj.sif docker://nbisweden/workshop-reproducible-research This should result in a file called mrsa_proj.sif . Running a singularity image # In the Docker image we included the code needed for the workflow in the /course directory of the image. These files are of course also available in the Singularity image. However, a Singularity image is read-only (unless using the sandbox feature). This will be a problem if we try to run the workflow within the /course directory, since the workflow will produce files and Snakemake will create a .snakemake directory. Instead, we need to provide the files externally from our host system and simply use the Singularity image as the environment to execute the workflow in ( i.e. all the software and dependencies). In your current working directory ( workshop-reproducible-research/tutorials/containers/ ) the vital MRSA project files are already available ( Snakefile , config.yml , code/header.tex and code/supplementary_material.Rmd ). Since Singularity bind mounts the current working directory we can simply execute the workflow and generate the output files using: singularity run mrsa_proj.sif This executes the default run command, which is snakemake -rp -c 1 --configfile config.yml (as defined in the original Dockerfile ). The previous step in this tutorial included running the run_qc.sh script, so that part of the workflow has already been run and Snakemake will continue from that automatically without redoing anything. Once completed you should see a bunch of directories and files generated in your current working directory, including the results/ directory containing the final HTML report. Containers at different platforms # A common problem with Singularity is that you can only create local builds if you are working on a Linux system, as local builds for MacOS and Windows are currently not supported. This means that you might favour using Docker instead of Singularity, but what happens when you need to use a HPC cluster such as Uppmax? Docker won't work there, as it requires root privileges, so Singularity is the only solution. You can only run Singularity images there, however, not build them... So, how do you get a Singularity image for use on Uppmax if you can't build it either locally or on Uppmax? While it's possible to do remote builds (via the --remote flag), in our experience this functionality is not stable and for a lot of cases it won't help. Since most researchers will want to work in private Git repositories they can't supply their Conda environment.yml file to remote builds (which only works for public repositories), which means that you\u2019ll have to specify packages manually inside the container instead. There is, however, another solution: using Singularity inside Docker. By creating a bare-bones, Linux-based Docker image with Singularity you can build Singularity images locally on non-Linux operating systems. This can be either done from Singularity definition files or directly from already existing Docker images. You can read more about this at the following GitHub repository . Quick recap In this section we've learned: How to convert Docker images to Singularity images. How to use singularity run for starting a container from an image. How to build a Singularity image using Singularity inside Docker.","title":"Singularity"},{"location":"containers/containers-7-singularity/#singularity-as-an-alternative-container-tool","text":"Singularity is a container software alternative to Docker. It was originally developed by researchers at Lawrence Berkeley National Laboratory with focus on security, scientific software, and HPC clusters. One of the ways in which Singularity is more suitable for HPC is that it very actively restricts permissions so that you do not gain access to additional resources while inside the container. Here we give a brief introduction to Singularity and specifically how it can be used on HPC clusters such as Uppmax. If you want to read more, here are some additional resources: Singularity docs Uppmax Singularity user guide Singularity and Apptainer Singularity has very recently been renamed to Apptainer , but we have opted to stick with the original name in the material for now, while the change is still being adopted by the community and various documentation online.","title":"Singularity as an alternative container tool"},{"location":"containers/containers-7-singularity/#converting-docker-images-to-singularity-files","text":"Singularity, unlike Docker, stores images as single files. A Singularity image file is self-contained (no shared layers) and can be moved around and shared like any other file. While it is possible to define and build Singularity images from scratch, in a manner similar to what you've already learned for Docker, this is not something we will cover here (but feel free to read more about this in e.g. the Singularity docs ). Instead, we will take advantage of the fact that Singularity can convert Docker images to the Singularity Image Format (SIF). This is great if there's a Docker image that you want to use on an HPC cluster such as Uppmax where you cannot use Docker. Tip! If you are running singularity through Vagrant VirtualBox you may have to set the temporary directory that Singularity uses during pull/build commands to something with more disk space. First run mkdir ~/tmp to create a tmp directory inside the home folder of the VirtualBox, then export SINGULARITY_TMPDIR=\"~/tmp\" . Let's try to convert the Docker image for this course directly from DockerHub using singularity pull : singularity pull mrsa_proj.sif docker://nbisweden/workshop-reproducible-research This should result in a file called mrsa_proj.sif .","title":"Converting Docker images to Singularity files"},{"location":"containers/containers-7-singularity/#running-a-singularity-image","text":"In the Docker image we included the code needed for the workflow in the /course directory of the image. These files are of course also available in the Singularity image. However, a Singularity image is read-only (unless using the sandbox feature). This will be a problem if we try to run the workflow within the /course directory, since the workflow will produce files and Snakemake will create a .snakemake directory. Instead, we need to provide the files externally from our host system and simply use the Singularity image as the environment to execute the workflow in ( i.e. all the software and dependencies). In your current working directory ( workshop-reproducible-research/tutorials/containers/ ) the vital MRSA project files are already available ( Snakefile , config.yml , code/header.tex and code/supplementary_material.Rmd ). Since Singularity bind mounts the current working directory we can simply execute the workflow and generate the output files using: singularity run mrsa_proj.sif This executes the default run command, which is snakemake -rp -c 1 --configfile config.yml (as defined in the original Dockerfile ). The previous step in this tutorial included running the run_qc.sh script, so that part of the workflow has already been run and Snakemake will continue from that automatically without redoing anything. Once completed you should see a bunch of directories and files generated in your current working directory, including the results/ directory containing the final HTML report.","title":"Running a singularity image"},{"location":"containers/containers-7-singularity/#containers-at-different-platforms","text":"A common problem with Singularity is that you can only create local builds if you are working on a Linux system, as local builds for MacOS and Windows are currently not supported. This means that you might favour using Docker instead of Singularity, but what happens when you need to use a HPC cluster such as Uppmax? Docker won't work there, as it requires root privileges, so Singularity is the only solution. You can only run Singularity images there, however, not build them... So, how do you get a Singularity image for use on Uppmax if you can't build it either locally or on Uppmax? While it's possible to do remote builds (via the --remote flag), in our experience this functionality is not stable and for a lot of cases it won't help. Since most researchers will want to work in private Git repositories they can't supply their Conda environment.yml file to remote builds (which only works for public repositories), which means that you\u2019ll have to specify packages manually inside the container instead. There is, however, another solution: using Singularity inside Docker. By creating a bare-bones, Linux-based Docker image with Singularity you can build Singularity images locally on non-Linux operating systems. This can be either done from Singularity definition files or directly from already existing Docker images. You can read more about this at the following GitHub repository . Quick recap In this section we've learned: How to convert Docker images to Singularity images. How to use singularity run for starting a container from an image. How to build a Singularity image using Singularity inside Docker.","title":"Containers at different platforms"},{"location":"containers/containers-8-extra-material/","text":"Containers can be large and complicated, but once you start using them regularly you'll find that you start understand these complexities. There are lots of different things you can do with images and containers in general, especially when it comes to optimising build time or final image size. Here is some small tips and tricks that you can be inspired from! If you want to read more about containers in general you can check out these resources: A \"Get started with Docker\" at docker.com . An early paper on the subject of using Docker for reproducible research. A base image with Conda # We've used Conda throughout this container tutorial, and we did it by installing Conda inside the image when we built it. Wouldn't it be nice if we didn't have to do this particular step? After all, installing Conda is just busy-work, compared to installing the actual environment that we want to use for the analyses. Luckily, there are already container images out there that have Conda (and Mamba ) installed, such as the ones over at condaforge/mambaforge ! What follows is a Dockerfile that you could use instead of the ones described above to install things using a Conda environment.yml file, without having to install Conda in the Docker image when building it! FROM condaforge/mambaforge:4.10.1-0 LABEL description = \"Image description\" MAINTAINER \"Firstname Lastname\" firstname.lastname@gmail.se # Use bash as shell SHELL [\"/bin/bash\", \"-c\"] # Set working directory WORKDIR /project # Copy and install the Conda environment COPY environment.yml ./ RUN conda config --set channel_priority strict \\ && mamba env update --name base --file environment.yml \\ && mamba clean --all --force-pkgs-dirs --yes # Start Bash shell by default CMD /bin/bash","title":"Extra material"},{"location":"containers/containers-8-extra-material/#a-base-image-with-conda","text":"We've used Conda throughout this container tutorial, and we did it by installing Conda inside the image when we built it. Wouldn't it be nice if we didn't have to do this particular step? After all, installing Conda is just busy-work, compared to installing the actual environment that we want to use for the analyses. Luckily, there are already container images out there that have Conda (and Mamba ) installed, such as the ones over at condaforge/mambaforge ! What follows is a Dockerfile that you could use instead of the ones described above to install things using a Conda environment.yml file, without having to install Conda in the Docker image when building it! FROM condaforge/mambaforge:4.10.1-0 LABEL description = \"Image description\" MAINTAINER \"Firstname Lastname\" firstname.lastname@gmail.se # Use bash as shell SHELL [\"/bin/bash\", \"-c\"] # Set working directory WORKDIR /project # Copy and install the Conda environment COPY environment.yml ./ RUN conda config --set channel_priority strict \\ && mamba env update --name base --file environment.yml \\ && mamba clean --all --force-pkgs-dirs --yes # Start Bash shell by default CMD /bin/bash","title":"A base image with Conda"},{"location":"course-information/code-of-conduct/","text":"NBIS Training Code of Conduct # Training is one of the core values of NBIS, and benefits from the contributions of the entire scientific community. We value the involvement of everyone in the community. We are committed to creating a friendly and respectful place for learning, teaching and contributing. All participants in our events and communications are expected to show respect and courtesy to others. To make clear what is expected, everyone participating in NBIS/ELIXIR courses is required to conform to the Code of Conduct. This Code of Conduct applies to all spaces managed by NBIS including, but not limited to, courses, email lists, and online forums such as Studium, GitHub, Slack, Twitter and LinkedIn. Course organizers and teachers are expected to assist with the enforcement of the Code of Conduct. We are dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. By participating in this event, participants accept to abide by the NBIS Code of Conduct and accept the procedures by which any Code of Conduct incidents are resolved. Any form of behaviour to exclude, intimidate, or cause discomfort is a violation of the Code of Conduct. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all platforms and training events: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best to all those involved in this training event Show courtesy and respect towards everyone involved in this training event The NBIS Training Coordinator is responsible for enforcing the Code of Conduct, and may be contacted by emailing education@nbis.se . All reports will be reviewed and will be kept confidential. If you believe someone is violating the Code of Conduct, we ask that you report it to the NBIS Training coordinator, who will take the appropriate action to address the situation. For an extended description please see the ELIXIR Code of Conduct .","title":"NBIS Training Code of Conduct"},{"location":"course-information/code-of-conduct/#nbis-training-code-of-conduct","text":"Training is one of the core values of NBIS, and benefits from the contributions of the entire scientific community. We value the involvement of everyone in the community. We are committed to creating a friendly and respectful place for learning, teaching and contributing. All participants in our events and communications are expected to show respect and courtesy to others. To make clear what is expected, everyone participating in NBIS/ELIXIR courses is required to conform to the Code of Conduct. This Code of Conduct applies to all spaces managed by NBIS including, but not limited to, courses, email lists, and online forums such as Studium, GitHub, Slack, Twitter and LinkedIn. Course organizers and teachers are expected to assist with the enforcement of the Code of Conduct. We are dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. By participating in this event, participants accept to abide by the NBIS Code of Conduct and accept the procedures by which any Code of Conduct incidents are resolved. Any form of behaviour to exclude, intimidate, or cause discomfort is a violation of the Code of Conduct. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all platforms and training events: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best to all those involved in this training event Show courtesy and respect towards everyone involved in this training event The NBIS Training Coordinator is responsible for enforcing the Code of Conduct, and may be contacted by emailing education@nbis.se . All reports will be reviewed and will be kept confidential. If you believe someone is violating the Code of Conduct, we ask that you report it to the NBIS Training coordinator, who will take the appropriate action to address the situation. For an extended description please see the ELIXIR Code of Conduct .","title":"NBIS Training Code of Conduct"},{"location":"course-information/lectures/","text":"This page contains links to all the lectures in the course in PDF format, which you can either view in your browser (click the link) or download (right-click and save the link to wherever you want it). If you want to view the raw R Markdown / Jupyter files you can find them at GitHub , along with instructions on how to render them. Lecture links # Introduction Data management Git Conda R Markdown Jupyter Snakemake Nextflow Containers Putting it together","title":"Lectures"},{"location":"course-information/lectures/#lecture-links","text":"Introduction Data management Git Conda R Markdown Jupyter Snakemake Nextflow Containers Putting it together","title":"Lecture links"},{"location":"course-information/pre-course-setup/","text":"All of the tutorials and the material in them is dependent on the GitHub repository for the course. The first step of the setup is thus to download all the files that you will need, which is done differently depending on which operating system you have. At the last day, you will have the opportunity to try out the different tools on one of your own projects. In case you don't want to use a project you are currently working on, we have prepared a small-scale project for you. If you would like to work on your own project, it would be great if you could have the code and data ready before Friday so that you have more time for the exercise. In case your analysis project contains computationally intense steps it may be good to scale them down for the sake of the exercise. You might, for example, subset your raw data to only contain a minuscule part of its original size. Setup for Mac / Linux users # First, cd into a directory on your computer (or create one) where it makes sense to download the course directory. cd /path/to/your/directory git clone https://github.com/NBISweden/workshop-reproducible-research.git cd workshop-reproducible-research Tip If you want to revisit the material from an older instance of this course, you can do that using git checkout tags/<tag-name> , e.g. git checkout tags/course_1905 . To list all available tags, use git tag . Run this command after you have cd into workshop-reproducible-research as described above. If you do that, you probably also want to view the same older version of this website. Until spring 2021, the website was hosted at https://nbis-reproducible-research.readthedocs.io. Locate the version box in the bottom right corner of the website and select the corresponding version. Setup for Windows users # Using a Windows computer for bioinformatic work has sadly not been ideal most of the time, but large advanced in recent years have made this quite feasible through the Windows 10 Linux subsystem . This is the only setup for Windows users that we allow for participants of this course, as all the material has been created and tested to work on Unix-based systems. Using the Linux subsystem will give you access to a full command-line bash shell based on Linux on your Windows 10 PC. For the difference between the Linux Bash Shell and the PowerShell on Windows 10, see e.g. this article . Install Bash on Windows 10, follow the instructions at e.g. one of these resources: Installing the Windows Subsystem and the Linux Bash Installing and using Linux Bash on Windows Installing Linux Bash on Windows Note If you run into error messages when trying to download files through the Linux shell ( e.g. curl:(6) Could not resolve host ) then try adding the Google nameserver to the internet configuration by running sudo nano /etc/resolv.conf then add nameserver 8.8.8.8 to the bottom of the file and save it. Open a bash shell Linux terminal and clone the GitHub repository containing all files you will need for completing the tutorials as follows. First, cd into a directory on your computer (or create one) where it makes sense to download the course directory. Tip You can find the directory where the Linux distribution is storing all its files by typing explorer.exe . . This will launch the Windows File Explorer showing the current Linux directory. Alternatively, you can find the Windows C drive from within the bash shell Linux terminal by navigating to /mnt/c/ . cd /path/to/your/directory git clone https://github.com/NBISweden/workshop-reproducible-research.git cd workshop-reproducible-research Whenever a setup instruction specifies Mac or Linux ( i.e. only those two, with no alternative for Windows), please follow the Linux instructions. Tip If you want to revisit the material from an older instance of this course, you can do that using git checkout tags/<tag-name> , e.g. git checkout tags/course_1905 . To list all available tags, use git tag . Run this command after you have cd into workshop-reproducible-research as described above. If you do that, you probably also want to view the same older version of this website. Until spring 2021, the website was hosted at https://nbis-reproducible-research.readthedocs.io/en/latest/. Locate the version box in the bottom right corner of the website and select the corresponding version. Installing Git # Chances are that you already have git installed on your computer. You can check by running e.g. git --version . If you don't have git, install it following the instructions here . If you have a very old version of git you might want to update to a later version. Configure git # If it is the first time you use git on your computer, you may want to configure it so that it is aware of your username and email. These should match those that you have registered on GitHub. This will make it easier when you want to sync local changes with your remote GitHub repository. git config --global user.name \"Mona Lisa\" git config --global user.email \"mona_lisa@gmail.com\" Tip If you have several accounts ( e.g. both a GitHub and Bitbucket account), and thereby several different usernames, you can configure git on a per-repository level. Change directory into the relevant local git repository and run git config user.name \"Mona Lisa\" . This will set the default username for that repository only. You will also need to configure the default branch name to be main instead of master : git config --global init.defaultBranch \"main\" The short version of why you need to do this is that GitHub uses main as the default branch while Git itself is still using master ; please read the box below for more information. The default branch name The default branch name for Git and many of the online resources for hosting Git repositories has traditionally been master , which historically comes from the \"master/slave\" repositories of BitKeeper . This has been heavily discussed and in 2020 the decision was made by many ( including GitHub ) to start using main instead. Any repository created with GitHub uses this new naming scheme since October of 2020, and Git itself is currently discussing implementing a similar change. Git did, however, introduce the ability to set the default branch name when using git init in version 2.28 , instead of using a hard-coded master . We at NBIS want to be a part of this change, so we have chosen to use main for this course. Installing Conda # Conda is installed by downloading and executing an installer from the Conda website, but which version you need depends on your operating system: # Install Miniconda3 for 64-bit Mac curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-MacOSX-x86_64.sh -O bash Miniconda3-4.7.12.1-MacOSX-x86_64.sh rm Miniconda3-4.7.12.1-MacOSX-x86_64.sh # Install Miniconda3 for 64-bit Linux curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O bash Miniconda3-4.7.12.1-Linux-x86_64.sh rm Miniconda3-4.7.12.1-Linux-x86_64.sh Attention! If you already have installed Conda but want to update, you should be able to simply run conda update conda and subsequently conda init , and skip the installation instructions below. Apple M1 Chips If you have a newer Apple computer with an M1 chip, make sure you have installed Rosetta before you run the installer. If you want to more fully utilise the new architecture, head over to Miniforge ! The installer will ask you questions during the installation: Do you accept the license terms? (Yes) Do you accept the installation path or do you want to choose a different one? (Probably yes) Do you want to run conda init to setup Conda on your system? (Yes) Restart your shell so that the settings in ~/.bashrc / ~/.bash_profile can take effect. You can verify that the installation worked by running: conda --version Different Condas There are three Conda-related things you may have encountered: the first is Conda , the package and environment manager we've been talking about so far. Second is Miniconda , which is the installer for Conda. The third is Anaconda , which is a distribution of not only Conda, but also over 150 scientific Python packages. It's generally better to stick with only Conda, i.e. installing with Miniconda, rather than installing 3 GB worth of packages you may not even use. Configuring Conda # Lastly, we will setup the default channels (from where packages will be searched for and downloaded if no channel is specified). conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Installing Snakemake # We will use Conda environments for the set up of this tutorial, but don't worry if you don't understand exactly what everything does - you'll learn all the details at the course. First make sure you're currently situated inside the tutorials directory ( workshop-reproducible-research/tutorials ) and then create the Conda environment like so: conda env create -f snakemake/environment.yml -n snakemake-env conda activate snakemake-env Check that Snakemake is installed correctly, for example by executing snakemake --help . This should output a list of available Snakemake settings. If you get bash: snakemake: command not found then you need to go back and ensure that the Conda steps were successful. Once you've successfully completed the above steps you can deactivate your Conda environment using conda deactivate and continue with the setup for the other tools. Installing Nextflow # We'll use Conda to install Nextflow as well: navigate to workshop-reproducible-research/tutorials and create the Conda environment: conda env create -f nextflow/environment.yml -n nextflow-env conda activate nextflow-env Check that Nextflow was installed correctly by running nextflow -version . Once you've successfully completed the installation you can deactive the environment using conda deactivate and continue with the other setups, as needed. Installing R Markdown # We also use Conda to install R Markdown: make sure your working directory is in the tutorials directory ( workshop-reproducible-research/tutorials ) and install the necessary R packages defined in the environment.yml : conda env create -f rmarkdown/environment.yml -n rmarkdown-env You can then activate the environment followed by running RStudio in the background from the command line: conda activate rmarkdown-env rstudio & The sluggishness of Conda Some environments are inherently quite complicated in that they have many and varied dependencies, meaning that the search space for the entire dependency hierarchy becomes huge - leading to slow and sluggish installations. This is often the case for R environments. This can be improved by using Mamba, a faster wrapper around Conda. Simply run conda install -n base mamba to install Mamba in your base environment, and replace any conda command with mamba - except activating and deactivating environments, which still needs to be done using Conda. Once you've successfully completed the above steps you can deactivate your Conda environment using conda deactivate and continue with the setup for the other tools. Windows users In case you are having trouble installing R and RStudio using Conda, both run well directly on Windows and you may therefore want to install Windows versions of these software for this tutorial (if you haven't done so already). Conda is, however, the recommended way. If you're having issues with graphical applications, please have a look at this website ; scroll down to the \"Graphical applications\". RStudio and Conda In some cases RStudio doesn't play well with Conda due to differing libpaths. The first and simplest thing to try is to always start RStudio from the command line ( rstudio & ). If you're still having issues, check the available library path by .libPaths() to make sure that it points to a path within your Conda environment. It might be that .libPaths() shows multiple library paths, in which case R packages will be searched for by R in all these locations. This means that your R session will not be completely isolated in your Conda environment and that something that works for you might not work for someone else using the same Conda environment, simply because you had additional packages installed in the second library location. One way to force R to just use the conda library path is to add a .Renviron file to the directory where you start R with these lines: ``` R_LIBS_USER=\"\" R_LIBS=\"\" ``` ... and restart RStudio. The rmarkdown/ directory in the course materials already contains this file, so you shouldn't have to add this yourself, but we mention it here for your future projects. Installing Jupyter # Let's continue using Conda for installing software, since it's so convenient to do so! Move in the tutorials directory ( workshop-reproducible-research/tutorials ), create a Conda environment from the jupyter/environment.yml file and test the installation of Jupyter, like so: conda env create -f jupyter/environment.yml -n jupyter-env conda activate jupyter-env Once you've successfully completed the above steps you can deactivate your Conda environment using conda deactivate and continue with the setup for the other tools. Installing Docker # Installing Docker is quite straightforward on Mac or Windows and a little more cumbersome on Linux. Note that Docker runs as root, which means that you have to have sudo privileges on your computer in order to install or run Docker. When you have finished installing docker, regardless of which OS you are on, please type docker --version to verify that the installation was successful! macOS # Go to docker.com and select download option that is suitable for your computer's architecture ( i.e. if you have an Intel chip or a newer Apple M1 chip). This will download a dmg file - click on it when it's done to start the installation. This will open up a window where you can drag the Docker.app to Applications. Close the window and click the Docker app from the Applications menu. Now it's basically just to click \"next\" a couple of times and we should be good to go. You can find the Docker icon in the menu bar in the upper right part of the screen. Linux # How to install Docker differs a bit depending on your Linux distribution, but the steps are the same. Please follow the instructions for your distribution on https://docs.docker.com/engine/install/#server . Tip As mentioned before, Docker needs to run as root. You can achieve this by prepending all Docker commands with sudo . This is the approach that we will take in this tutorial, since the set up becomes a little simpler that way. If you plan on continuing using Docker you can get rid of this by adding your user to the group docker . Here are instructions for how to do this: https://docs.docker.com/engine/installation/linux/linux-postinstall/ . Windows # In order to run Docker on Windows your computer must support Hardware Virtualization Technology and virtualization must be enabled. This is typically done in BIOS. Setting this is outside the scope of this tutorial, so we'll simply go ahead as if though it's enabled and hope that it works. On Windows 10 we will install Docker for Windows, which is available at docker.com . Click the link Download from Docker Hub , and select Get Docker . Once the download is complete, execute the file and follow the instructions . You can now start Docker from the Start menu. You can search for it if you cannot find it; the Docker whale icon should appear in the task bar. You will probably need to enable integration with the Linux subsystem, if you haven't done so during the installation of Docker Desktop. Right-click on the Docker whale icon in the task bar and select Settings . Choose Resources and select WPS integration . Enable integration with the Linux subsystem and click Apply & Restart ; also restart the Linux subsystem. Installing Singularity # Installation of Singularity depends, again, on your operating system. When you have finished, regardless of your OS, please type singularity --version to verify that your installation was successful! Both Mac and Windows utilise Vagrant, for which the information in the box below may help you. Vagrant and VirtualBox The Vagrant VirtualBox with Singularity can be started like this: Move into the folder vm-singularity where you installed Singularity. Type vagrant up and once this has finished, verify that the Vagrant VirtualBox is running with vagrant status . Now, type vagrant ssh , which will open the Vagrant VirtualBox. The first time you open the Vagrant VirtualBox like this, you will have to download the course material to obtain a copy for the Singularity tutorial within the Vagrant VirtualBox by typing git clone https://github.com/NBISweden/workshop-reproducible-research.git . macOS # Please follow the Mac-specific instructions at the Singularity website . Linux # Follow the Linux-specific instruction at the Singularity website . Windows # Please follow the Windows-specific instructions at the Singularity website . Notes Last time we checked, the software \"Vagrant Manager\" was not available for download but the installation of Singularity was successful even without it. Version 6.1.28 of \"Virtual box for Windows\" may not work, please install version 6.1.26 from here in case you encounter problems when trying to start the Vagrant VirtualBox. Testing sra-tools # On some computers we've found that the package sra-tools which is used in the course is not working properly. The error seems to be related to some certificate used to communicate with remote read archives and may affect all environments with sra-tools on the dependency list. If you run into errors with the program fastq-dump from the sra-tools package try the following: Remove sra-tools from the relevant environment: conda remove sra-tools Download the most recent binaries for your operating system from here (example shown for Mac OSX): curl --output sratoolkit.tar.gz https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-mac64.tar.gz Create a temporary directory for the installation: mkdir tmp_out Extract the binary files: tar -C tmp_out -zxvf sratoolkit.tar.gz */bin/* Copy binary files into the conda environment: cp -r tmp_out/*/bin/* $CONDA_PREFIX/bin/ Remove the downloaded files: rm -r sratoolkit.tar.gz tmp_out/","title":"Setup"},{"location":"course-information/pre-course-setup/#setup-for-mac-linux-users","text":"First, cd into a directory on your computer (or create one) where it makes sense to download the course directory. cd /path/to/your/directory git clone https://github.com/NBISweden/workshop-reproducible-research.git cd workshop-reproducible-research Tip If you want to revisit the material from an older instance of this course, you can do that using git checkout tags/<tag-name> , e.g. git checkout tags/course_1905 . To list all available tags, use git tag . Run this command after you have cd into workshop-reproducible-research as described above. If you do that, you probably also want to view the same older version of this website. Until spring 2021, the website was hosted at https://nbis-reproducible-research.readthedocs.io. Locate the version box in the bottom right corner of the website and select the corresponding version.","title":"Setup for Mac / Linux users"},{"location":"course-information/pre-course-setup/#setup-for-windows-users","text":"Using a Windows computer for bioinformatic work has sadly not been ideal most of the time, but large advanced in recent years have made this quite feasible through the Windows 10 Linux subsystem . This is the only setup for Windows users that we allow for participants of this course, as all the material has been created and tested to work on Unix-based systems. Using the Linux subsystem will give you access to a full command-line bash shell based on Linux on your Windows 10 PC. For the difference between the Linux Bash Shell and the PowerShell on Windows 10, see e.g. this article . Install Bash on Windows 10, follow the instructions at e.g. one of these resources: Installing the Windows Subsystem and the Linux Bash Installing and using Linux Bash on Windows Installing Linux Bash on Windows Note If you run into error messages when trying to download files through the Linux shell ( e.g. curl:(6) Could not resolve host ) then try adding the Google nameserver to the internet configuration by running sudo nano /etc/resolv.conf then add nameserver 8.8.8.8 to the bottom of the file and save it. Open a bash shell Linux terminal and clone the GitHub repository containing all files you will need for completing the tutorials as follows. First, cd into a directory on your computer (or create one) where it makes sense to download the course directory. Tip You can find the directory where the Linux distribution is storing all its files by typing explorer.exe . . This will launch the Windows File Explorer showing the current Linux directory. Alternatively, you can find the Windows C drive from within the bash shell Linux terminal by navigating to /mnt/c/ . cd /path/to/your/directory git clone https://github.com/NBISweden/workshop-reproducible-research.git cd workshop-reproducible-research Whenever a setup instruction specifies Mac or Linux ( i.e. only those two, with no alternative for Windows), please follow the Linux instructions. Tip If you want to revisit the material from an older instance of this course, you can do that using git checkout tags/<tag-name> , e.g. git checkout tags/course_1905 . To list all available tags, use git tag . Run this command after you have cd into workshop-reproducible-research as described above. If you do that, you probably also want to view the same older version of this website. Until spring 2021, the website was hosted at https://nbis-reproducible-research.readthedocs.io/en/latest/. Locate the version box in the bottom right corner of the website and select the corresponding version.","title":"Setup for Windows users"},{"location":"course-information/pre-course-setup/#installing-git","text":"Chances are that you already have git installed on your computer. You can check by running e.g. git --version . If you don't have git, install it following the instructions here . If you have a very old version of git you might want to update to a later version.","title":"Installing Git"},{"location":"course-information/pre-course-setup/#configure-git","text":"If it is the first time you use git on your computer, you may want to configure it so that it is aware of your username and email. These should match those that you have registered on GitHub. This will make it easier when you want to sync local changes with your remote GitHub repository. git config --global user.name \"Mona Lisa\" git config --global user.email \"mona_lisa@gmail.com\" Tip If you have several accounts ( e.g. both a GitHub and Bitbucket account), and thereby several different usernames, you can configure git on a per-repository level. Change directory into the relevant local git repository and run git config user.name \"Mona Lisa\" . This will set the default username for that repository only. You will also need to configure the default branch name to be main instead of master : git config --global init.defaultBranch \"main\" The short version of why you need to do this is that GitHub uses main as the default branch while Git itself is still using master ; please read the box below for more information. The default branch name The default branch name for Git and many of the online resources for hosting Git repositories has traditionally been master , which historically comes from the \"master/slave\" repositories of BitKeeper . This has been heavily discussed and in 2020 the decision was made by many ( including GitHub ) to start using main instead. Any repository created with GitHub uses this new naming scheme since October of 2020, and Git itself is currently discussing implementing a similar change. Git did, however, introduce the ability to set the default branch name when using git init in version 2.28 , instead of using a hard-coded master . We at NBIS want to be a part of this change, so we have chosen to use main for this course.","title":"Configure git"},{"location":"course-information/pre-course-setup/#installing-conda","text":"Conda is installed by downloading and executing an installer from the Conda website, but which version you need depends on your operating system: # Install Miniconda3 for 64-bit Mac curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-MacOSX-x86_64.sh -O bash Miniconda3-4.7.12.1-MacOSX-x86_64.sh rm Miniconda3-4.7.12.1-MacOSX-x86_64.sh # Install Miniconda3 for 64-bit Linux curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O bash Miniconda3-4.7.12.1-Linux-x86_64.sh rm Miniconda3-4.7.12.1-Linux-x86_64.sh Attention! If you already have installed Conda but want to update, you should be able to simply run conda update conda and subsequently conda init , and skip the installation instructions below. Apple M1 Chips If you have a newer Apple computer with an M1 chip, make sure you have installed Rosetta before you run the installer. If you want to more fully utilise the new architecture, head over to Miniforge ! The installer will ask you questions during the installation: Do you accept the license terms? (Yes) Do you accept the installation path or do you want to choose a different one? (Probably yes) Do you want to run conda init to setup Conda on your system? (Yes) Restart your shell so that the settings in ~/.bashrc / ~/.bash_profile can take effect. You can verify that the installation worked by running: conda --version Different Condas There are three Conda-related things you may have encountered: the first is Conda , the package and environment manager we've been talking about so far. Second is Miniconda , which is the installer for Conda. The third is Anaconda , which is a distribution of not only Conda, but also over 150 scientific Python packages. It's generally better to stick with only Conda, i.e. installing with Miniconda, rather than installing 3 GB worth of packages you may not even use.","title":"Installing Conda"},{"location":"course-information/pre-course-setup/#configuring-conda","text":"Lastly, we will setup the default channels (from where packages will be searched for and downloaded if no channel is specified). conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge","title":"Configuring Conda"},{"location":"course-information/pre-course-setup/#installing-snakemake","text":"We will use Conda environments for the set up of this tutorial, but don't worry if you don't understand exactly what everything does - you'll learn all the details at the course. First make sure you're currently situated inside the tutorials directory ( workshop-reproducible-research/tutorials ) and then create the Conda environment like so: conda env create -f snakemake/environment.yml -n snakemake-env conda activate snakemake-env Check that Snakemake is installed correctly, for example by executing snakemake --help . This should output a list of available Snakemake settings. If you get bash: snakemake: command not found then you need to go back and ensure that the Conda steps were successful. Once you've successfully completed the above steps you can deactivate your Conda environment using conda deactivate and continue with the setup for the other tools.","title":"Installing Snakemake"},{"location":"course-information/pre-course-setup/#installing-nextflow","text":"We'll use Conda to install Nextflow as well: navigate to workshop-reproducible-research/tutorials and create the Conda environment: conda env create -f nextflow/environment.yml -n nextflow-env conda activate nextflow-env Check that Nextflow was installed correctly by running nextflow -version . Once you've successfully completed the installation you can deactive the environment using conda deactivate and continue with the other setups, as needed.","title":"Installing Nextflow"},{"location":"course-information/pre-course-setup/#installing-r-markdown","text":"We also use Conda to install R Markdown: make sure your working directory is in the tutorials directory ( workshop-reproducible-research/tutorials ) and install the necessary R packages defined in the environment.yml : conda env create -f rmarkdown/environment.yml -n rmarkdown-env You can then activate the environment followed by running RStudio in the background from the command line: conda activate rmarkdown-env rstudio & The sluggishness of Conda Some environments are inherently quite complicated in that they have many and varied dependencies, meaning that the search space for the entire dependency hierarchy becomes huge - leading to slow and sluggish installations. This is often the case for R environments. This can be improved by using Mamba, a faster wrapper around Conda. Simply run conda install -n base mamba to install Mamba in your base environment, and replace any conda command with mamba - except activating and deactivating environments, which still needs to be done using Conda. Once you've successfully completed the above steps you can deactivate your Conda environment using conda deactivate and continue with the setup for the other tools. Windows users In case you are having trouble installing R and RStudio using Conda, both run well directly on Windows and you may therefore want to install Windows versions of these software for this tutorial (if you haven't done so already). Conda is, however, the recommended way. If you're having issues with graphical applications, please have a look at this website ; scroll down to the \"Graphical applications\". RStudio and Conda In some cases RStudio doesn't play well with Conda due to differing libpaths. The first and simplest thing to try is to always start RStudio from the command line ( rstudio & ). If you're still having issues, check the available library path by .libPaths() to make sure that it points to a path within your Conda environment. It might be that .libPaths() shows multiple library paths, in which case R packages will be searched for by R in all these locations. This means that your R session will not be completely isolated in your Conda environment and that something that works for you might not work for someone else using the same Conda environment, simply because you had additional packages installed in the second library location. One way to force R to just use the conda library path is to add a .Renviron file to the directory where you start R with these lines: ``` R_LIBS_USER=\"\" R_LIBS=\"\" ``` ... and restart RStudio. The rmarkdown/ directory in the course materials already contains this file, so you shouldn't have to add this yourself, but we mention it here for your future projects.","title":"Installing R Markdown"},{"location":"course-information/pre-course-setup/#installing-jupyter","text":"Let's continue using Conda for installing software, since it's so convenient to do so! Move in the tutorials directory ( workshop-reproducible-research/tutorials ), create a Conda environment from the jupyter/environment.yml file and test the installation of Jupyter, like so: conda env create -f jupyter/environment.yml -n jupyter-env conda activate jupyter-env Once you've successfully completed the above steps you can deactivate your Conda environment using conda deactivate and continue with the setup for the other tools.","title":"Installing Jupyter"},{"location":"course-information/pre-course-setup/#installing-docker","text":"Installing Docker is quite straightforward on Mac or Windows and a little more cumbersome on Linux. Note that Docker runs as root, which means that you have to have sudo privileges on your computer in order to install or run Docker. When you have finished installing docker, regardless of which OS you are on, please type docker --version to verify that the installation was successful!","title":"Installing Docker"},{"location":"course-information/pre-course-setup/#macos","text":"Go to docker.com and select download option that is suitable for your computer's architecture ( i.e. if you have an Intel chip or a newer Apple M1 chip). This will download a dmg file - click on it when it's done to start the installation. This will open up a window where you can drag the Docker.app to Applications. Close the window and click the Docker app from the Applications menu. Now it's basically just to click \"next\" a couple of times and we should be good to go. You can find the Docker icon in the menu bar in the upper right part of the screen.","title":"macOS"},{"location":"course-information/pre-course-setup/#linux","text":"How to install Docker differs a bit depending on your Linux distribution, but the steps are the same. Please follow the instructions for your distribution on https://docs.docker.com/engine/install/#server . Tip As mentioned before, Docker needs to run as root. You can achieve this by prepending all Docker commands with sudo . This is the approach that we will take in this tutorial, since the set up becomes a little simpler that way. If you plan on continuing using Docker you can get rid of this by adding your user to the group docker . Here are instructions for how to do this: https://docs.docker.com/engine/installation/linux/linux-postinstall/ .","title":"Linux"},{"location":"course-information/pre-course-setup/#windows","text":"In order to run Docker on Windows your computer must support Hardware Virtualization Technology and virtualization must be enabled. This is typically done in BIOS. Setting this is outside the scope of this tutorial, so we'll simply go ahead as if though it's enabled and hope that it works. On Windows 10 we will install Docker for Windows, which is available at docker.com . Click the link Download from Docker Hub , and select Get Docker . Once the download is complete, execute the file and follow the instructions . You can now start Docker from the Start menu. You can search for it if you cannot find it; the Docker whale icon should appear in the task bar. You will probably need to enable integration with the Linux subsystem, if you haven't done so during the installation of Docker Desktop. Right-click on the Docker whale icon in the task bar and select Settings . Choose Resources and select WPS integration . Enable integration with the Linux subsystem and click Apply & Restart ; also restart the Linux subsystem.","title":"Windows"},{"location":"course-information/pre-course-setup/#installing-singularity","text":"Installation of Singularity depends, again, on your operating system. When you have finished, regardless of your OS, please type singularity --version to verify that your installation was successful! Both Mac and Windows utilise Vagrant, for which the information in the box below may help you. Vagrant and VirtualBox The Vagrant VirtualBox with Singularity can be started like this: Move into the folder vm-singularity where you installed Singularity. Type vagrant up and once this has finished, verify that the Vagrant VirtualBox is running with vagrant status . Now, type vagrant ssh , which will open the Vagrant VirtualBox. The first time you open the Vagrant VirtualBox like this, you will have to download the course material to obtain a copy for the Singularity tutorial within the Vagrant VirtualBox by typing git clone https://github.com/NBISweden/workshop-reproducible-research.git .","title":"Installing Singularity"},{"location":"course-information/pre-course-setup/#macos_1","text":"Please follow the Mac-specific instructions at the Singularity website .","title":"macOS"},{"location":"course-information/pre-course-setup/#linux_1","text":"Follow the Linux-specific instruction at the Singularity website .","title":"Linux"},{"location":"course-information/pre-course-setup/#windows_1","text":"Please follow the Windows-specific instructions at the Singularity website . Notes Last time we checked, the software \"Vagrant Manager\" was not available for download but the installation of Singularity was successful even without it. Version 6.1.28 of \"Virtual box for Windows\" may not work, please install version 6.1.26 from here in case you encounter problems when trying to start the Vagrant VirtualBox.","title":"Windows"},{"location":"course-information/pre-course-setup/#testing-sra-tools","text":"On some computers we've found that the package sra-tools which is used in the course is not working properly. The error seems to be related to some certificate used to communicate with remote read archives and may affect all environments with sra-tools on the dependency list. If you run into errors with the program fastq-dump from the sra-tools package try the following: Remove sra-tools from the relevant environment: conda remove sra-tools Download the most recent binaries for your operating system from here (example shown for Mac OSX): curl --output sratoolkit.tar.gz https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-mac64.tar.gz Create a temporary directory for the installation: mkdir tmp_out Extract the binary files: tar -C tmp_out -zxvf sratoolkit.tar.gz */bin/* Copy binary files into the conda environment: cp -r tmp_out/*/bin/* $CONDA_PREFIX/bin/ Remove the downloaded files: rm -r sratoolkit.tar.gz tmp_out/","title":"Testing sra-tools"},{"location":"course-information/schedule/","text":"Day 1 Time Topic Teacher 09:00 Setting up JS, JW, LM, TL 10:00 Introduction to Reproducible Research EF 10:30 Break 10:45 Data management and project organization JS 11:15 Break-out rooms and ice breaker session JS, JW, LM, TL 11:30 Distributing and version tracking your code - Introduction to version control and Git - Practical tutorial: Git JS 12:00 Lunch 13:00 ... continued: Git tutorial JS, JW, LM, TL 14:30 Wrap-up day 1 JS Day 2 Time Topic Teacher 09:00 Master your dependencies - environments and reproducibility - Introduction to the package and environment manager Conda - Practical tutorial: Conda JS 10:15 Break 10:30 ... continued: Conda tutorial JS, EP, LM, TL 11:00 Wrap up: Conda tutorial JS 11:15 Organize your analysis using workflow managers - Introduction to Snakemake - Practical tutorial: Snakemake JS 12:00 Lunch 13:00 ... continued: Snakemake tutorial JS, JW, LM, TL 14:30 Wrap-up day 2 JS Day 3 Time Topic Teacher 09:00 ... continued: Snakemake tutorial JS, JW, LM, TL 10:15 Break 10:30 ... continued: Snakemake tutorial JS, JW, LM, TL 11:00 Wrap-up: Snakemake tutorial JS 11:15 Organize your analysis using workflow managers - Introduction to Nextflow - Practical tutorial: Nextflow EF 12:00 Lunch 13:00 ... continued: Nextflow tutorial JS, JW, LM, TL, EF 14:30 Wrap-up day 3 EF Day 4 Time Topic Teacher 09:00 ... continued: Nextflow tutorial JS, JW, LM, TL, EF 10:00 Wrap-up: Nextflow tutorial EF 10:15 Break 10:30 Computational notebooks and reproducible reports - Introduction to R Markdown - Practical tutorial: R Markdown JS 12:00 Lunch 13:00 Computational notebooks and reproducible reports - Introduction to Jypyter - Practical tutorial: Jupyter JS 14:30 Wrap-up day 4 JS Day 5 Time Topic Teacher 09:00 Containerization - Introduction to containers - Practical tutorial: Containers JS 10:30 Break 10:45 ... continued: Containers JS, JW, LM, TL 12:15 Lunch 13:15 Putting the pieces together - How to put all the tools and procedures together - How to implement these procedures on a day-to-day basis JS 14:30 End of the course! JS The above schedule is approximate; variations may occur. EF Erik Fasterius JS John Sundh VK Verena Kutschera TL Tomas Larsson JW Jakub Westholm LM Lokesh Manoharan EP Estelle Proux-W\u00e9ra","title":"Schedule"},{"location":"course-information/take-down/","text":"There might be a lot of files stored on your computer after you've taken the course, depending on how many modules you've gone through. Here are instructions for how to remove them. All the tutorials depend on you cloning the workshop-reproducible-research GitHub repo. This can be removed like any other directory; via Finder, Explorer or rm -rf workshop-reproducible-research . Note that this will also delete the hidden directories .git , which contains the history of the repo, and .snakemake , which contains the history of any Snakemake runs. Conda # Several of the tutorials use Conda for installing packages. This amounts to about 2.6 GB if you've done all the tutorials. If you plan on using Conda in the future you can remove just the packages, or you can remove everything including Conda itself. In order to remove all your Conda environments, you first need to list them: conda env list For each of the environments except \"base\" run the following: conda remove -n envname --all And, finally: conda clean --all If you also want to remove Conda itself ( i.e. removing all traces of Conda), you need to check where Conda is installed. Look for the row \"base environment\". conda info This should say something like /Users/<user>/miniconda3 . Then remove the entire Conda directory: rm -rf /Users/<user>/miniconda3 Lastly, open your ~/.bashrc file (or ~/.bash_profile if on Mac) in a text editor and remove the path to Conda from PATH. Snakemake # Snakemake is installed via Conda and will be removed if you follow the instructions in the Conda section above. Note that Snakemake also generates a hidden .snakemake directory in the directory where it's run. You can remove this with the following: rm -rf workshop-reproducible-research/tutorials/snakemake/.snakemake Nextflow # Since we installed Nextflow using Conda we can remove it in the same way as above. You may also want to remove the results/ and work/ directories, which you can do like so: rm -rf workshop-reproducible-research/tutorials/nextflow/results rm -rf workshop-reproducible-research/tutorials/nextflow/work Jupyter # Jupyter is installed via Conda and will be removed if you follow the instructions in the Conda section above. Docker # Docker is infamous for quickly taking up huge amounts of space, and some maintenance is necessary every now and then. Here is how to uninstall Docker completely. Let's start by removing individual images and containers: # Remove unused images docker image prune # Remove stopped containers docker container prune # Remove unused volumes (not used here, but included for reference) docker volume prune # Stop and remove ALL containers docker container rm $(docker container ls -a -q) # Remove ALL images docker image rm $(docker image ls -a -q) Removing Docker itself works differently on the three operating systems, which is described below: macOS # Click the Docker icon in the menu bar (upper right part of the screen) and select \"Preferences\". In the upper right corner, you should find a little bug icon. Click on that icon and select \"Reset to factory defaults\". You may have to fill in your password. Then select \"Uninstall\". Once it's done uninstalling, drag the Docker app from Applications to Trash. Linux # If you've installed Docker with apt-get , uninstall it like this: apt-get purge docker-ce Images, containers, and volumes are not automatically removed. To delete all of them: rm -rf /var/lib/docker Windows # Uninstall Docker for Windows (on Windows 10) or Docker Toolbox (on Windows 7) via Control Panel > Programs > Programs and Features. Docker Toolbox will also have installed Oracle VM VirtualBox, so uninstall that as well if you're not using it for other purposes. Singularity # Singularity images are files that can simply be deleted. Singularity also creates a hidden directory .singularity in your home directory that contains its cache, which you may delete. Linux # If you want to uninstall Singularity, its removal will depend on the installation method you chose. Please refer to the Singularity installation guide to find out which tools and dependencies you need to uninstall. Mac # The DMG file you downloaded to install Singularity Desktop on your Mac contains an uninstall tool that you can use. Windows # On Windows, you will need to uninstall Git for Windows, VirtualBox, Vagrant and Vagrant Manager (see the Singularity installation guide ).","title":"Take down"},{"location":"course-information/take-down/#conda","text":"Several of the tutorials use Conda for installing packages. This amounts to about 2.6 GB if you've done all the tutorials. If you plan on using Conda in the future you can remove just the packages, or you can remove everything including Conda itself. In order to remove all your Conda environments, you first need to list them: conda env list For each of the environments except \"base\" run the following: conda remove -n envname --all And, finally: conda clean --all If you also want to remove Conda itself ( i.e. removing all traces of Conda), you need to check where Conda is installed. Look for the row \"base environment\". conda info This should say something like /Users/<user>/miniconda3 . Then remove the entire Conda directory: rm -rf /Users/<user>/miniconda3 Lastly, open your ~/.bashrc file (or ~/.bash_profile if on Mac) in a text editor and remove the path to Conda from PATH.","title":"Conda"},{"location":"course-information/take-down/#snakemake","text":"Snakemake is installed via Conda and will be removed if you follow the instructions in the Conda section above. Note that Snakemake also generates a hidden .snakemake directory in the directory where it's run. You can remove this with the following: rm -rf workshop-reproducible-research/tutorials/snakemake/.snakemake","title":"Snakemake"},{"location":"course-information/take-down/#nextflow","text":"Since we installed Nextflow using Conda we can remove it in the same way as above. You may also want to remove the results/ and work/ directories, which you can do like so: rm -rf workshop-reproducible-research/tutorials/nextflow/results rm -rf workshop-reproducible-research/tutorials/nextflow/work","title":"Nextflow"},{"location":"course-information/take-down/#jupyter","text":"Jupyter is installed via Conda and will be removed if you follow the instructions in the Conda section above.","title":"Jupyter"},{"location":"course-information/take-down/#docker","text":"Docker is infamous for quickly taking up huge amounts of space, and some maintenance is necessary every now and then. Here is how to uninstall Docker completely. Let's start by removing individual images and containers: # Remove unused images docker image prune # Remove stopped containers docker container prune # Remove unused volumes (not used here, but included for reference) docker volume prune # Stop and remove ALL containers docker container rm $(docker container ls -a -q) # Remove ALL images docker image rm $(docker image ls -a -q) Removing Docker itself works differently on the three operating systems, which is described below:","title":"Docker"},{"location":"course-information/take-down/#macos","text":"Click the Docker icon in the menu bar (upper right part of the screen) and select \"Preferences\". In the upper right corner, you should find a little bug icon. Click on that icon and select \"Reset to factory defaults\". You may have to fill in your password. Then select \"Uninstall\". Once it's done uninstalling, drag the Docker app from Applications to Trash.","title":"macOS"},{"location":"course-information/take-down/#linux","text":"If you've installed Docker with apt-get , uninstall it like this: apt-get purge docker-ce Images, containers, and volumes are not automatically removed. To delete all of them: rm -rf /var/lib/docker","title":"Linux"},{"location":"course-information/take-down/#windows","text":"Uninstall Docker for Windows (on Windows 10) or Docker Toolbox (on Windows 7) via Control Panel > Programs > Programs and Features. Docker Toolbox will also have installed Oracle VM VirtualBox, so uninstall that as well if you're not using it for other purposes.","title":"Windows"},{"location":"course-information/take-down/#singularity","text":"Singularity images are files that can simply be deleted. Singularity also creates a hidden directory .singularity in your home directory that contains its cache, which you may delete.","title":"Singularity"},{"location":"course-information/take-down/#linux_1","text":"If you want to uninstall Singularity, its removal will depend on the installation method you chose. Please refer to the Singularity installation guide to find out which tools and dependencies you need to uninstall.","title":"Linux"},{"location":"course-information/take-down/#mac","text":"The DMG file you downloaded to install Singularity Desktop on your Mac contains an uninstall tool that you can use.","title":"Mac"},{"location":"course-information/take-down/#windows_1","text":"On Windows, you will need to uninstall Git for Windows, VirtualBox, Vagrant and Vagrant Manager (see the Singularity installation guide ).","title":"Windows"},{"location":"git/git-1-introduction/","text":"Git is a widely used system (both in academia and industry) for version controlling files and collaborating on code. It is used to track changes in (text) files, thereby establishing a history of all edits made to each file, together with short messages about each change and information about who made it. Git is mainly run from the command line, but there are several tools that have implemented a graphical user interface to run Git commands. Using version control for tracking your files, and edits to those, is an essential step in making your computational research reproducible. A typical Git workflow consists of: Making distinct and related edits to one or several files Committing those changes ( i.e. telling Git to add those edits to the history, together with a message about what those changes involve) Pushing the commit to a remote repository ( i.e. syncing your local project directory with one in the cloud) There are many benefits of using Git in your research project: You are automatically forced into a more organized way of working, which is usually a first step towards reproducibility. If you have made some changes to a file and realize that those were probably not a good idea after all, it is simple to view exactly what the changes were and revert them. If there is more than one person involved in the project, Git makes it easy to collaborate by tracking all edits made by each person. It will also handle any potential conflicting edits. Using a cloud-based repository hosting service (the one you push your commits to), like e.g. GitHub or Bitbucket , adds additional features, such as being able to discuss the project, comment on edits, or report issues. If at some point your project will be published GitHub or Bitbucket (or similar) are excellent places to publicly distribute your code. Other researchers can then use Git to access the code needed for reproducing your results, in exactly the state it was when used for the publication. If needed, you can host private repositories on GitHub and Bitbucket as well. This may be convenient during an ongoing research project, before it is publicly published. These tutorials will walk you through the basics of using Git as a tool for reproducible research. The things covered in these tutorials are what you will be using most of the time in your day-to-day work with Git, but Git has many more advanced features that might be of use to you. This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already.","title":"Introduction"},{"location":"git/git-2-creating-repositories/","text":"In order to create a new Git repository, we first need a directory to track. For this tutorial, go ahead and create a directory called git_tutorial , then navigate into it. Attention! The directory should not be within the workshop-reproducible-research directory, since this is itself a Git-tracked directory. Once we are inside the desired directory, we can initialise Git with the following command: git init The directory is now a version-tracked directory. How can you know? Run the command git status , which will probably return something like this: On branch main No commits yet nothing to commit (create/copy files and use \"git add\" to track) Tip If you try to run git status in a non-Git directory, it will say that it is not a git repository . The way this works is that Git adds a hidden directory .git/ in the root of a Git tracked directory (run ls -a to see it). This hidden directory contains all information and settings Git needs in order to run and version track your files. This also means that your Git-tracked directory is self-contained, i.e. you can simply delete it and everything that has to do with Git in connection to that directory will be gone. The text nothing to commit (create/copy files and use \"git add\" to track) tells us that while we are inside a directory that Git is currently tracking, there are currently no files being tracked; let's add some! Copy the following files from the workshop-reproducible-research/tutorials/git directory into your git_tutorial directory: Dockerfile Snakefile config.yml environment.yml Once you have done that, run git status again. It will tell you that there are files in the directory that are not version tracked by Git. Note For the purpose of this tutorial, the exact contents of the files you just copied are not important. But you will probably recognize many of them, as they are all files used in the MRSA case study described in the introduction to the tutorials . The details of what these files do are described in their respective sessions later in the course, but we provide a brief overview here: The environment.yml file contains the Conda environment with all the software used in the analysis (see the Conda tutorial ). The Snakefile and config.yml are both used to define the Snakemake workflow, that we'll go through in the Snakemake tutorial . The Dockerfile contains the recipe for making a Docker container for the analysis, which will be covered in detail in the Container tutorial . Quick recap We have used two git commands this far: git init tells Git to track the current directory. git status is a command you should use a lot . It will tell you, amongst other things, the status of your Git clone in relation to the online remote repository.","title":"Creating a git repository"},{"location":"git/git-3-committing-changes/","text":"We will now commit the untracked files. A commit is essentially a set of changes to a set of files. Preferably, the changes making out a commit should be related to something, e.g. a specific bug fix or a new feature. Our first commit will be to add the copied files to the repository. Run the following (as suggested by git status ): git add Dockerfile Snakefile Run git status again! See that we have added Dockerfile and Snakefile to our upcoming commit (listed under \" Changes to be committed \"). This is called the staging area, and the files there are staged to be committed. We might as well commit all files in one go! Use git add on the remaining files as well: git add config.yml environment.yml Run git status and see that all files are in the staging area, and that no files are listed as untracked. We are now ready to commit! Run the following: git commit -m \"Add initial files\" The -m option adds a commit message. This should be a short description of what the commit contains. Good commit messages Writing informative and succinct commit messages can be tricky when you're just starting out. Here are some general guidelines that can help you write good commit messages from the start: Separate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs. how In the command above we just added a short subject line (\"Add initial files\"). It is capitalized, less than 50 characters, does not end with a period, and uses imperative mood (Add!). It is possible to add a descriptive body text as well, as hinted by the points above. This is easiest done in a text editor. If you run git commit without the -m flag, Git will open the default terminal text editor (which can be configured with the core.editor variable) where you can write a longer commit message and body. If you want to read more about the motivation for these points, please see this website . Run git status again. It should tell you \"nothing to commit, working directory clean\" . What have we done, so far? We had some files in our working directory that we added to the Git staging area, which we subsequently committed to our Git repository. A schematic overview of this process can be seen in the following figure: { width=600px } Let's repeat this process by editing a file! Open up environment.yml in your favorite editor, and change the version of bowtie2 to a different value, e.g. bowtie2=2.2.4 . Run git status . It will tell you that there are modifications in one file ( environment.yml ) compared to the previous commit. This is nice! We don't have to keep track of which files we have edited, Git will do that for us. Run git diff environment.yml . This will show you the changes made to the file. A - means a deleted line, a + means an added line. There are also shown a few lines before and after the changes, to put them in context. Let's edit another file! Open config.yml and change the line genome_id: NCTC8325 to genome_id: ST398 . Run git status . Run git diff . If we don't specify a file, it will show all changes made in any file, compared to the previous commit. Do you see your changes? Ok, we made our changes. Let's commit them! Run: git add config.yml environment.yml This will add both our files to the staging area at the same time. Run git status and see that the changes in both config.yml and environment.yml are ready to be committed. But wait a minute! Shouldn't each commit optimally be a conceptual unit of change? Here we have one change to the genome ID used for an analysis and one change where another software version is specified: these should probably be separate. We thus want to make two commits, one for each change. Let's remove environment.yml from the staging area. git status tells us how to do this: \"(use \"git reset HEAD ...\" to unstage)\" . So run: git reset HEAD environment.yml Note Maybe you didn't see the same message as indicated above? Is Git telling you to use a git restore instead? This is another one of Git's newer and experimental commands, which aims to remove some confusion about what commands do what (as many have multiple functions). While we have opted to stick with the old and stable commands until the new commands are no longer considered experimental, you are very welcome to use git restore instead of git reset to unstage the file above! Run git status again. See that now only config.yml is staged for being committed, whereas the changes in environment.yml are tracked by Git, but not ready to be committed. Commit the changes in config.yml : git commit -m \"Change to ST398 for alignment\" Add and commit the changes in environment.yml : git status git add environment.yml git status git commit -m \"Change bowtie2 version\" git status You don't have to run git status between each command, but it can be useful in the beginning while learning what each command does. As you can see, each commit is a point in history. The more often you commit, and the more specific you keep your commits, the better (more fine-grained) history and version tracking you will have of your files. We can also try to delete a file: rm Dockerfile Run git status . As you can see, Git tells us that the file is deleted, but that the deletion is not committed. In the same way as we commit edits to files, we need to commit a deletion of a file: git add Dockerfile git status git commit -m \"Remove Dockerfile\" git status git log Here we used rm Dockerfile to delete the file and git add Dockerfile to stage the deletion. You can also use git rm Dockerfile to do both these operations in one step. To see a history of our changes so far, run: git log Tip Since Git keeps track of changes in text, e.g. code and text-based documentation, there are some files which you should not commit. Examples of such files are file formats that are not text-based, e.g. Microsoft Word/Excel files or PDFs - although one might sometimes want to track one of these files regardless, such as when you have a static PDF report you received from a sequencing platform that's never going to change. Other files you shouldn't track are vary large text files, e.g. those larger than 50 MB. Quick recap We added four important Git commands to our repertoire: git add adds a file to the staging area git commit commits the changes we have staged git rm is shorthand for rm <file>; git add <file> git log shows us the commit history","title":"Committing changes"},{"location":"git/git-4-ignoring-files/","text":"Git is aware of all files within the repository. However, it is not uncommon to have files that we don't want Git to track. For instance, our analysis might produce several intermediate files and results. We typically don't track such files. Rather, we want to track the actual code and other related files ( e.g. configuration files) that produce the intermediate and result files, given the raw input data. Let's make some mock-up intermediate and result files. These are some of the files that would have been generated by the Snakemake workflow if it was run. mkdir intermediate mkdir results touch intermediate/multiqc_general_stats.txt touch results/supplementary.pdf touch log.tmp Run git status . You will see that Git tells you that you have untracked files. However, we don't want Git to track these files anyway. To tell Git what files to ignore we use a file called .gitignore . Let's create it: touch .gitignore Open the .gitignore file in a text editor and add the following lines to it: # Ignore these directories: results/ intermediate/ # Ignore temporary files: *.tmp Run git status again. Now there is no mention of the results and intermediate directories or the log.tmp file. Notice that we can use wildcards (*) to ignore files with a given pattern, e.g. a specific file extension. Sometimes you want to ignore all files in a directory with one or two exceptions. For example, you don't want to track all your huge raw data files, but there may be a smaller data file that you do want to track, e.g. metadata or a list of barcodes used in your experiment. Let's add some mock data: mkdir data touch data/huge.fastq.gz touch data/metadata.txt Git allows you to ignore all files using the aforementioned wildcard, but then exclude certain files from that ignore command. Open the .gitignore file again and add the following: # Ignore all files in the data/ directory data/* # Exclude the metadata file by prefixing it with an exclamation mark !data/metadata.txt Finish up by adding the .gitignore and data/metadata.txt files to the staging area and committing them: git add .gitignore git commit -m \"Add .gitignore file\" git add data/metadata.txt git commit -m \"Add metadata file\" Tip It is common for certain programming languages or text editors to leave e.g. swap files or hidden data files in the working directory, which you don't want to track using Git. Instead of manually adding these to every single project you have, you can use the .gitignore_global file, which should be placed in your home directory. It works exactly like a normal gitignore file, but is applied to all Git repositories that you are using on your machine. Some common file extensions that might be put in the global gitignore are .DS_Store if you're working in R or .swp if you're coding in vim. To configure git to use the .gitignore_global file you can run git config --global core.excludesfile ~/.gitignore_global . Quick recap We now learned how to ignore certain files and directories: The .gitignore file controls which files and directories Git should ignore, if any. Specific files can be excluded from ignored directories using the exclamation mark ( ! ) prefix.","title":"Ignoring files"},{"location":"git/git-5-branches/","text":"One of the most useful features of Git is called branching . Branching allows you to diverge from the main line of work and edit or update your code and files ( e.g. to test out a new analysis or some experimental feature) without affecting your main work. If the work you did in the branch turns out to be useful you can merge that back into your main branch. On the other hand, if the work didn't turn out as planned, you can simply delete the branch and continue where you left off in your main line of work. Another use case for branching is when you are working in a project with multiple people. Branching can be a way of compartmentalizing your team's work on different parts of the project and enables merging back into the main branch in a controlled fashion; we will learn more about this in the section about working remotely. Let's start trying out branching! We can see the current branch by running: git branch This tells us that there is only the main branch at the moment. Main and Master If your branch is called master instead of main that's perfectly fine as well, but do check out the Git section of the pre-course setup for more details about the choice of default branch names. Let's make a new branch: git branch test_alignment Run git branch again to see the available branches. Do you note which one is selected as the active branch? Let's move to our newly created branch using the checkout command: git checkout test_alignment Tip You can create and checkout a new branch in one line with git checkout -b branch_name . Let's add some changes to our new branch! We'll use this to try out a different set of parameters on the sequence alignment step of the case study project. Edit the Snakefile so that the shell command of the align_to_genome rule looks like this (add the --very-sensitive-local option): bowtie2 --very-sensitive-local -x $indexBase -U {input.fastq} > {output} 2> {log} Add and commit the change! To get a visual view of your branches and commits you can use the command: git log --graph --all --oneline It is often useful to see what differences exist between branches. You can use the diff command for this: git diff main This shows the difference between the active branch ( test_alignment ) and main on a line-per-line basis. Do you see which lines have changed between test_alignment and main branches? Tip We can also add the --color-words flag to git diff , which instead displays the difference on a word-per-word basis rather than line-per-line. Note Git is constantly evolving, along with some of its commands. While the checkout command is quite versatile (it's used for more than just switching branches), this versatility can sometimes be confusing. The Git team thus added a new command, git switch , that can be used instead. This command is still experimental, however, so we have opted to stick with checkout for the course - for now. Now, let's assume that we have tested our code and the alignment analysis is run successfully with our new parameters. We thus want to merge our work into the main branch. It is good to start with checking the differences between branches (as we just did) so that we know what we will merge. Checkout the branch you want to merge into, i.e. main : git checkout main To merge, run the following code: git merge test_alignment Run git log --graph --all --oneline again to see how the merge commit brings back the changes made in test_alignment to main . Tip If working on different features or parts of an analysis on different branches, and at the same time maintaining a working main branch for the stable code, it is convenient to periodically merge the changes made to main into relevant branches ( i.e. the opposite to what we did above). That way, you keep your experimental branches up-to-date with the newest changes and make them easier to merge into main when time comes. If we do not want to do more work in test_alignment we can delete that branch: git branch -d test_alignment Run git log --graph --all --oneline again. Note that the commits and the graph history are still there? A branch is simply a pointer to a specific commit, and that pointer has been removed. Tip There are many types of so-called \"branching models\", each with varying degrees of complexity depending on the developer's needs and the number of collaborators. While there certainly isn't a single branching model that can be considered to be the \"best\", it is very often most useful to keep it simple. An example of a simple and functional model is to have a main branch that is always working ( i.e. can successfully run all your code and without known bugs) and develop new code on feature branches (one new feature per branch). Feature branches are short-lived, meaning that they are deleted once they are merged into main . Quick recap We have now learned how to divide our work into branches and how to manage them: git branch <branch> creates a new branch. git checkout <branch> moves the repository to the state in which the specified branch is currently in. git merge <branch> merges the specified branch into the current one.","title":"Branching and merging"},{"location":"git/git-6-tags/","text":"Git allows us to tag commits, i.e. give names to specific points in the history of our project. This can be particularly important for reproducible research, but also for development projects that want to highlight specific versions of a software. A tag can be, for example, the version of the repository that was used for the manuscript submission, the version used during resubmission, and, most importantly, the version used for the final publication. The first two examples are mainly useful internally, but the latter is essential for other researchers to be able to rerun your published analysis. Let's assume that the status of the repository as it is now is ready for a submission to a journal. It may for example contain the scripts that were used to generate the manuscript figures. Let's add a tag: git tag \"submission1\" We can now list all the tags available in the current repository: git tag Tip You can use the flag -a or --annotate to give more detailed information about a specific tag, similar to a commit message. This can be quite useful when there are many changes that happened, in that it allows you to summarise them. You can, for example, do git tag -a submission1 -m \"Annotation for tag submission1\" to write the annotation along with the command (similar to the -m flag for committing) or just git tag -a submission1 to write the annotation with your default editor. To list all your tags along with their annotations you can use e.g. git tag -n10 (which will list the first 10 lines of each tag's annotation). Let's assume we now got comments from the reviewers, and by fixing those we had to update our code. Open config.yml and change the line max_reads: 25000 to max_reads: 50000 . Commit and tag the changes: git add config.yml git commit -m \"Increase number of reads\" git tag \"revision1\" Now let's say that the reviewers were happy and the manuscript was accepted for publication. Let's immediately add a tag: git tag \"publication\" A good thing about using tags is that you can easily switch between versions of your code. Let's move to the first submission version: git checkout submission1 Open config.yml and note that the max_reads variable is 25000 ! To go back to the latest version, run: git checkout main Open config.yml and see that the value is now 50000 . Tip You can also see the difference between tags in the same way as for branches and commits using e.g. git diff <tag1> <tag2> . At this point could run git log --oneline --decorate to get a condensed commit history, where you should also be able to see the tagged commits. Quick recap We have now learned how to tag important commits: git tag adds a tag to a commit. git checkout moves between tags in a similar fashion as between branches.","title":"Tagging commits"},{"location":"git/git-7-working-remotely/","text":"So far we've only been working on files present on our own computer, i.e. locally. While Git is an amazing tool for reproducibility even if you're working alone, it really starts to shine in collaborative work. This entails working with remote repositories, i.e. repositories that are stored somewhere online; some of the most common places to store your repositories are GitHub , BitBucket and GitLab . GitHub is the most popular of these, and is what we'll be using for this tutorial. An important thing to keep in mind here is the difference between Git (the version control system) and online hosting of Git repositories (such as GitHub): the former is the core of keeping track of your code's history, while the latter is how to store and share that history with others. GitHub setup # If you have not done so already, go to github.com and create an account. You can also create an account on another online hosting service for version control, e.g. Bitbucket or GitLab . The exercises below are written with examples from GitHub (as that is the most popular platform with the most extensive features), but the same thing can be done on alternative services, although the exact menu structure and link placements differ a bit. Any upload to and from GitHub requires you to authenticate yourself. GitHub used to allow authentication with your account and password, but this is no longer the case - using SSH keys is favoured instead. Knowing exactly what these are is not necessary to get them working, but we encourage you to read the box below to learn more about them! GitHub has excellent, platform-specific instructions both on how to generate and add SSH keys to your account, so please use them before moving on! SSH keys and authentication Using SSH (Secure Shell) for authentication basically entails setting up a pair of keys: one private and one public. You keep the private key on your local computer and give the public key to anywhere you want to be able to connect to, e.g. GitHub. The public key can be used to encrypt messages that only the corresponding private key can decrypt. A simplified description of how SSH authentication works goes like this: The client ( i.e. the local computer) sends the ID of the SSH key pair it would like to use for authentication to the server ( e.g. GitHub) If that ID is found, the server generates a random number and encrypts this with the public key and sends it back to the client The client decrypts the random number with the private key and sends it back to the server Notice that the private key always remains on the client's side and is never transferred over the connection; the ability to decrypt messages encrypted with the public key is enough to ascertain the client's authenticity. This is in contrast with using passwords, which are themselves sent across a connection (albeit encrypted). It is also important to note that even though the keys come in pairs it is impossible to derive the private key from the public key. If you want to read more details about how SSH authentication work you can check out this website , which has more in-depth information than we provide here. Create a remote repository # Log in to your GitHub account and press the New button: Make sure you are listed as the owner Add a repository name, e.g. git_tutorial You can keep the repo private or make it public, as you wish Skip including a README, a .gitignore and licence { width=600px } You will now be redirected to the repository page which will list several ways for you to start adding content (files) to the repository. What we will do is to connect the local repository we've been working on so far to the remote GitHub server using SSH: Add a remote SSH address to your local repository (make sure you change user to your GitHub username and git_tutorial to your repository name): git remote add origin git@github.com:user/git_tutorial.git Run git remote -v . This will show you what remote location is connected to your local Git clone. The short name of the default remote is usually \" origin \" by convention. Note Make sure you've used an SSH address ( i.e. starting with git@github.com rather than an HTTPS address (starting with https://github.com )! We have not yet synced the local and remote repositories, though, we've simply connected them. Let's sync them now: git push origin main The push command sends our local history of the main branch to the same branch on the remote ( origin ). Our Git repository is now stored on GitHub! Run git status . This should tell you that: On branch main nothing to commit, working tree clean You always need to specify git push origin main by default, but you can circumvent this by telling Git that you always want to push to origin/main when you're on your local main branch. To do this, use the command git branch --set-upstream-to origin/main . Try it out now . Now run git-status again. You should see that now git additionally tells you that your local branch is up to date with the remote branch. If you go to the repository's GitHub page you should now be able to see all your files and your code there! It should look something like this: { width=600px } You can see a lot of things there, such as each file and the latest commit that changed them, the repository's branches and a message from GitHub at the bottom: \"Help people interested in this repository understand your project by adding a README.\" This refers to GitHub's built-in functionality of automatically rendering any markdown document named README or README.md in the repository's root directory and displaying it along with what you can already see. Let's try it out! Let's create a README.md file and fill it with the following text: # A Git tutorial This repository contains tutorial information related to the **NBIS/ELIXIR** course *Tools for Reproducible Research*, specifically the session on using the `git` software for version control. ## Links You can find the latest stable version of the Git tutorial for the course [here](https://uppsala.instructure.com/courses/73110/pages/git-1-introduction?module_item_id=367079). Add, commit and push these changes to GitHub. git add README.md git commit -m \"Add README.md\" git push origin main You should now be able to see the rendered markdown document, which looks a bit different from the text you copied in from above. Note that there are two different header levels, which come from the number of hash signs ( # ) used. You can also see bold text (which was surrounded by two asterisks), italic text (surrounded by one asterisk), in-line code (surrounded by acute accents) and a link (link text inside square brackets followed by link address inside parentheses). It is important to add README-files to your repositories so that they are better documented and more easily understood by others and, more likely, your future self. In fact, documentation is an important part of reproducible research! While the tools that you are introduced to by this course are all directly related to making science reproducible, you will also need good documentation. Make it a habit of always adding README-files for your repositories, fully explaining the ideas and rationale behind the project. You can even add README-files to sub-directories as well, giving you the opportunity to go more in-depth where you so desire. Tip There are a lot more things you can do with markdown than what we show here. Indeed, this entire course is mostly written in markdown! You can read more about markdown here . Quick recap We learned how to connect local Git repositories to remote locations such as GitHub and how to upload commits using git push . We also learned the basics of markdown and how it can be used to document Git repositories. Browsing GitHub # GitHub and the rest of the websites that offer remote hosting of git repositories all have numerous features, which can be somewhat difficult to navigate in the beginning. We here go through some of the basics of what you can do with GitHub. Go to your GitHub repository in your browser again and click on Code to the left. Click on config.yml . You will see the contents of the file. Notice that it is the latest version, where we previously changed the genome_id variable: { width=600px } Click on History . You will see an overview of the commits involving changes made to this file: { width=600px } Click on the Change to ST398 for alignment commit. You will see the changes made to config.yml file compared to the previous commit. { width=600px } Go back to the repository's main page and click on the commit tracker on the right above the list of files, which will give you an overview of all commits made. Clicking on a specific commit lets you see the changes introduced by that commit. Click on the commit that was the initial commit, where we added all the files. { width=600px } You will now see the files as they were when we first added them. Specifically you can see that the Dockerfile is back, even though we deleted it! Click on the Code tab to the left to return to the overview of the latest repository version. Quick recap We learned some of the most important features of the GitHub interface and how repositories can be viewed online. Working with remote repositories # While remote repositories are extremely useful as backups and for collaborating with others, that's not their only use: remotes also help when you are working from different computers, a computer cluster or a cloud service. Let's pretend that you want to work on this repository from a different computer. First, create a different directory ( e.g. git_remote_tutorial ) in a separate location that is not already tracked by Git and cd into it. Now we can download the repository we just uploaded using the following: git clone git@github.com:user/git_tutorial.git . Again, make sure to replace user with your GitHub user name. Notice the dot at the end of the command above, which will put the clone into the current directory, instead of creating a new directory with the same name as the remote repository. You will see that all your files are here, identical to the original git_tutorial repository! Since you already gave the address to Git when you cloned the repository, you don't have to add it manually as before. Verify this with git remote -v . Let's say that we now want to change the multiqc software to an earlier version: open the environment.yml file in the second local repo and change multiqc=1.12 to multiqc=1.7 ; add and commit the change. We can now use push again to sync our remote repository with the new local changes. Refresh your web page again and see that the changes have taken effect. Since we have now updated the remote repository with code that came from the second local repository, the first local repository is now outdated. We thus need to update the first local repo with the new changes. This can be done with the pull command. cd back into the first local repository ( e.g. git_tutorial ) and run the git pull command. This will download the newest changes from the remote repository and merge them locally automatically. Check that everything is up-to-date with git status . Another command is git fetch , which will download remote changes without merging them. This can be useful when you want to see if there are any remote changes that you may want to merge, without actually doing it, such as in a collaborative setting. In fact, git pull in its default mode is just a shorthand for git fetch followed by git merge FETCH_HEAD (where FETCH_HEAD points to the tip of the branch that was just fetched). That's quite a few concepts and commands you've just learnt! It can be a bit hard to keep track of everything and the connections between local and remote Git repositories and how you work with them, but hopefully the following figure will give you a short visual summary: { width=600px } Quick recap We have learned the difference between local and remote copies of git repositories and how to sync them: git push uploads commits to a remote repository git pull downloads commits from a remote repository and merges them to the local branch git fetch downloads commits from a remote repository without merging them to the local branch git clone makes a local copy of a remote repository Remote branches # Remote branches work much in the same way a local branches, but you have to push them separately; you might have noticed that GitHub only listed our repository as having one branch (you can see this by going to the Code tab). This is because we only pushed our main branch to the remote. Let's create a new local branch and add some changes that we'll push as a separate branch to our remote - you should do this in the original git_tutorial repository, so move back into that directory. Create a new branch named trimming and add the --trim5 5 flag to the bowtie2-command part of the Snakefile , which should now look like this: bowtie2 --trim5 5 --very-sensitive-local -x $indexBase -U {input.fastq} > {output} 2> {log} Add and commit the change to your local repository. Instead of doing what we previously did, i.e. merge the trimming branch into the main branch, we'll push trimming straight to our remote: git push origin trimming Go the repository at GitHub and see if the new branch has appeared. Just above the file listing click the Branch drop-down and select the new branch to view it. Can you see the difference in the Snakefile depending on which branch you choose? We now have two branches both locally and remotely: main and trimming . We can continue working on our trimming branch until we're satisfied (all the while pushing to the remote branch with the same name), at which point we want to merge it into main . Checkout your local main branch and merge it with the trimming branch. Push your main branch to your remote and subsequently delete your local trimming branch. The above command only deleted the local branch. If you want to remove the branch from the remote repository as well, run: git push origin --delete trimming Quick recap We learned how to push local branches to a remote with git push origin <branch> and how to delete remote branches with git push origin --delete <branch> . Sharing tags # Your local repository tags are not included when you do a normal push. To push tags to the remote you need to supply the --tags flag to the git push command: git push --tags Go to the repository overview page on GitHub. You will see that the repository now has three tags! If you click on Tags you will be given an overview of the existing tags for your repository - if you click Releases you will see more or less the same information. Confusing? Well, a tag is a Git concept while a release is a GitHub concept that is based on Git tags. Releases add some extra features that can be useful for distributing software and are done manually from the repository's GitHub page. Click on one of the tags. Here users can download a compressed file containing the repository at the version specified by the tags. { width=600px } Alternatively, Git users who want to reproduce your analysis with the code used for the publication can clone the GitHub repository and then run git checkout publication . Quick recap We learned how to push Git tags to a remote by using the --tags flag.","title":"Working remotly"},{"location":"git/git-7-working-remotely/#github-setup","text":"If you have not done so already, go to github.com and create an account. You can also create an account on another online hosting service for version control, e.g. Bitbucket or GitLab . The exercises below are written with examples from GitHub (as that is the most popular platform with the most extensive features), but the same thing can be done on alternative services, although the exact menu structure and link placements differ a bit. Any upload to and from GitHub requires you to authenticate yourself. GitHub used to allow authentication with your account and password, but this is no longer the case - using SSH keys is favoured instead. Knowing exactly what these are is not necessary to get them working, but we encourage you to read the box below to learn more about them! GitHub has excellent, platform-specific instructions both on how to generate and add SSH keys to your account, so please use them before moving on! SSH keys and authentication Using SSH (Secure Shell) for authentication basically entails setting up a pair of keys: one private and one public. You keep the private key on your local computer and give the public key to anywhere you want to be able to connect to, e.g. GitHub. The public key can be used to encrypt messages that only the corresponding private key can decrypt. A simplified description of how SSH authentication works goes like this: The client ( i.e. the local computer) sends the ID of the SSH key pair it would like to use for authentication to the server ( e.g. GitHub) If that ID is found, the server generates a random number and encrypts this with the public key and sends it back to the client The client decrypts the random number with the private key and sends it back to the server Notice that the private key always remains on the client's side and is never transferred over the connection; the ability to decrypt messages encrypted with the public key is enough to ascertain the client's authenticity. This is in contrast with using passwords, which are themselves sent across a connection (albeit encrypted). It is also important to note that even though the keys come in pairs it is impossible to derive the private key from the public key. If you want to read more details about how SSH authentication work you can check out this website , which has more in-depth information than we provide here.","title":"GitHub setup"},{"location":"git/git-7-working-remotely/#create-a-remote-repository","text":"Log in to your GitHub account and press the New button: Make sure you are listed as the owner Add a repository name, e.g. git_tutorial You can keep the repo private or make it public, as you wish Skip including a README, a .gitignore and licence { width=600px } You will now be redirected to the repository page which will list several ways for you to start adding content (files) to the repository. What we will do is to connect the local repository we've been working on so far to the remote GitHub server using SSH: Add a remote SSH address to your local repository (make sure you change user to your GitHub username and git_tutorial to your repository name): git remote add origin git@github.com:user/git_tutorial.git Run git remote -v . This will show you what remote location is connected to your local Git clone. The short name of the default remote is usually \" origin \" by convention. Note Make sure you've used an SSH address ( i.e. starting with git@github.com rather than an HTTPS address (starting with https://github.com )! We have not yet synced the local and remote repositories, though, we've simply connected them. Let's sync them now: git push origin main The push command sends our local history of the main branch to the same branch on the remote ( origin ). Our Git repository is now stored on GitHub! Run git status . This should tell you that: On branch main nothing to commit, working tree clean You always need to specify git push origin main by default, but you can circumvent this by telling Git that you always want to push to origin/main when you're on your local main branch. To do this, use the command git branch --set-upstream-to origin/main . Try it out now . Now run git-status again. You should see that now git additionally tells you that your local branch is up to date with the remote branch. If you go to the repository's GitHub page you should now be able to see all your files and your code there! It should look something like this: { width=600px } You can see a lot of things there, such as each file and the latest commit that changed them, the repository's branches and a message from GitHub at the bottom: \"Help people interested in this repository understand your project by adding a README.\" This refers to GitHub's built-in functionality of automatically rendering any markdown document named README or README.md in the repository's root directory and displaying it along with what you can already see. Let's try it out! Let's create a README.md file and fill it with the following text: # A Git tutorial This repository contains tutorial information related to the **NBIS/ELIXIR** course *Tools for Reproducible Research*, specifically the session on using the `git` software for version control. ## Links You can find the latest stable version of the Git tutorial for the course [here](https://uppsala.instructure.com/courses/73110/pages/git-1-introduction?module_item_id=367079). Add, commit and push these changes to GitHub. git add README.md git commit -m \"Add README.md\" git push origin main You should now be able to see the rendered markdown document, which looks a bit different from the text you copied in from above. Note that there are two different header levels, which come from the number of hash signs ( # ) used. You can also see bold text (which was surrounded by two asterisks), italic text (surrounded by one asterisk), in-line code (surrounded by acute accents) and a link (link text inside square brackets followed by link address inside parentheses). It is important to add README-files to your repositories so that they are better documented and more easily understood by others and, more likely, your future self. In fact, documentation is an important part of reproducible research! While the tools that you are introduced to by this course are all directly related to making science reproducible, you will also need good documentation. Make it a habit of always adding README-files for your repositories, fully explaining the ideas and rationale behind the project. You can even add README-files to sub-directories as well, giving you the opportunity to go more in-depth where you so desire. Tip There are a lot more things you can do with markdown than what we show here. Indeed, this entire course is mostly written in markdown! You can read more about markdown here . Quick recap We learned how to connect local Git repositories to remote locations such as GitHub and how to upload commits using git push . We also learned the basics of markdown and how it can be used to document Git repositories.","title":"Create a remote repository"},{"location":"git/git-7-working-remotely/#browsing-github","text":"GitHub and the rest of the websites that offer remote hosting of git repositories all have numerous features, which can be somewhat difficult to navigate in the beginning. We here go through some of the basics of what you can do with GitHub. Go to your GitHub repository in your browser again and click on Code to the left. Click on config.yml . You will see the contents of the file. Notice that it is the latest version, where we previously changed the genome_id variable: { width=600px } Click on History . You will see an overview of the commits involving changes made to this file: { width=600px } Click on the Change to ST398 for alignment commit. You will see the changes made to config.yml file compared to the previous commit. { width=600px } Go back to the repository's main page and click on the commit tracker on the right above the list of files, which will give you an overview of all commits made. Clicking on a specific commit lets you see the changes introduced by that commit. Click on the commit that was the initial commit, where we added all the files. { width=600px } You will now see the files as they were when we first added them. Specifically you can see that the Dockerfile is back, even though we deleted it! Click on the Code tab to the left to return to the overview of the latest repository version. Quick recap We learned some of the most important features of the GitHub interface and how repositories can be viewed online.","title":"Browsing GitHub"},{"location":"git/git-7-working-remotely/#working-with-remote-repositories","text":"While remote repositories are extremely useful as backups and for collaborating with others, that's not their only use: remotes also help when you are working from different computers, a computer cluster or a cloud service. Let's pretend that you want to work on this repository from a different computer. First, create a different directory ( e.g. git_remote_tutorial ) in a separate location that is not already tracked by Git and cd into it. Now we can download the repository we just uploaded using the following: git clone git@github.com:user/git_tutorial.git . Again, make sure to replace user with your GitHub user name. Notice the dot at the end of the command above, which will put the clone into the current directory, instead of creating a new directory with the same name as the remote repository. You will see that all your files are here, identical to the original git_tutorial repository! Since you already gave the address to Git when you cloned the repository, you don't have to add it manually as before. Verify this with git remote -v . Let's say that we now want to change the multiqc software to an earlier version: open the environment.yml file in the second local repo and change multiqc=1.12 to multiqc=1.7 ; add and commit the change. We can now use push again to sync our remote repository with the new local changes. Refresh your web page again and see that the changes have taken effect. Since we have now updated the remote repository with code that came from the second local repository, the first local repository is now outdated. We thus need to update the first local repo with the new changes. This can be done with the pull command. cd back into the first local repository ( e.g. git_tutorial ) and run the git pull command. This will download the newest changes from the remote repository and merge them locally automatically. Check that everything is up-to-date with git status . Another command is git fetch , which will download remote changes without merging them. This can be useful when you want to see if there are any remote changes that you may want to merge, without actually doing it, such as in a collaborative setting. In fact, git pull in its default mode is just a shorthand for git fetch followed by git merge FETCH_HEAD (where FETCH_HEAD points to the tip of the branch that was just fetched). That's quite a few concepts and commands you've just learnt! It can be a bit hard to keep track of everything and the connections between local and remote Git repositories and how you work with them, but hopefully the following figure will give you a short visual summary: { width=600px } Quick recap We have learned the difference between local and remote copies of git repositories and how to sync them: git push uploads commits to a remote repository git pull downloads commits from a remote repository and merges them to the local branch git fetch downloads commits from a remote repository without merging them to the local branch git clone makes a local copy of a remote repository","title":"Working with remote repositories"},{"location":"git/git-7-working-remotely/#remote-branches","text":"Remote branches work much in the same way a local branches, but you have to push them separately; you might have noticed that GitHub only listed our repository as having one branch (you can see this by going to the Code tab). This is because we only pushed our main branch to the remote. Let's create a new local branch and add some changes that we'll push as a separate branch to our remote - you should do this in the original git_tutorial repository, so move back into that directory. Create a new branch named trimming and add the --trim5 5 flag to the bowtie2-command part of the Snakefile , which should now look like this: bowtie2 --trim5 5 --very-sensitive-local -x $indexBase -U {input.fastq} > {output} 2> {log} Add and commit the change to your local repository. Instead of doing what we previously did, i.e. merge the trimming branch into the main branch, we'll push trimming straight to our remote: git push origin trimming Go the repository at GitHub and see if the new branch has appeared. Just above the file listing click the Branch drop-down and select the new branch to view it. Can you see the difference in the Snakefile depending on which branch you choose? We now have two branches both locally and remotely: main and trimming . We can continue working on our trimming branch until we're satisfied (all the while pushing to the remote branch with the same name), at which point we want to merge it into main . Checkout your local main branch and merge it with the trimming branch. Push your main branch to your remote and subsequently delete your local trimming branch. The above command only deleted the local branch. If you want to remove the branch from the remote repository as well, run: git push origin --delete trimming Quick recap We learned how to push local branches to a remote with git push origin <branch> and how to delete remote branches with git push origin --delete <branch> .","title":"Remote branches"},{"location":"git/git-7-working-remotely/#sharing-tags","text":"Your local repository tags are not included when you do a normal push. To push tags to the remote you need to supply the --tags flag to the git push command: git push --tags Go to the repository overview page on GitHub. You will see that the repository now has three tags! If you click on Tags you will be given an overview of the existing tags for your repository - if you click Releases you will see more or less the same information. Confusing? Well, a tag is a Git concept while a release is a GitHub concept that is based on Git tags. Releases add some extra features that can be useful for distributing software and are done manually from the repository's GitHub page. Click on one of the tags. Here users can download a compressed file containing the repository at the version specified by the tags. { width=600px } Alternatively, Git users who want to reproduce your analysis with the code used for the publication can clone the GitHub repository and then run git checkout publication . Quick recap We learned how to push Git tags to a remote by using the --tags flag.","title":"Sharing tags"},{"location":"git/git-8-conflicts/","text":"It is not uncommon to run into conflicts when you are trying to merge separate branches, and it's even more common when you're working in a collaborative setting with remote repositories. It'll happen sooner or later, even if you're only working locally, so it's important to know how to deal with them! We'll now introduce a conflict on purpose, which we can then solve. Remember that we have two separate local copies of the same repository? Let's go into the first one, git_tutorial , and change the MultiQC version in the environment.yml file: multiqc=1.8 Add, commit and push your change to the remote. Now we have a change in our remote and one of our local copies, but not in the other. This could happen if a collaborator of yours committed a change and pushed it to GitHub. Let's create a conflict! Move into your other local repository, git_remote_tutorial , which doesn't have the new change. Run git status . Notice that Git says: \" Your branch is up-to-date with 'origin/main'. \". We know that this is not true, but this local clone is not yet aware of the remote changes. Let's change the environment.yml file in this local repository as well, but to version 1.6, instead! It may be the case that your collaborator thought it was good to use MultiQC version 1.8, whereas you thought it would be better to use MultiQC version 1.6, but neither of you communicated that to the other. Add and commit your change and try to push the commit, which should give you an error message that looks like this: ! [rejected] main -> main (fetch first) error: failed to push some refs to 'https://github.com/user/git_tutorial.git' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details. This error message is thankfully quite informative in regards to what is going on and what might be done about it. In essence it will not allow you to push to the remote since there are conflicting changes made to it. Let's download the changes made to the remote, but without trying to merge them directly. This can be done using the following command: git fetch Note The fetch command is very similar to pull in that it downloads remote changes that are not present locally, but differs in that it doesn't try to merge them locally; pull both downloads and merges (unless there's a conflict, in which case it will tell you so and raise an error like the one above). You can thus skip fetch and just do pull straight away, if you prefer. Now run git status . Unlike before, our local Git clone now is aware of the latest changes pushed to the remote. It will tell you something along the lines: \" Your branch and 'origin/main' have diverged, and have 1 and 1 different commit each, respectively. \". We can now run the following to see what the difference is between the current state of our local clone and the main branch on the remote origin: git diff origin/main Now let's try to integrate the remote changes with our local changes and get up to sync with the remote: git merge Unsurprisingly, the git merge command resulted in a conflict. Git tells us about this and suggests that we should fix the conflicts and commit that. As always, run git status to get an overview: you will see that you have so-called unmerged paths and that the conflicting file is environment.yml , since both modified the same line in this file. To fix a conflict, open the affected file in a text editor. You will see that it now looks something like this: channels: - conda-forge - bioconda - main - r dependencies: - python=3.9.12 - fastqc=0.11.9 - sra-tools=2.10.1 - snakemake=7.3.8 <<<<<<< HEAD - multiqc=1.6 ======= - multiqc=1.8 >>>>>>> refs/remotes/origin/main - bowtie2=2.4.5 - tbb=2020.2 - samtools=1.15.1 - subread=2.0.1 - bedtools=2.29.2 - wget=1.20.3 - graphviz=3.0.0 - r-base=4.1.3 - r-ggplot2=3.3.5 - r-reshape2=1.4.4 - r-stringi=1.7.6 - r-pheatmap=1.0.12 - r-rmarkdown=2.13 - r-r.utils=2.11.0 - bioconductor-rtracklayer=1.54.0 - bioconductor-geoquery=2.62.0 - xorg-libxrender - xorg-libxpm The part between <<<<<<< HEAD and ======= is your local version, and the part between ======= and >>>>>>> refs/remotes/origin/main is the one added to the remote and which caused the conflict when you tried to merge those changes to your local repository. It is now up to you to decide which version to keep, or to change it to a third alternative. Let's say that you are confident that it is better to run MultiQC 1.6 rather than 1.8. Edit the file so that it looks like you want it to, i.e. remove the lines added by Git and delete the line with multiqc=1.8 . The final file should look like this: channels: - conda-forge - bioconda - main - r dependencies: - python=3.9.12 - fastqc=0.11.9 - sra-tools=2.10.1 - snakemake=7.3.8 - multiqc=1.6 - bowtie2=2.4.5 - tbb=2020.2 - samtools=1.15.1 - subread=2.0.1 - bedtools=2.29.2 - wget=1.20.3 - graphviz=3.0.0 - r-base=4.1.3 - r-ggplot2=3.3.5 - r-reshape2=1.4.4 - r-stringi=1.7.6 - r-pheatmap=1.0.12 - r-rmarkdown=2.13 - r-r.utils=2.11.0 - bioconductor-rtracklayer=1.54.0 - bioconductor-geoquery=2.62.0 - xorg-libxrender - xorg-libxpm Run git status again. Notice that it says use \"git add <file>...\" to mark resolution ? Let's do that! git add environment.yml Run git status again! It will now tell us: All conflicts fixed but you are still merging. (use \"git commit\" to conclude merge) . So, you probably guessed it, run: git commit -m \"Merge and set multiqc to v1.6\" Finally, push these changes to GitHub: git push Go to GitHub in the browser and click the commit tracker again. You will see a list of commits including where MultiQC was first changed to version 1.7 from our previous work, then to 1.8 , 1.6 and, finally, followed by a merge where the version was set to 1.6 . Note While the example we've used here is from a collaborative setting, conflicts also arise when you are working alone. They usually happen when you have several feature branches that you want to merge into main and you've forgot to keep all branches up-to-date with each other. Quick recap We learned about how conflicting commits can happen and how to deal with them by inspecting the affected files and looking for the source of the conflict.","title":"Conflict"},{"location":"git/git-9-extra-material/","text":"The following extra material contains some more advanced things you can do with Git and the command line in general, which is not part of the main course materials. All the essential skills of Git are covered by the previous sections; the material here should be considered tips and tricks from people who use Git every day. You thus don't need to use these things unless you want to, and you can even skip this part of the lesson if you like! If you are interested in learning more about Git in general, here are some reading tips for you: Git cheat-sheet A simple Git guide Resources to learn Git Git reference manual Decorating your prompt # When you are working on the command line interface (CLI), you will usually have some small pieces of information relating to your current directory, the name of the computer or host you're working on, and so forth. You've probably already seen your prompt while working with Git throughout this lesson, but here's an example of what one might look like: erikfmbp:~/teaching/workshop-reproducible-research erik.fasterius $ The above prompt contains the name of the computer, a colon, the current working directory, the username and a dollar-sign; it is stored in the variable PS1 . You can type echo $PS1 to see what variables your prompt is made up of; the above example contains \\h:\\W \\u\\$ , where \\h is the hostname, \\W the working directory and \\u the username. Some people like to also show the current branch on their prompt, thus avoiding having to type git branch continuously. There are several ways you might do this, and we're only presenting one of them here: a bash function. git_branch() { git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/' } This function does a number of things: Ejects the error message from Git if the current directory isn't a part of a Git repository into /dev/null ( i.e. into nothing). Find the current branch by searching for a line that starts with * ( i.e. the current branch) using the command line program sed . Put the current branch into parentheses with a space before it. We can then build our new prompt by adding this function into it: # The first part of the old prompt PS1='\\h:\\W \\u' # Add the Git branch PS1=$PS1'$(git_branch)' # Add the last part of the old prompt PS1=$PS1' \\$' Now you should see the current Git branch on your prompt! The only problem now is that this only works for your current session: once you restart your CLI you'll have to re-define your prompt again. This can be circumvented, though. What you need to do is to add the code defining your prompt into your so-called bash profile: ~/.bash_profile . Every time you load a new CLI session this file is read and any code inside it is executed. You might already have this file, so make sure you don't overwrite it! Bash aliases for git # Some Git commands are used over and over again when working with git, such as git status . Some people like to have aliases ( i.e. shortcuts) for these common commands. Here is a small list of such aliases that you may find useful or, even better, might inspire you to create your own! Add them to your ~/.bash_profile as above, so that they're available across sessions. # Basic git commands alias gb='git branch' alias ga='git add' alias gd='git diff' alias gcm='git commit' alias gp='git push' alias gu='git pull' alias gm='git merge' alias gco='git checkout' alias gl='git log' # Git status in short format alias gst='git status -s' # Add and commit all tracked and modified files alias gca='git commit -a' # Create and checkout a new branch alias gcob='git checkout -b' # Git log with one line per commit alias glo='git log --oneline' Forking # When you want to work on an Open Source project that is available on e.g. GitHub, you usually don't have permission to directly push code to the project's repository - this is so that the project's maintainers are the only ones that can directly change anything in their codebase. How do you then contribute to projects that don't allow you to push your code to their repository? Simple: use forking ! Forking is when you make your own copy of a repository on your GitHub account, which you will then have permissions to change as you see fit. You can then create pull requests from your fork to the original repository, rather than pushing code to a new branch and making a pull request from that. Working with forks just adds an additional step to the whole workflow: instead of being \"clone; code and commit changes on a new branch; push branch to remote; pull request from branch\" it becomes \"fork; clone; code and commit changes; push code to fork; pull request from fork\". You might also want to do a fork of a project simply because you want to have your own copy of it as well, without ever having the intention of changing it. This is, of course, perfectly fine as well, but do keep in mind that developers are usually quite happy to incorporate new changes from contributors if they are reasonable and fulfil a purpose and add functionality to the project. It is quite common that you have a use-case the maintainer didn't think of before, and that you've helped the project grow by contributing your code!","title":"Extra material"},{"location":"git/git-9-extra-material/#decorating-your-prompt","text":"When you are working on the command line interface (CLI), you will usually have some small pieces of information relating to your current directory, the name of the computer or host you're working on, and so forth. You've probably already seen your prompt while working with Git throughout this lesson, but here's an example of what one might look like: erikfmbp:~/teaching/workshop-reproducible-research erik.fasterius $ The above prompt contains the name of the computer, a colon, the current working directory, the username and a dollar-sign; it is stored in the variable PS1 . You can type echo $PS1 to see what variables your prompt is made up of; the above example contains \\h:\\W \\u\\$ , where \\h is the hostname, \\W the working directory and \\u the username. Some people like to also show the current branch on their prompt, thus avoiding having to type git branch continuously. There are several ways you might do this, and we're only presenting one of them here: a bash function. git_branch() { git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/' } This function does a number of things: Ejects the error message from Git if the current directory isn't a part of a Git repository into /dev/null ( i.e. into nothing). Find the current branch by searching for a line that starts with * ( i.e. the current branch) using the command line program sed . Put the current branch into parentheses with a space before it. We can then build our new prompt by adding this function into it: # The first part of the old prompt PS1='\\h:\\W \\u' # Add the Git branch PS1=$PS1'$(git_branch)' # Add the last part of the old prompt PS1=$PS1' \\$' Now you should see the current Git branch on your prompt! The only problem now is that this only works for your current session: once you restart your CLI you'll have to re-define your prompt again. This can be circumvented, though. What you need to do is to add the code defining your prompt into your so-called bash profile: ~/.bash_profile . Every time you load a new CLI session this file is read and any code inside it is executed. You might already have this file, so make sure you don't overwrite it!","title":"Decorating your prompt"},{"location":"git/git-9-extra-material/#bash-aliases-for-git","text":"Some Git commands are used over and over again when working with git, such as git status . Some people like to have aliases ( i.e. shortcuts) for these common commands. Here is a small list of such aliases that you may find useful or, even better, might inspire you to create your own! Add them to your ~/.bash_profile as above, so that they're available across sessions. # Basic git commands alias gb='git branch' alias ga='git add' alias gd='git diff' alias gcm='git commit' alias gp='git push' alias gu='git pull' alias gm='git merge' alias gco='git checkout' alias gl='git log' # Git status in short format alias gst='git status -s' # Add and commit all tracked and modified files alias gca='git commit -a' # Create and checkout a new branch alias gcob='git checkout -b' # Git log with one line per commit alias glo='git log --oneline'","title":"Bash aliases for git"},{"location":"git/git-9-extra-material/#forking","text":"When you want to work on an Open Source project that is available on e.g. GitHub, you usually don't have permission to directly push code to the project's repository - this is so that the project's maintainers are the only ones that can directly change anything in their codebase. How do you then contribute to projects that don't allow you to push your code to their repository? Simple: use forking ! Forking is when you make your own copy of a repository on your GitHub account, which you will then have permissions to change as you see fit. You can then create pull requests from your fork to the original repository, rather than pushing code to a new branch and making a pull request from that. Working with forks just adds an additional step to the whole workflow: instead of being \"clone; code and commit changes on a new branch; push branch to remote; pull request from branch\" it becomes \"fork; clone; code and commit changes; push code to fork; pull request from fork\". You might also want to do a fork of a project simply because you want to have your own copy of it as well, without ever having the intention of changing it. This is, of course, perfectly fine as well, but do keep in mind that developers are usually quite happy to incorporate new changes from contributors if they are reasonable and fulfil a purpose and add functionality to the project. It is quite common that you have a use-case the maintainer didn't think of before, and that you've helped the project grow by contributing your code!","title":"Forking"},{"location":"introduction/introduction/","text":"Welcome to the tutorials! Here we will learn how to make a computational research project reproducible using several different tools, described in the figure below: { width=700px } The figure above gives an overview of the different parts of computational reproducibility (data, code, workflow and environment), as well as the various tools that are used for each part; Git is, arguably, integral to all of the parts, but we only listed it in the code section for a less cluttered figure. The course has a tutorial for each of the tools, all made so that they can be completed independently of each other. It is therefore perfectly possible to go through them in whatever order you prefer, but we suggest the following order: Git Conda Snakemake Nextflow R Markdown Jupyter Containers You will find the tutorials in the Modules section in the navigation menu. Please make sure to carefully follow the pre-course setup to install the tools and download the course material before starting with any of the tutorials. These will create quite a lot of files on your computer, some of which will actually take up a bit of storage space too. In order to remove any traces of these after completing the tutorials, please refer to the Take down section . Before going into the tutorials themselves, we first describe the case study from which the example data comes from. The case study # We will be running a small bioinformatics project as a case study, and use that to exemplify the different steps of setting up a reproducible research project. To give you some context, the study background and analysis steps are briefly described below. Background # The data is taken from Osmundson, Dewell, and Darst (2013) , who have studied methicillin-resistant Staphylococcus aureus (MRSA). MRSA is resistant to broad spectrum beta-lactam antibiotics and lead to difficult-to-treat infections in humans. Lytic bacteriophages have been suggested as potential therapeutic agents, or as the source of novel antibiotic proteins or peptides. One such protein, gp67, was identified as a transcription-inhibiting transcription factor with an antimicrobial effect. To identify S. aureus genes repressed by gp67, the authors expressed gp67 in S. aureus cells. RNA-seq was then performed on three S. aureus strains: RN4220 with pRMC2 with gp67 RN4220 with empty pRMC2 NCTC8325-4 Analysis # The graph below shows the different steps of the analysis that are included in this project: { width=400px } The input files are: RNA-seq raw data (FASTQ files) for the three strains S. aureus genome sequence (a FASTA file) S. aureus genome annotation (a GFF file) The workflow itself will perform the following tasks: Downloading and indexing of the reference genome using Bowtie2 Downloading the raw FASTQ data from the Sequence Read Archive (SRA) Run some quality controls on the data using FastQC and MultiQC Align the raw data to the genome and calculate the gene expression using featureCounts Produce supplementary materials using data from quality controls, gene expression and the workflow figure shown above","title":"Introduction"},{"location":"introduction/introduction/#the-case-study","text":"We will be running a small bioinformatics project as a case study, and use that to exemplify the different steps of setting up a reproducible research project. To give you some context, the study background and analysis steps are briefly described below.","title":"The case study"},{"location":"introduction/introduction/#background","text":"The data is taken from Osmundson, Dewell, and Darst (2013) , who have studied methicillin-resistant Staphylococcus aureus (MRSA). MRSA is resistant to broad spectrum beta-lactam antibiotics and lead to difficult-to-treat infections in humans. Lytic bacteriophages have been suggested as potential therapeutic agents, or as the source of novel antibiotic proteins or peptides. One such protein, gp67, was identified as a transcription-inhibiting transcription factor with an antimicrobial effect. To identify S. aureus genes repressed by gp67, the authors expressed gp67 in S. aureus cells. RNA-seq was then performed on three S. aureus strains: RN4220 with pRMC2 with gp67 RN4220 with empty pRMC2 NCTC8325-4","title":"Background"},{"location":"introduction/introduction/#analysis","text":"The graph below shows the different steps of the analysis that are included in this project: { width=400px } The input files are: RNA-seq raw data (FASTQ files) for the three strains S. aureus genome sequence (a FASTA file) S. aureus genome annotation (a GFF file) The workflow itself will perform the following tasks: Downloading and indexing of the reference genome using Bowtie2 Downloading the raw FASTQ data from the Sequence Read Archive (SRA) Run some quality controls on the data using FastQC and MultiQC Align the raw data to the genome and calculate the gene expression using featureCounts Produce supplementary materials using data from quality controls, gene expression and the workflow figure shown above","title":"Analysis"},{"location":"jupyter/jupyter-1-introduction/","text":"The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain code, equations, visualizations and text. The functionality is partly overlapping with R Markdown (see the tutorial ), in that they both use markdown and code chunks to generate reports that integrate results of computations with the code that generated them. Jupyter Notebook comes from the Python community while R Markdown was developed by RStudio, but you could use most common programming languages in either alternative. In practice though, it's quite common that R developers use Jupyter but probably not very common that Python developers use RStudio. Some reasons to use Jupyter include: Python is lacking a really good IDE for doing exploratory scientific data analysis, like RStudio or Matlab. Some people use Jupyter simply as an alternative for that. The community around Jupyter notebooks is large and dynamic, and there are lots of tools for sharing, displaying or interacting with notebooks. An early ambition with Jupyter notebooks (and its predecessor IPython notebooks) was to be analogous to the lab notebook used in a wet lab. It would allow the data scientist to document his or her day-to-day work and interweave results, ideas, and hypotheses with the code. From a reproducibility perspective, this is one of the main advantages. Jupyter notebooks can be used, just like R Markdown, to provide a tighter connection between your data and your results by integrating results of computations with the code that generated them. They can also do this in an interactive way that makes them very appealing for sharing with others. As always, the best way is to try it out yourself and decide what to use it for! This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. Then open up a terminal and go to workshop-reproducible-research/tutorials/jupyter and activate your jupyter-env Conda environment. A note on nomenclature Jupyter: a project to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. Lives at jupyter.org . Jupyter Notebook: A web application that you use for creating and managing notebooks. One of the outputs of the Jupyter project. Jupyter notebook: The actual .ipynb file that constitutes your notebook.","title":"Introduction"},{"location":"jupyter/jupyter-2-the-basics/","text":"One thing that sets Jupyter Notebook apart from what you might be used to is that it's a web application, i.e. you edit and run your code from your browser. But first you have to start the Jupyter Notebook server: jupyter notebook --allow-root You should see something similar to this printed to your terminal: [I 18:02:26.722 NotebookApp] Serving notebooks from local directory: /Users/john/workshop-reproducible-research/tutorials/jupyter [I 18:02:26.723 NotebookApp] 0 active kernels [I 18:02:26.723 NotebookApp] The Jupyter Notebook is running at: [I 18:02:26.723 NotebookApp] http://localhost:8888/?token=e03f10ccb40efc3c6154358593c410a139b76acf2cae000 [I 18:02:26.723 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 18:02:26.724 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=e03f10ccb40efc3c6154358593c410a139b76acf2cae785c [I 18:02:27.209 NotebookApp] Accepting one-time-token-authenticated connection from ::1 A note for Windows users If you see the error message Start : This command cannot be run due to the error: The system cannot find the file specified. ... then try starting jupyter with jupyter notebook --no-browser then copy the URL given into the browser directly. Jupyter Notebook probably opened up a web browser for you automatically, otherwise go to the address specified in the message in the terminal. Note that the server is running locally (as http://localhost:8888 ) so this does not require that you have an active internet connection. Also note that it says: Serving notebooks from local directory: </some/local/path/workshop-reproducible-research/tutorials/jupyter> Everything you do in your Notebook session will be stored in this directory, so you won't lose any work if you shut down the server. { width=700px } What you're looking at is the Notebook dashboard. This is where you manage your files, notebooks, and kernels. The Files tab shows the files in your directory. The Running tab keeps track of all your processes. The third tab, Clusters, is used for parallel computing and won't be discussed further in this tutorial. Finally, the Nbextensions tab shows a list of configurable notebook extensions that you can use to add functionality to your notebook (as we'll see below). Let's start by creating an empty notebook by selecting the Files tab and clicking New > Python 3. This will open up a new tab or window looking like this: { width=700px } Tip If you want to start Jupyter Notebooks on a cluster that you SSH to ( e.g. Uppmax) see the section in the Extra material Jupyter notebooks are made up of cells, and you are currently standing in the first cell in your notebook. The fact that it has a green border indicates that it's in \"Edit mode\", so you can write stuff in it. A blue border indicates \"Command mode\" (see below). Cells in Jupyter notebooks can be of two types: markdown or code . Markdown: These cells contain static material such as captions, text, lists, images and so on. You express this using Markdown, which is a lightweight markup language. Markdown documents can then be converted to other formats for viewing (the document you're reading now is written in Markdown and then converted to HTML). The format is discussed a little more in detail in the R Markdown tutorial . Jupyter Notebook uses a dialect of Markdown called Github Flavored Markdown, which is described here . Code: These are the cells that actually do something, just as code chunks do in R Markdown. You can write code in dozens of languages and all do all kinds of clever tricks. You then run the code cell and any output the code generates, such as text or figures, will be displayed beneath the cell. We will get back to this in much more detail, but for now it's enough to understand that code cells are for executing code that is interpreted by a kernel (in this case the Python version in your Conda environment). Before we continue, here are some shortcuts that can be useful. Note that they are only applicable when in command mode (blue frames). Most of them are also available from the menus. These shortcuts are also available from the Help menu in your notebook (there's even an option there to edit shortcuts). Shortcut Effect `enter` enter Edit mode `escape` Enter Command mode `ctrl` - `enter` Run the cell `shift` - `enter` Run the cell and select the cell below `alt` - `enter` Run the cell and insert a new cell below `s` Save the notebook `tab` For code completion or indentation `m`, `y` Toggle between Markdown and Code cells `d`- `d` Delete a cell `a` Insert cells above current cell `b` Insert cells below current cell `x` Cut currently selected cells `o` Toggle output of current cell Writing markdown # Let's use our first cell to create a header. Change the format from Code to Markdown using the drop-down list in the Notebook Toolbar, or by pressing the m key when in command mode. Double click on the cell, or hit enter to enter editing mode (green frame) and input \"# My notebook\" (\"#\" is used in Markdown for header 1). Run the cell with ctrl - enter . Tada! Markdown is a simple way to structure your notebook into sections with descriptive notes, lists, links, images etc. Below are some examples of what you can do in markdown. Paste all or parts of it into one or more cells in your notebook to see how it renders. Make sure you set the cell type to Markdown. ## Introduction In this notebook I will try out some of the **fantastic** concepts of Jupyter Notebooks. ## Markdown basics Examples of text attributes are: * *italics* * **bold** * `monospace` Sections can be separated by horizontal lines. --- Blockquotes can be added, for instance to insert a Monty Python quote: Spam! Spam! Spam! Spam! See [here](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) for more information. Writing code # Now let's write some code! Since we chose a Python kernel, Python would be the native language to run in a cell. Enter this code in the second cell and run it: print(\"Hello world!\") Note how the output is displayed below the cell. This interactive way of working is one of the things that sets Jupyter Notebook apart from RStudio and R Markdown. R Markdown is typically rendered top-to-bottom in one run, while you work in a Jupyter notebook in a different way. This has partly changed with newer versions of RStudio, but it's probably still how most people use the two tools. What is a Jupyter notebook? Let's look a little at the notebook we're currently working in. Jupyter Notebooks are autosaved every minute or so, so you will already have it available. We can be a little meta and do this from within the notebook itself. We do it by running some shell commands in the third code cell instead of Python code. This very handy functionality is possible by prepending the command with ! . Try !ls to list the files in the current directory. Aha, we have a new file called Untitled.ipynb ! This is our notebook. Look at the first ten lines of the file by using !head Untitled.ipynb . Seems like it's just a plain old JSON file. Since it's a text file it's suitable for version control with for example Git. It turns out that Github and Jupyter notebooks are the best of friends, as we will see more of later. This switching between languages and whatever-works mentality is very prominent within the Jupyter notebook community. Variables defined in cells become variables in the global namespace. You can therefore share information between cells. Try to define a function or variable in one cell and use it in the next. For example: def print_me(str): print(str) ... and ... print_me(\"Hi!\") Your notebook should now look something like this. { width=700px } The focus of this tutorial is not on how to write Markdown or Python; you can make really pretty notebooks with Markdown and you can code whatever you want with Python. Rather, we will focus on the Jupyter Notebook features that allow you to do a little more than that. Quick recap In this section we've learned: That a Jupyter notebook consists of a series of cells, and that they can be either markdown or code cells. That we execute the code in a code cell with the kernel that we chose when opening the notebook. We can run shell commands by prepending them with ! . A Jupyter notebook is simply a text file in JSON format.","title":"The basics"},{"location":"jupyter/jupyter-2-the-basics/#writing-markdown","text":"Let's use our first cell to create a header. Change the format from Code to Markdown using the drop-down list in the Notebook Toolbar, or by pressing the m key when in command mode. Double click on the cell, or hit enter to enter editing mode (green frame) and input \"# My notebook\" (\"#\" is used in Markdown for header 1). Run the cell with ctrl - enter . Tada! Markdown is a simple way to structure your notebook into sections with descriptive notes, lists, links, images etc. Below are some examples of what you can do in markdown. Paste all or parts of it into one or more cells in your notebook to see how it renders. Make sure you set the cell type to Markdown. ## Introduction In this notebook I will try out some of the **fantastic** concepts of Jupyter Notebooks. ## Markdown basics Examples of text attributes are: * *italics* * **bold** * `monospace` Sections can be separated by horizontal lines. --- Blockquotes can be added, for instance to insert a Monty Python quote: Spam! Spam! Spam! Spam! See [here](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) for more information.","title":"Writing markdown"},{"location":"jupyter/jupyter-2-the-basics/#writing-code","text":"Now let's write some code! Since we chose a Python kernel, Python would be the native language to run in a cell. Enter this code in the second cell and run it: print(\"Hello world!\") Note how the output is displayed below the cell. This interactive way of working is one of the things that sets Jupyter Notebook apart from RStudio and R Markdown. R Markdown is typically rendered top-to-bottom in one run, while you work in a Jupyter notebook in a different way. This has partly changed with newer versions of RStudio, but it's probably still how most people use the two tools. What is a Jupyter notebook? Let's look a little at the notebook we're currently working in. Jupyter Notebooks are autosaved every minute or so, so you will already have it available. We can be a little meta and do this from within the notebook itself. We do it by running some shell commands in the third code cell instead of Python code. This very handy functionality is possible by prepending the command with ! . Try !ls to list the files in the current directory. Aha, we have a new file called Untitled.ipynb ! This is our notebook. Look at the first ten lines of the file by using !head Untitled.ipynb . Seems like it's just a plain old JSON file. Since it's a text file it's suitable for version control with for example Git. It turns out that Github and Jupyter notebooks are the best of friends, as we will see more of later. This switching between languages and whatever-works mentality is very prominent within the Jupyter notebook community. Variables defined in cells become variables in the global namespace. You can therefore share information between cells. Try to define a function or variable in one cell and use it in the next. For example: def print_me(str): print(str) ... and ... print_me(\"Hi!\") Your notebook should now look something like this. { width=700px } The focus of this tutorial is not on how to write Markdown or Python; you can make really pretty notebooks with Markdown and you can code whatever you want with Python. Rather, we will focus on the Jupyter Notebook features that allow you to do a little more than that. Quick recap In this section we've learned: That a Jupyter notebook consists of a series of cells, and that they can be either markdown or code cells. That we execute the code in a code cell with the kernel that we chose when opening the notebook. We can run shell commands by prepending them with ! . A Jupyter notebook is simply a text file in JSON format.","title":"Writing code"},{"location":"jupyter/jupyter-3-magics/","text":"Magics constitute a simple command language that significantly extends the power of Jupyter notebooks. There are two types of magics: Line magics : Commands that are prepended by % , and whose arguments only extend to the end of the line. Cell magics : Commands that start with %% and then applies to the whole cell. Must be written on the first line of a cell. Now list all available magics with %lsmagic (which itself is a magic). You add a question mark to a magic to show the help ( e.g. %lsmagic? ). Some of them act as shortcuts for commonly used shell commands ( %ls , %cp , %cat , ..). Others are useful for debugging and optimizing your code ( %timeit , %debug , %prun , ..). For more information see the magics documentation . A very useful magic, in particular when using shell commands a lot in your work, is %%capture . This will capture the stdout/stderr of any code cell and store them in a Python object. Run %%capture? to display the help and try to understand how it works. Try it out with either some Python code, other magics or shell commands. Here is an example of how you can make it work: %%capture output %%bash echo \"Print to stdout\" echo \"Print to stderr\" >&2 ... and in another cell: print(\"stdout:\" + output.stdout) print(\"stderr:\" + output.stderr) Tip You can capture the output of some magics directly like this: my_dir = %pwd . The %%script magic is used for specifying a program (bash, perl, ruby, ..) with which to run the code (similar to a shebang). For some languages it's possible to use these shortcuts: %%ruby %%perl %%bash %%html %%latex %%R (here you have to first install the rpy2 extension, for example with Conda, and then load with %load_ext rpy2.ipython ) Try this out if you know any of the languages above. Otherwise you can always try to print the quadratic formula with LaTeX! \\begin{array}{*{20}c} {x = \\frac{{ - b \\pm \\sqrt {b^2 - 4ac} }}{{2a}}} & {{\\rm{when}}} & {ax^2 + bx + c = 0} \\\\ \\end{array} Another useful magic is %precision which sets the floating point precision in the notebook. As a quick example, add the following to a cell and run it: float(100/3) Next set the precision to 4 decimal points by running a cell with: %precision 4 Now run the cell with float(100/3) again to see the difference. Running %precision without additional arguments will restore the default. Quick recap In this section we've learned: The basics of Jupyter magics and the difference between line magics and cell magics How to capture and use output from notebook cells with %%capture How to use magics to run non-Python code in notebooks","title":"Magics"},{"location":"jupyter/jupyter-4-plotting/","text":"An essential feature of Jupyter Notebooks is of course the ability to visualize data and results via plots. A full guide to plotting in Python is beyond the scope of this course, but we'll offer a few glimpses into the plotting landscape of Python. First of all, Python has a library for plotting called matplotlib , which comes packed with functionality for creating high-quality plots. Below is an example of how to generate a line plot of a sine wave. # Import packages import numpy as np import matplotlib.pyplot as plt # Generate a set of evenly spaced numbers between 0 and 100 x = np.linspace(0,3*np.pi,100) # Use the sine function to generate y-values y = np.sin(x) # Plot the data line, = plt.plot(x, y, color='red', linestyle=\"-\") By default plots are rendered in the notebook as rasterized images which can make the quality poor. To render in scalable vector graphics format use the set_matplotlib_formats from the matplotlib_inline package: import matplotlib_inline matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg') Now try running the code for the sine wave plot again. Other packages for plotting # As we mentioned Matplotlib comes with a lot of functionality which is great because it allows you to create all sorts of plots and modify them exactly to your liking. However, this can also mean that creating very basic plots might involve a lot of cumbersome coding, when all you want is a simple bar chart! Fortunately there are a number of Python packages that build upon matplotlib but with a much simplified interface. One such popular package is seaborn . Below we'll see how to generate a nice looking bar plot with error bars. First import the seaborn package (using an abbreviated name to simplify typing): import seaborn as sns Next we'll load some example data of penguins collected at the Palmer Station, in Antarctica. penguins = sns.load_dataset(\"penguins\") # Look at first 5 lines of the data penguins.head(5) The most basic way to generate a bar plot of this data with seaborn is: sns.barplot(data=penguins) Simple right? Yes, but maybe not very informative. Here seaborn simply calculates the mean of all numeric variables for the penguins and plots them with error bars representing a 95% confidence interval. Let's say that instead we want to plot the mean value of the body mass of the penguins at the different islands where they were examined. sns.barplot(data=penguins, x=\"island\", y=\"body_mass_g\", ci=\"sd\", errwidth=.5); Here we specified to use values in the 'island' column as categories for the x-axis, and values in the 'body_mass_g' column as values for the y-axis. The barplot function of seaborn will then calculate the mean body mass for each island and plot the bars. With ci=\"sd\" we tell the function to draw the standard deviation as error bars, instead of computing a confidence interval. Finally errwidth=.5 sets the linewidth of the error bars. If we instead want to visualize the data as a scatterplot we can use the sns.scatterplot function. Let's plot the body mass vs. bill length for all penguins and color the data points by species. We'll also move the legend outside of the plotting area and modify the x and y-axis labels: # Store the matplotlib axes containing the plot in a variable called 'ax' ax = sns.scatterplot(data=penguins, x=\"bill_length_mm\", y=\"body_mass_g\", hue=\"species\") # Modify the labels of the plot ax.set_xlabel(\"Bill length (mm)\") ax.set_ylabel(\"Body mass (g)\") # Set legend position outside of plot ax.legend(bbox_to_anchor=(1,1)); If you want to save a plot to file you can use the plt.savefig function. Add the following to the bottom of the cell with the scatterplot code: plt.savefig(\"scatterplot.pdf\", bbox_inches=\"tight\") The bbox_inches=\"tight\" setting ensures that the figure is not clipped when saved to file. The Seaborn website contains great tutorials and examples of other ways to plot data! Quick recap In this section we've learned: How to generate simple plots with matplotlib How to import and use the seaborn package for plotting How to save plots from notebooks to a file","title":"Plotting"},{"location":"jupyter/jupyter-4-plotting/#other-packages-for-plotting","text":"As we mentioned Matplotlib comes with a lot of functionality which is great because it allows you to create all sorts of plots and modify them exactly to your liking. However, this can also mean that creating very basic plots might involve a lot of cumbersome coding, when all you want is a simple bar chart! Fortunately there are a number of Python packages that build upon matplotlib but with a much simplified interface. One such popular package is seaborn . Below we'll see how to generate a nice looking bar plot with error bars. First import the seaborn package (using an abbreviated name to simplify typing): import seaborn as sns Next we'll load some example data of penguins collected at the Palmer Station, in Antarctica. penguins = sns.load_dataset(\"penguins\") # Look at first 5 lines of the data penguins.head(5) The most basic way to generate a bar plot of this data with seaborn is: sns.barplot(data=penguins) Simple right? Yes, but maybe not very informative. Here seaborn simply calculates the mean of all numeric variables for the penguins and plots them with error bars representing a 95% confidence interval. Let's say that instead we want to plot the mean value of the body mass of the penguins at the different islands where they were examined. sns.barplot(data=penguins, x=\"island\", y=\"body_mass_g\", ci=\"sd\", errwidth=.5); Here we specified to use values in the 'island' column as categories for the x-axis, and values in the 'body_mass_g' column as values for the y-axis. The barplot function of seaborn will then calculate the mean body mass for each island and plot the bars. With ci=\"sd\" we tell the function to draw the standard deviation as error bars, instead of computing a confidence interval. Finally errwidth=.5 sets the linewidth of the error bars. If we instead want to visualize the data as a scatterplot we can use the sns.scatterplot function. Let's plot the body mass vs. bill length for all penguins and color the data points by species. We'll also move the legend outside of the plotting area and modify the x and y-axis labels: # Store the matplotlib axes containing the plot in a variable called 'ax' ax = sns.scatterplot(data=penguins, x=\"bill_length_mm\", y=\"body_mass_g\", hue=\"species\") # Modify the labels of the plot ax.set_xlabel(\"Bill length (mm)\") ax.set_ylabel(\"Body mass (g)\") # Set legend position outside of plot ax.legend(bbox_to_anchor=(1,1)); If you want to save a plot to file you can use the plt.savefig function. Add the following to the bottom of the cell with the scatterplot code: plt.savefig(\"scatterplot.pdf\", bbox_inches=\"tight\") The bbox_inches=\"tight\" setting ensures that the figure is not clipped when saved to file. The Seaborn website contains great tutorials and examples of other ways to plot data! Quick recap In this section we've learned: How to generate simple plots with matplotlib How to import and use the seaborn package for plotting How to save plots from notebooks to a file","title":"Other packages for plotting"},{"location":"jupyter/jupyter-5-widgets/","text":"Since we're typically running our notebooks in a web browser, they are quite well suited for also including more interactive elements. A typical use case could be that you want to communicate some results to a collaborator or to a wider audience, and that you would like them to be able to modify how the results are displayed. It could, for example, be to select which gene to plot for, or to see how some parameter value affects a clustering. Jupyter notebooks has great support for this in the form of widgets . Widgets are eventful Python objects that have a representation in the browser, often as a control like a slider, textbox, etc. These are implemented in the ipywidgets package. The easiest way to get started with using widgets are via the interact and interactive functions. These functions autogenerate widgets from functions that you define, and then call those functions when you manipulate the widgets. Too abstract? Let's put it into practice! Let's try to add sliders that allow us to change the frequency, amplitude and phase of the sine curve we plotted previously. # Import the interactive function from ipywidgets from ipywidgets import interactive # Also import numpy (for calculating the sine curve) # and pyplot from matplotlib for plotting import numpy as np import matplotlib.pyplot as plt # Define the function for plotting the sine curve def sine_curve(A, f, p): # Set up the plot plt.figure(1, figsize=(4,4)) # Create a range of 100 evenly spaced numbers between 0 and 100 x = np.linspace(0,10,100) # Calculate the y values using the supplied parameters y = A*np.sin(x*f+p) # Plot the x and y values ('r-' specifies color and line style) plt.plot(x, y, color='red', linestyle=\"-\") plt.show() # Here we supply the sine_curve function to interactive, # and set some limits on the input parameters interactive_plot = interactive(sine_curve, A=(1, 5, 1), f=(0, 5, 1), p=(1, 5, 0.5)) # Display the widgets and the plot interactive_plot The code above defines a function called sine_curve which takes three arguments: A = the amplitude of the curve f = the frequency of the curve p = the phase of the curve The function creates a plot area, generates x-values and calculates y-values using the np.sin function and the supplied parameters. Finally, the x and y values are plotted. Below the function definition we use interactive with the sine_curve function as the first parameter. This means that the widgets will be tied to the sine_curve function. As you can see we also supply the A , f and p keyword arguments. Importantly, all parameters defined in the sine_curve function must be given in the interactive call and a widget is created for each one. Depending on the type of the passed argument different types of widgets will be created by interactive . For instance: int or float arguments will generate a slider bool arguments (True/False) will generate checkbox widgets list arguments will generate a dropdown str arguments will generate a text-box By supplying the arguments in the form of tuples we can adjust the properties of the sliders. f=(1, 5, 1) creates a widget with minimum value of 1 , maximum value of 5 and a step size of 1 . Try adjusting these numbers in the interactive call to see how the sliders change (you have to re-execute the cell). The final line of the cell ( interactive_plot ) is where the actual widgets and plot are displayed. This code can be put in a separate cell, so that you can define functions and widgets in one part of your notebook, and reuse them somewhere else. This is how it should look if everything works. You can now set the frequency amplitude and phase of the sine curve by moving the sliders. { width=700px } There are lots of widgets, e.g. : Dropdown menus Toggle buttons Range sliders File uploader ... and much, much more. Here is a list of all available widgets together with documentation and examples. Some of these widgets cannot be autogenerated by interactive , but fear not! Instead of relying on autogeneration we can define the widget and supply it directly to interactive . To see this in practice, change out the A argument to a pre-defined IntSlider widget. First define the slider: from ipywidgets import widgets A = widgets.IntSlider(value=2, min=1, max=5, step=1) Then replace the call to interactive so that it looks like this: interactive_plot = interactive(sine_curve, A=A, f=5, p=5) Extra challenge # If you can't get enough of widgets you might want to try this out: see if you can figure out how to add a widget that lets you pick the color for the sine curve line. Search for the appropriate widget in the Widget list . You'll need to update the sine_curve function and pass the new widget as an argument in the call to interactive . If you need help, see the code chunk below: Click to show # Import the interactive function from ipywidgets from ipywidgets import interactive # Also import numpy (for calculating the sine curve) # and pyplot from matplotlib for plotting import numpy as np from ipywidgets import widgets ## <- import widgets import matplotlib.pyplot as plt # Define the function for plotting the sine curve def sine_curve(A, f, p, color): ## <- add parameter here # Set up the plot plt.figure(1, figsize=(4,4)) # Create a range of 100 evenly spaced numbers between 0 and 100 x = np.linspace(0,10,100) # Calculate the y values using the supplied parameters y = A*np.sin(x*f+p) # Plot the x and y values plt.plot(x, y, color=color) ## <- Use color from widget here plt.show() # Here we supply the sine_curve function to interactive, # and set some limits on the input parameters # Define the colorpicker widget colorpicker = widgets.ColorPicker(description='color',value=\"red\") interactive_plot = interactive(sine_curve, A=(1, 5, 1), f=(0, 5, 1), p=(1, 5, 0.5), color=colorpicker) ## <- Supply the colorpicker to the function # Display the widgets and the plot interactive_plot Attention! Note that you may have to close the color picker once you've made your choice in order to make the plot update. Other interactive plots # Jupyter widgets, like we used here, is the most vanilla way of getting interactive graphs in Jupyter notebooks. Some other alternatives are: Plotly is actually an API to a web service that renders your graph and returns it for display in your Jupyter notebook. Generates very visually appealing graphs, but from a reproducibility perspective it's maybe not a good idea to be so reliant on a third party. Bokeh is another popular tool for interactive graphs. Most plotting packages for Python are built on top of matplotlib, but Bokeh has its own library. This can give a steeper learning curve if you're used to the standard packages. mpld3 tries to integrate matplotlib with Javascript and the D3js package. It doesn't scale well for very large datasets, but it's easy to use and works quite seamlessly. Quick recap In the three previous sections we've learned: How to implement interactive widgets in notebooks","title":"Widgets"},{"location":"jupyter/jupyter-5-widgets/#extra-challenge","text":"If you can't get enough of widgets you might want to try this out: see if you can figure out how to add a widget that lets you pick the color for the sine curve line. Search for the appropriate widget in the Widget list . You'll need to update the sine_curve function and pass the new widget as an argument in the call to interactive . If you need help, see the code chunk below: Click to show # Import the interactive function from ipywidgets from ipywidgets import interactive # Also import numpy (for calculating the sine curve) # and pyplot from matplotlib for plotting import numpy as np from ipywidgets import widgets ## <- import widgets import matplotlib.pyplot as plt # Define the function for plotting the sine curve def sine_curve(A, f, p, color): ## <- add parameter here # Set up the plot plt.figure(1, figsize=(4,4)) # Create a range of 100 evenly spaced numbers between 0 and 100 x = np.linspace(0,10,100) # Calculate the y values using the supplied parameters y = A*np.sin(x*f+p) # Plot the x and y values plt.plot(x, y, color=color) ## <- Use color from widget here plt.show() # Here we supply the sine_curve function to interactive, # and set some limits on the input parameters # Define the colorpicker widget colorpicker = widgets.ColorPicker(description='color',value=\"red\") interactive_plot = interactive(sine_curve, A=(1, 5, 1), f=(0, 5, 1), p=(1, 5, 0.5), color=colorpicker) ## <- Supply the colorpicker to the function # Display the widgets and the plot interactive_plot Attention! Note that you may have to close the color picker once you've made your choice in order to make the plot update.","title":"Extra challenge"},{"location":"jupyter/jupyter-5-widgets/#other-interactive-plots","text":"Jupyter widgets, like we used here, is the most vanilla way of getting interactive graphs in Jupyter notebooks. Some other alternatives are: Plotly is actually an API to a web service that renders your graph and returns it for display in your Jupyter notebook. Generates very visually appealing graphs, but from a reproducibility perspective it's maybe not a good idea to be so reliant on a third party. Bokeh is another popular tool for interactive graphs. Most plotting packages for Python are built on top of matplotlib, but Bokeh has its own library. This can give a steeper learning curve if you're used to the standard packages. mpld3 tries to integrate matplotlib with Javascript and the D3js package. It doesn't scale well for very large datasets, but it's easy to use and works quite seamlessly. Quick recap In the three previous sections we've learned: How to implement interactive widgets in notebooks","title":"Other interactive plots"},{"location":"jupyter/jupyter-6-extensions/","text":"Jupyter Notebook extensions are add-ons that can increase the functionality of your notebooks. These were installed in the setup section for this tutorial by including the jupyter_contrib_nbextensions package in the conda environment file. You can read more about the extensions here . To manage extensions go to the Jupyter dashboard in your browser and click the Nbextensions tab. You should see something similar to this: { width=700px } Clicking an extension in the list displays more information about it. To enable/disable extensions simply click the checkbox next to the extension name in the list. Some useful extensions include Hide input all , which allows you to hide all code cells with the click of a button. Collapsible Headings , which allows you to collapse sections below markdown headings to increase readability. Table of Contents (2) , which adds a table of contents to the notebook making navigation a lot quicker especially for long notebooks. Feel free to peruse the list and find your own favourites! Keep in mind that these are unofficial, community-contributed extensions and as such they come with few, if any, guarantees. Quick recap In this section we've learned: What Jupyter extensions are and how to enable/disable them","title":"Extensions"},{"location":"jupyter/jupyter-7-converting-notebooks/","text":"Notebooks can be converted to various output formats such as HTML, PDF, LaTeX etc. directly from the File -> Download as menu. Conversion can also be performed on the command line using the jupyter nbconvert command. nbconvert is installed together with the jupyter Conda package and is executed on the command line by running jupyter nbconvert . The syntax for converting a Jupyter notebook is: jupyter nbconvert --to <FORMAT> notebook.ipynb Here <FORMAT> can be any of asciidoc , custom , html , latex , markdown , notebook , pdf , python , rst , script , slides . Converting to some output formats ( e.g. PDF) may require you to install separate software such as Pandoc or a TeX environment. Try converting the Untitled.ipynb notebook that you have been working on so far to HTML using jupyter nbconvert . Tip To export notebooks in the form they appear with Jupyter Extensions activated you can make use of the nbextensions template that is installed with the jupyter_contrib_nbextensions package. Adding --template=nbextensions to the jupyter nbconvert call should do the trick, but note that not all extensions are guaranteed to display right after exporting. nbconvert can also be used to run a Jupyter notebook from the command line by running: jupyter nbconvert --execute --to <FORMAT> notebook.ipynb nbconvert executes the cells in a notebook, captures the output and saves the results in a new file. Try running it on the Untitled.ipynb notebook. You can also specify a different output file with --output <filename> . So in order to execute your Untitled.ipynb notebook and save it to a file named report.html you could run: jupyter nbconvert --to html --output report.html --execute Untitled.ipynb Quick recap In this section we've learned: How to convert Jupyter notebooks to various other formats How to use nbconvert to convert notebooks on the command line","title":"Converting notebooks"},{"location":"jupyter/jupyter-8-the-mrsa-case-study/","text":"As you might remember from the intro , we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus ). We have already seen how to define the project environment in the Conda tutorial and how to set up the workflow in the Snakemake tutorial . Here we explore the results from the Snakemake tutorial and generate a Supplementary Material file with some basic stats. In the workshop-reproducible-research/tutorials/jupyter/ directory you will find a notebook called supplementary_material.ipynb . Open this notebook with Jupyter by running: jupyter notebook supplementary_material.ipynb Tip Using what you've learned about markdown in notebooks, add headers and descriptive text to subdivide sections as you add them. This will help you train how to structure and keep note of your work with a notebook. You will see that the notebook contains only a little markdown text and a code cell with a function get_geodata . We'll start by adding a cell with some import statements. Run the cell with the get_geodata function and add a new cell directly after it. Then add the following to the new cell: import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np This imports the pandas (for working with tables), seaborn and matplotlib.pyplot (for plotting) and numpy (for numerical operations) Python modules. Also add: import matplotlib_inline matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg') to set high-quality output for plots. Run the cell and create a new one below it. In the next cell we'll define some parameters to use for the notebook: counts_file=\"results/tables/counts.tsv\" summary_file=\"results/tables/counts.tsv.summary\" multiqc_file=\"intermediate/multiqc_general_stats.txt\" rulegraph_file=\"results/rulegraph.png\" SRR_IDs=[\"SRR935090\",\"SRR935091\",\"SRR935092\"] GSM_IDs=[\"GSM1186459\",\"GSM1186460\",\"GSM1186461\"] GEO_ID=\"GSE48896\" As you can see we add paths to results files and define lists with some sample IDS. Run this cell and add a new one below it. Next, we'll fetch some sample information from NCBI using the get_geodata function defined at the start of the notebook and collate it into a dataframe. id_df = pd.DataFrame(data=GSM_IDs, index=SRR_IDs, columns=[\"geo_accession\"]) geo_df = get_geodata(GEO_ID) name_df = pd.merge(id_df, geo_df, left_on=\"geo_accession\", right_index=True) # Create a dictionary to rename sample ids in downstream plots name_dict = name_df.to_dict() Run the cell and take a look at the contents of the name_df dataframe ( e.g. run a cell with that variable only to output it below the cell). Now we'll load some statistics from the QC part of the workflow, specifically the 'general_stats' file from multiqc . Add the following to a new cell and run it: qc = pd.read_csv(multiqc_file, sep=\"\\t\") qc.rename(columns=lambda x: x.replace(\"FastQC_mqc-generalstats-fastqc-\", \"\").replace(\"_\", \" \"), inplace=True) qc = pd.merge(qc, name_df, left_on=\"Sample\", right_index=True) qc In the code above we load the multiqc file, rename the columns by stripping the FastQC_mqc-generalstats-fastqc- part from column names and replace underscores with spaces. Finally the table is merged with the information obtained in the step above and output to show summary statistics from the QC stage. Next it's time to start loading gene count results from the workflow. Start by reading the counts and summary results, then edit the columns and index: # Read count data counts = pd.read_csv(counts_file, sep=\"\\t\", header=0, comment=\"#\", index_col=0) # Read summary data counts_summary = pd.read_csv(summary_file, sep=\"\\t\", index_col=0) # Rename columns to extract SRR ids counts.rename(columns = lambda x: x.split(\"/\")[-1].replace(\".sorted.bam\",\"\"), inplace=True) counts_summary.rename(columns = lambda x: x.split(\"/\")[-1].replace(\".sorted.bam\",\"\"), inplace=True) Take a look at the counts dataframe to get an idea of the data structure. As you can see the dataframe shows genes as rows while the columns shows various information such as start and stop, strand and length of the genes. The last three columns contain counts of the genes in each of the samples. If you have a look at the counts_summary dataframe you will see statistics from the read assignment step, showing number of reads that could be properly assigned as well as number of reads that could not be assigned to genes for various reasons. Now let's generate a barplot of the summary statistics. Before we plot, we'll remove rows that have only zero values: # Remove rows with only zero values summary_plot_data = counts_summary.loc[counts_summary.sum(axis=1)>0] Now for the plotting: # Set color palette to 'Set2' colors = sns.color_palette(\"Set2\") # Create a stacked barplot ax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors) # Move legend and set legend title ax.legend(bbox_to_anchor=(1,1), title=\"Category\"); The final plot will be a heatmap of gene counts for a subset of the genes. We'll select genes whose standard deviation/mean count across samples is greater than 1.5, and have a maximum of at least 5 reads in 1 or more sample: # Slice the dataframe to only sample counts count_data = counts.loc[:, SRR_IDs] # Filter to genes with std/mean > 1.2 and with a max of at least 5 heatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))>1.2)&(count_data.max(axis=1)>5)] We'll also replace the SRR ids with the title of samples used in the study, using the name_dict dictionary created further up in the notebook: heatmap_data = heatmap_data.rename(columns = name_dict['title']) Now let's plot the heatmap. We'll log-transform the counts, set color scale to Blue-Yellow-Red and cluster both samples and genes using 'complete' linkage clustering: with sns.plotting_context(\"notebook\", font_scale=0.7): ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\", method=\"complete\", yticklabels=True, linewidth=.5, cbar_pos=(.7, .85, .05, .1), figsize=(3,9)) plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270) In the code above we use the seaborn plotting_context function to scale all text elements of the heatmap in one go. As a final step we'll add some info for reproducibility under the Reproducibility section. To add the overview image of the workflow found in results/rulegraph.png we can use the Image function from IPython.display : from IPython.display import Image Image(rulegraph_file) Let's also output the full conda environment so that all packages and versions are included in the notebook. There are several ways this can be done, for example you could simply add: !conda list to the end of the notebook. Tip If you want to know more about how notebooks can be integrated into Snakemake worfklows, see the Extra material at the end of this tutorial Sharing your work # The files you're working with come from a GitHub repo. Both GitHub and Bitbucket can render Jupyter notebooks as well as other types of Markdown documents. Now go to our GitHub repo at https://github.com/NBISweden/workshop-reproducible-research and navigate to tutorials/jupyter/supplementary_material.ipynb . { width=700px } As you can imagine, having this very effortless way of sharing results can greatly increase the visibility of your work. You work as normal on your project, and push regularly to the repository as you would anyways, and the output is automatically available for anyone to see. Or for a select few if you're not ready to share your findings with the world quite yet. Say your notebook isn't on Github/Bitbucket. All hope isn't lost there. Jupyter.org provides a neat functionality called nbviewer , where you can paste a URL to any notebook and they will render it for you. Go to https://nbviewer.jupyter.org and try this out with our notebook. https://raw.githubusercontent.com/NBISweden/workshop-reproducible-research/main/tutorials/jupyter/supplementary_material.ipynb Shared interactive notebooks # So far we've only shared static representations of notebooks. A strong trend at the moment is to run your notebooks in the cloud, so that the person you want to share with could actually execute and modify your code. This is a great way of increasing visibility and letting collaborators or readers get more hands-on with your data and analyses. From a reproducibility perspective, there are both advantages and drawbacks. On the plus side is that running your work remotely forces you to be strict when it comes to defining the environment it uses (probably in the form of a Conda environment or Docker image). On the negative side is that you become reliant on a third-party service that might change input formats, go out of business, or change payment model. Here we will try out a service called Binder, which lets you run and share Jupyter Notebooks in Git repositories for free. There are a number of example repositories that are setup to be used with Binder. Navigate to https://github.com/binder-examples/conda/ to see one such example. As you can see the repository contains a LICENSE file, a README, an environment file and a notebook. To use a repository with Binder the environment file should contain all the packages needed to run notebooks in the repo. So let's try to run the index.ipynb file using Binder: Just go to https://mybinder.org and paste the link to the GitHub repo. Note the link that you can use to share your notebook. Then press \"launch\". { width=700px } What will happen now is that: Binder detects the environment.yml file in the root of the repo. Binder then builds a Docker image based on the file. This might take a minute or two. You can follow the progress in the build log. Binder then launches the Jupyter Notebook server in the Docker container.. ..and opens a browser tab with it for you. Once the process is finished you will be presented with a Jupyter server overview of the contents in the repository. Click on the index.ipynb notebook to open it. Tada! You are now able to interact with (and modify) someone else's notebook online. Applied to your own projects you now have a way to run analyses in the cloud and in an environment that you define yourself. All that's needed for someone to replicate your analyses is that you share a link with them. Note that notebooks on Binder are read-only; its purpose is for trying out and showing existing notebooks rather than making new ones. Tip By default Binder looks for configuration files such as environment.yml in the root of the repository being built. But you may also put such files outside the root by making a binder/ folder in the root and placing the file there. A note on transparency Resources like Github/Bitbucket and Jupyter Notebooks have changed the way we do scientific research by encouraging visibility, social interaction and transparency. It was not long ago that the analysis scripts and workflows in a lab were well-guarded secrets that we only most reluctantly shared with others. Assuming that it was even possible. In most cases, the only postdoc who knew how to get it to work had left for a new position in industry, or no one could remember the password to the file server. If you're a PhD student, we encourage you to embrace this new development wholeheartedly, for it will make your research better and make you into a better scientist. And you will have more fun. Quick recap In this section we've learned: How notebooks can be used to generate summary statistics and plots using the results of a workflow run How to share notebooks via nbviewer and Binder","title":"The MRSA case study"},{"location":"jupyter/jupyter-8-the-mrsa-case-study/#sharing-your-work","text":"The files you're working with come from a GitHub repo. Both GitHub and Bitbucket can render Jupyter notebooks as well as other types of Markdown documents. Now go to our GitHub repo at https://github.com/NBISweden/workshop-reproducible-research and navigate to tutorials/jupyter/supplementary_material.ipynb . { width=700px } As you can imagine, having this very effortless way of sharing results can greatly increase the visibility of your work. You work as normal on your project, and push regularly to the repository as you would anyways, and the output is automatically available for anyone to see. Or for a select few if you're not ready to share your findings with the world quite yet. Say your notebook isn't on Github/Bitbucket. All hope isn't lost there. Jupyter.org provides a neat functionality called nbviewer , where you can paste a URL to any notebook and they will render it for you. Go to https://nbviewer.jupyter.org and try this out with our notebook. https://raw.githubusercontent.com/NBISweden/workshop-reproducible-research/main/tutorials/jupyter/supplementary_material.ipynb","title":"Sharing your work"},{"location":"jupyter/jupyter-8-the-mrsa-case-study/#shared-interactive-notebooks","text":"So far we've only shared static representations of notebooks. A strong trend at the moment is to run your notebooks in the cloud, so that the person you want to share with could actually execute and modify your code. This is a great way of increasing visibility and letting collaborators or readers get more hands-on with your data and analyses. From a reproducibility perspective, there are both advantages and drawbacks. On the plus side is that running your work remotely forces you to be strict when it comes to defining the environment it uses (probably in the form of a Conda environment or Docker image). On the negative side is that you become reliant on a third-party service that might change input formats, go out of business, or change payment model. Here we will try out a service called Binder, which lets you run and share Jupyter Notebooks in Git repositories for free. There are a number of example repositories that are setup to be used with Binder. Navigate to https://github.com/binder-examples/conda/ to see one such example. As you can see the repository contains a LICENSE file, a README, an environment file and a notebook. To use a repository with Binder the environment file should contain all the packages needed to run notebooks in the repo. So let's try to run the index.ipynb file using Binder: Just go to https://mybinder.org and paste the link to the GitHub repo. Note the link that you can use to share your notebook. Then press \"launch\". { width=700px } What will happen now is that: Binder detects the environment.yml file in the root of the repo. Binder then builds a Docker image based on the file. This might take a minute or two. You can follow the progress in the build log. Binder then launches the Jupyter Notebook server in the Docker container.. ..and opens a browser tab with it for you. Once the process is finished you will be presented with a Jupyter server overview of the contents in the repository. Click on the index.ipynb notebook to open it. Tada! You are now able to interact with (and modify) someone else's notebook online. Applied to your own projects you now have a way to run analyses in the cloud and in an environment that you define yourself. All that's needed for someone to replicate your analyses is that you share a link with them. Note that notebooks on Binder are read-only; its purpose is for trying out and showing existing notebooks rather than making new ones. Tip By default Binder looks for configuration files such as environment.yml in the root of the repository being built. But you may also put such files outside the root by making a binder/ folder in the root and placing the file there. A note on transparency Resources like Github/Bitbucket and Jupyter Notebooks have changed the way we do scientific research by encouraging visibility, social interaction and transparency. It was not long ago that the analysis scripts and workflows in a lab were well-guarded secrets that we only most reluctantly shared with others. Assuming that it was even possible. In most cases, the only postdoc who knew how to get it to work had left for a new position in industry, or no one could remember the password to the file server. If you're a PhD student, we encourage you to embrace this new development wholeheartedly, for it will make your research better and make you into a better scientist. And you will have more fun. Quick recap In this section we've learned: How notebooks can be used to generate summary statistics and plots using the results of a workflow run How to share notebooks via nbviewer and Binder","title":"Shared interactive notebooks"},{"location":"jupyter/jupyter-9-extra-material/","text":"Here are some useful resources if you want to read more about Jupyter in general: The Jupyter project site contains a lot of information and inspiration. The Jupyter Notebook documentation . A guide to using widgets for creating interactive notebooks. Running jupyter notebooks on a cluster # Login to Uppmax, making sure to use a specific login node, e.g. rackham1 : ssh <your-user-name>@rackham1.uppmax.uu.se Create/activate a conda environment containing jupyter then run: jupyter notebook When the Jupyter server starts up you should see something resembling: [I 11:00:00.000 NotebookApp] Serving notebooks from local directory: <path-to-your-local-dir> [I 11:00:00.000 NotebookApp] Jupyter Notebook 6.4.6 is running at: [I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000 [I 11:00:00.000 NotebookApp] or http://127.0.0.1:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000 [I 11:00:00.000 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). Now a Jupyter notebook server is running on the Uppmax end. The line that says: [I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000 contains information on the port used on the server side (8889 in this case) and the token required to use the server ( 357d65100058efa40a0641fce7005addcff339876c5e8000 ). Next step is to use this information to login to the server from your local computer. On your local computer In a terminal, run the following command to start port forwarding of port 8080 on your local computer to the remote port on the Uppmax side. Replace with the port given when you started the server on Uppmax. Also replace with your user name on Uppmax. ssh -N -L localhost:8080:localhost:<remote-port> <your-user-name>@rackham1.uppmax.uu.se As long as this process is running the port forwarding is running. To disable it simply interrupt it with CTRL + C . Connect to the jupyter server by opening localhost:8080 in your browser. When prompted, paste the token you got when starting the server on Uppmax. You are now (hopefully) accessing the jupyter server that's running on Uppmax, via your local browser. Integrating notebooks with Snakemake workflows # In the case study section of this tutorial we created a Jupyter notebook that used output from a Snakemake workflow and produced some summary results and plots. Wouldn't it be nice if this was actually part of the workflow itself? To generate a HTML version of the notebook we can use what we learned in the section about converting notebooks . The command to execute the notebook and save it in HTML format in a file results/supplementary.html would be: jupyter nbconvert --to HTML --output-dir results --output supplementary.html --execute supplementary_material.ipynb This command could be used in a rule, e.g. make_supplementary , the input of which would be results/tables/counts.tsv , intermediate/multiqc_general_stats.txt , and results/rulegraph.png . See if you can work out how to implement such a rule at the end of the Snakefile found in the jupyter/ directory. You can find an example in the code chunk below: rule make_supplementary: input: counts = \"results/tables/counts.tsv\", summary = \"results/tables/counts.tsv.summary\", multiqc_file = \"intermediate/multiqc_general_stats.txt\", rulegraph = \"results/rulegraph.png\" output: \"results/supplementary.html\" params: base = lambda wildcards, output: os.path.basename(output[0]), dir = lambda wildcards, output: os.path.dirname(output[0]) shell: \"\"\" jupyter nbconvert --to HTML --output-dir {params.dir} --output {params.base} \\ --execute supplementary_material.ipynb \"\"\" Note The Conda enivronment for the jupyter tutorial does not contain packages required to run the full snakemake workflow. So if you wish to test jupyter integration fully you should update the conda environment by running conda install snakemake-minimal fastqc sra-tools multiqc bowtie2 tbb samtools htseq bedtools wget graphviz More integrations # Snakemake actually supports the execution of notebooks via the notebook: rules directive. See more about Jupyter integration in the snakemake docs . In the notebook: directive of such a rule you specify the path to a jupyter notebook (relative to the Snakefile) which is then executed when the rule is run. So how is this useful? In the notebook itself this gives you access to a snakemake object containing information about input and output files for the rule via snakemake.input and snakemake.output . Similarly, you can access rule wildcards with snakemake.wildcards , params with snakemake.params , and config settings with snakemake.config . When snakemake runs the rule with the notebook: directive jupyter-nbconvert is used to execute the notebook. No HTML output is generated here but it is possible to store a version of the notebook in its final processed form by adding the following to the rule: log: notebook=\"<path>/<to>/<processed>/<notebook.ipynb>\" Because you won't get the notebook in full HTML glory, this type of integration is better suited if you want to use a notebook to generate figures and store these in local files ( e.g. pdf/svg/png formats). We'll use the supplementary_material.ipynb notebook as an example! Let's say that instead of exporting the entire notebook to HTML we want a rule that outputs pdf versions of the barplot and heatmap figures we created. Let's start by setting up the rule. For simplicity we'll use the same input as when we edited the notebook in the first place. The output will be results/barplot.pdf and results/heatmap.pdf . Let's also output a finalized version of the notebook using the log: notebook= directive: rule make_supplementary_plots: input: counts = \"results/tables/counts.tsv\", summary = \"results/tables/counts.tsv.summary\", multiqc = \"intermediate/multiqc_general_stats.txt\", rulegraph = \"results/rulegraph.png\" output: barplot = \"results/barplot.pdf\", heatmap = \"results/heatmap.pdf\" log: notebook = \"results/supplementary.ipynb\" The notebook will now have access to snakemake.input.counts , snakemake.output.barplot and snakemake.output.heatmap when executed from within the workflow. Let's go ahead and edit the notebook! In the cell where we defined notebook parameters edit the code so that it looks like this: counts_file=snakemake.input.counts summary_file=snakemake.input.summary multiqc_file=snakemake.input.multiqc rulegraph_file=snakemake.input.rulegraph SRR_IDs=snakemake.params.SRR_IDs GSM_IDs=snakemake.params.GSM_IDs GEO_ID=snakemake.params.GEO_ID Notice that we set the SRR_IDs , GSM_IDs and GEO_ID variables using variables in snakemake.params ? However, we haven't defined these in our rule yet so let's go ahead and do that now. Add the params section so that the make_supplementary_plots in the Snakefile looks like this: rule make_supplementary_plots: input: counts = \"results/tables/counts.tsv\", multiqc = \"intermediate/multiqc_general_stats.txt\", rulegraph = \"results/rulegraph.png\" output: barplot = \"results/barplot.pdf\", heatmap = \"results/heatmap.pdf\" log: notebook = \"results/supplementary.ipynb\" params: SRR_IDs = [\"SRR935090\",\"SRR935091\",\"SRR935092\"], GSM_IDs = [\"GSM1186459\", \"GSM1186460\", \"GSM1186461\"], GEO_ID = \"GSE48896\" notebook: \"supplementary_material.ipynb\" Tip One way to further generalize this rule could be to define the SRR_IDs, GSM_IDs and GEO_ID parameters in a config file instead, in which case they would be directly accessible from within the notebook using snakemake.config['SRR_IDs'] etc. Now the rule contains everything needed, but we still need to edit the notebook to save the plots to the output files. First, edit the cell that generates the barplot so that it looks like this: # Create a stacked barplot ax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors) # Move legend and set legend title ax.legend(bbox_to_anchor=(1,1), title=\"Category\"); plt.savefig(snakemake.output.barplot, dpi=300, bbox_inches=\"tight\") ## <-- Add this line! Finally, edit the cell that generates the heatmap so that it looks like this: count_data = counts.loc[:, SRR_IDs] heatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))>1.2)&(count_data.max(axis=1)>5)] heatmap_data = heatmap_data.rename(columns = name_dict['title']) with sns.plotting_context(\"notebook\", font_scale=0.7): ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\", method=\"complete\", yticklabels=True, linewidth=.5, cbar_pos=(.7, .85, .05, .1), figsize=(3,9)) plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270) plt.savefig(snakemake.output.heatmap, dpi=300, bbox_inches=\"tight\") ## <-- Add this line! Now you can run the following to generate the plots: snakemake -j 1 make_supplementary_plots Presentations with Jupyter # As if all the above wasn't enough you can also create presentations/slideshows with Jupyter! Simply use conda to install the RISE extension to your jupyter environment: conda install -c conda-forge rise then open up a notebook of your choice. In the menu click View -> Cell Toolbar -> Slideshow . Now every cell will have a drop-down in the upper right corner allowing you to set the cell type: Slide : a regular slide Sub-Slide : a regular slide that will be displayed below the previous Fragment : these cells split up slides so that content (fragments) are added only when you press Space Skip : these cells will not appear in the presentation Notes : these cells act as notes, shown in the speaker view but not in the main view The presentation can be run directly from the notebook by clicking the 'Enter/Exit RISE Slideshow' button (looks like a bar chart) in the toolbar, or by using the keyboard shortcut Alt-r . Running it directly from a notebook means you can also edit and run cells during your presentation. The downside is that the presentation is not as portable because it may rely on certain software packages that other people are not comfortable with installing. You can also export the notebook to an HTML-file with jupyter nbconvert --execute --to SLIDES <your-notebook.ipynb> . The resulting file, with the slideshow functionality included, can be opened in any browser. However, in this format you cannot run/edit cells.","title":"Extra material"},{"location":"jupyter/jupyter-9-extra-material/#running-jupyter-notebooks-on-a-cluster","text":"Login to Uppmax, making sure to use a specific login node, e.g. rackham1 : ssh <your-user-name>@rackham1.uppmax.uu.se Create/activate a conda environment containing jupyter then run: jupyter notebook When the Jupyter server starts up you should see something resembling: [I 11:00:00.000 NotebookApp] Serving notebooks from local directory: <path-to-your-local-dir> [I 11:00:00.000 NotebookApp] Jupyter Notebook 6.4.6 is running at: [I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000 [I 11:00:00.000 NotebookApp] or http://127.0.0.1:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000 [I 11:00:00.000 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). Now a Jupyter notebook server is running on the Uppmax end. The line that says: [I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000 contains information on the port used on the server side (8889 in this case) and the token required to use the server ( 357d65100058efa40a0641fce7005addcff339876c5e8000 ). Next step is to use this information to login to the server from your local computer. On your local computer In a terminal, run the following command to start port forwarding of port 8080 on your local computer to the remote port on the Uppmax side. Replace with the port given when you started the server on Uppmax. Also replace with your user name on Uppmax. ssh -N -L localhost:8080:localhost:<remote-port> <your-user-name>@rackham1.uppmax.uu.se As long as this process is running the port forwarding is running. To disable it simply interrupt it with CTRL + C . Connect to the jupyter server by opening localhost:8080 in your browser. When prompted, paste the token you got when starting the server on Uppmax. You are now (hopefully) accessing the jupyter server that's running on Uppmax, via your local browser.","title":"Running jupyter notebooks on a cluster"},{"location":"jupyter/jupyter-9-extra-material/#integrating-notebooks-with-snakemake-workflows","text":"In the case study section of this tutorial we created a Jupyter notebook that used output from a Snakemake workflow and produced some summary results and plots. Wouldn't it be nice if this was actually part of the workflow itself? To generate a HTML version of the notebook we can use what we learned in the section about converting notebooks . The command to execute the notebook and save it in HTML format in a file results/supplementary.html would be: jupyter nbconvert --to HTML --output-dir results --output supplementary.html --execute supplementary_material.ipynb This command could be used in a rule, e.g. make_supplementary , the input of which would be results/tables/counts.tsv , intermediate/multiqc_general_stats.txt , and results/rulegraph.png . See if you can work out how to implement such a rule at the end of the Snakefile found in the jupyter/ directory. You can find an example in the code chunk below: rule make_supplementary: input: counts = \"results/tables/counts.tsv\", summary = \"results/tables/counts.tsv.summary\", multiqc_file = \"intermediate/multiqc_general_stats.txt\", rulegraph = \"results/rulegraph.png\" output: \"results/supplementary.html\" params: base = lambda wildcards, output: os.path.basename(output[0]), dir = lambda wildcards, output: os.path.dirname(output[0]) shell: \"\"\" jupyter nbconvert --to HTML --output-dir {params.dir} --output {params.base} \\ --execute supplementary_material.ipynb \"\"\" Note The Conda enivronment for the jupyter tutorial does not contain packages required to run the full snakemake workflow. So if you wish to test jupyter integration fully you should update the conda environment by running conda install snakemake-minimal fastqc sra-tools multiqc bowtie2 tbb samtools htseq bedtools wget graphviz","title":"Integrating notebooks with Snakemake workflows"},{"location":"jupyter/jupyter-9-extra-material/#more-integrations","text":"Snakemake actually supports the execution of notebooks via the notebook: rules directive. See more about Jupyter integration in the snakemake docs . In the notebook: directive of such a rule you specify the path to a jupyter notebook (relative to the Snakefile) which is then executed when the rule is run. So how is this useful? In the notebook itself this gives you access to a snakemake object containing information about input and output files for the rule via snakemake.input and snakemake.output . Similarly, you can access rule wildcards with snakemake.wildcards , params with snakemake.params , and config settings with snakemake.config . When snakemake runs the rule with the notebook: directive jupyter-nbconvert is used to execute the notebook. No HTML output is generated here but it is possible to store a version of the notebook in its final processed form by adding the following to the rule: log: notebook=\"<path>/<to>/<processed>/<notebook.ipynb>\" Because you won't get the notebook in full HTML glory, this type of integration is better suited if you want to use a notebook to generate figures and store these in local files ( e.g. pdf/svg/png formats). We'll use the supplementary_material.ipynb notebook as an example! Let's say that instead of exporting the entire notebook to HTML we want a rule that outputs pdf versions of the barplot and heatmap figures we created. Let's start by setting up the rule. For simplicity we'll use the same input as when we edited the notebook in the first place. The output will be results/barplot.pdf and results/heatmap.pdf . Let's also output a finalized version of the notebook using the log: notebook= directive: rule make_supplementary_plots: input: counts = \"results/tables/counts.tsv\", summary = \"results/tables/counts.tsv.summary\", multiqc = \"intermediate/multiqc_general_stats.txt\", rulegraph = \"results/rulegraph.png\" output: barplot = \"results/barplot.pdf\", heatmap = \"results/heatmap.pdf\" log: notebook = \"results/supplementary.ipynb\" The notebook will now have access to snakemake.input.counts , snakemake.output.barplot and snakemake.output.heatmap when executed from within the workflow. Let's go ahead and edit the notebook! In the cell where we defined notebook parameters edit the code so that it looks like this: counts_file=snakemake.input.counts summary_file=snakemake.input.summary multiqc_file=snakemake.input.multiqc rulegraph_file=snakemake.input.rulegraph SRR_IDs=snakemake.params.SRR_IDs GSM_IDs=snakemake.params.GSM_IDs GEO_ID=snakemake.params.GEO_ID Notice that we set the SRR_IDs , GSM_IDs and GEO_ID variables using variables in snakemake.params ? However, we haven't defined these in our rule yet so let's go ahead and do that now. Add the params section so that the make_supplementary_plots in the Snakefile looks like this: rule make_supplementary_plots: input: counts = \"results/tables/counts.tsv\", multiqc = \"intermediate/multiqc_general_stats.txt\", rulegraph = \"results/rulegraph.png\" output: barplot = \"results/barplot.pdf\", heatmap = \"results/heatmap.pdf\" log: notebook = \"results/supplementary.ipynb\" params: SRR_IDs = [\"SRR935090\",\"SRR935091\",\"SRR935092\"], GSM_IDs = [\"GSM1186459\", \"GSM1186460\", \"GSM1186461\"], GEO_ID = \"GSE48896\" notebook: \"supplementary_material.ipynb\" Tip One way to further generalize this rule could be to define the SRR_IDs, GSM_IDs and GEO_ID parameters in a config file instead, in which case they would be directly accessible from within the notebook using snakemake.config['SRR_IDs'] etc. Now the rule contains everything needed, but we still need to edit the notebook to save the plots to the output files. First, edit the cell that generates the barplot so that it looks like this: # Create a stacked barplot ax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors) # Move legend and set legend title ax.legend(bbox_to_anchor=(1,1), title=\"Category\"); plt.savefig(snakemake.output.barplot, dpi=300, bbox_inches=\"tight\") ## <-- Add this line! Finally, edit the cell that generates the heatmap so that it looks like this: count_data = counts.loc[:, SRR_IDs] heatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))>1.2)&(count_data.max(axis=1)>5)] heatmap_data = heatmap_data.rename(columns = name_dict['title']) with sns.plotting_context(\"notebook\", font_scale=0.7): ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\", method=\"complete\", yticklabels=True, linewidth=.5, cbar_pos=(.7, .85, .05, .1), figsize=(3,9)) plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270) plt.savefig(snakemake.output.heatmap, dpi=300, bbox_inches=\"tight\") ## <-- Add this line! Now you can run the following to generate the plots: snakemake -j 1 make_supplementary_plots","title":"More integrations"},{"location":"jupyter/jupyter-9-extra-material/#presentations-with-jupyter","text":"As if all the above wasn't enough you can also create presentations/slideshows with Jupyter! Simply use conda to install the RISE extension to your jupyter environment: conda install -c conda-forge rise then open up a notebook of your choice. In the menu click View -> Cell Toolbar -> Slideshow . Now every cell will have a drop-down in the upper right corner allowing you to set the cell type: Slide : a regular slide Sub-Slide : a regular slide that will be displayed below the previous Fragment : these cells split up slides so that content (fragments) are added only when you press Space Skip : these cells will not appear in the presentation Notes : these cells act as notes, shown in the speaker view but not in the main view The presentation can be run directly from the notebook by clicking the 'Enter/Exit RISE Slideshow' button (looks like a bar chart) in the toolbar, or by using the keyboard shortcut Alt-r . Running it directly from a notebook means you can also edit and run cells during your presentation. The downside is that the presentation is not as portable because it may rely on certain software packages that other people are not comfortable with installing. You can also export the notebook to an HTML-file with jupyter nbconvert --execute --to SLIDES <your-notebook.ipynb> . The resulting file, with the slideshow functionality included, can be opened in any browser. However, in this format you cannot run/edit cells.","title":"Presentations with Jupyter"},{"location":"nextflow/nextflow-1-introduction/","text":"Nextflow is a workflow management system (WfMS), and is one of the most common such systems within the bioinformatic and academic communities. These systems are important for scientific reproducibility in that they greatly facilitate keeping track of which files have been processed in what way throughout an entire project. Nextflow is built from the ground-up to be portable, scalable, reproducible and usable in a platform-agnostic sense. This means that any workflow you write in Nextflow can be run locally on your laptop, a computer cluster or a cloud service (as long as your architecture has the necessary computational resources). You can also define the compute environment in which each task is carried out on a per-task basis. You might thus develop your workflow on your local computer using a minimal test dataset, but run the full analyses with all samples on e.g. a computer cluster. Nextflow can work on both files and arbitrary values, oftentimes connected in useful and advanced ways. Nextflow can easily work with dynamic inputs where the exact output is unknown, e.g. the exact number of files or which samples pass some arbitrary quality control threshold. While Nextflow is based on the Groovy language, you don't need to know how to code Groovy to be able to write good Nextflow workflows. Nextflow has a large community centered around it, including the nf-core curated collection of high quality pipelines used by e.g. the National Genomics Infrastructure . This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already, then open up a terminal and go to workshop-reproducible-research/tutorials/nextflow and activate your nextflow-env Conda environment.","title":"Introduction"},{"location":"nextflow/nextflow-2-the-basics/","text":"We'll start by creating a very simple workflow from scratch, to show how Nextflow works: it will take two input files and convert them to UPPERCASE letters. Start by running the following commands: touch main.nf echo \"This is a.txt\" > a.txt echo \"This is b.txt\" > b.txt Open the main.nf file with an editor of your choice. This is the main workflow file used in Nextflow, where workflows and their processes are defined. Copy the following code into your main.nf file: // Enable DSL2 functionality nextflow.enable.dsl = 2 // Workflow definition workflow { // Define input files ch_input = Channel.fromPath(\"a.txt\") // Run workflow CONVERT_TO_UPPER_CASE(ch_input) } // Process definition process CONVERT_TO_UPPER_CASE { publishDir \"results/\", mode: \"copy\" input: path(file) output: path(\"a.upper.txt\") script: \"\"\" tr [a-z] [A-Z] < ${file} > a.upper.txt \"\"\" } Here we have three separate parts. The first part enables the DSL2 ( Domain Specific Language 2 ) functionality, and is required to use some of the newer and more powerful features of Nextflow. The next part is the workflow definition , while the last is a process . Let's go through the last two in more detail! Nextflow comments Double-slashes ( // ) are used for comments in Nextflow. Nextflow and whitespace Nextflow is not indentation-sensitive. In fact, Nextflow doesn't care at all about whitespace, so go ahead and use it in whatever manner you think is easiest to read and work with! Do keep in mind that indentations and other types of whitespace does improve readability, so it's generally not a good idea to forego it entirely, even though you can. Workflow definitions # The workflow definition here has two parts, each doing an important job for any Nextflow workflow. The first part defines a channel , which is an asynchronous first-in-first-out stream of data that connect a workflow's various inputs and outputs. In this particular case, we define a Channel using the .fromPath channel factory on the specific file path a.txt , and name the channel ch_input . You can read this as \"create the channel ch_input and send the file a.txt into it\" . Naming channels A channel can be named anything you like, but it is good practice to prepend them with ch_ , as that makes it clear which variables are channels and which are just normal variables. How do we use these channels then? Channels pass data to and from processes through our workflow. By providing channels as arguments to processes, we describe how we want data to flow. This is exactly what we do in the second part: we call our CONVERT_TO_UPPER_CASE process with the ch_input as input argument - this is very similar to functional programming. This is our entire workflow, for now: the creation of a channel followed by using the contents of that channel as input to a single process. Let's look at how processes themselves are defined! Process definitions # Looking at the process in the code above, we can see several parts. The process block starts with its name, in this case CONVERT_TO_UPPER_CASE , followed by several sections: publishDir , input , output and script . Naming processes A process can be named using any case, but a commonly used convention is to use UPPERCASE letters for processes to visually distinguish them in the workflow. You do not have to follow this if you don't want to, but we do so here. Let's ignore the first section for now and focus on the last three. The input and output sections describe the data expected to come through the channel for this specific process. Each line of input describes the data expected for each process argument, in the order used in the workflow. In this case, CONVERT_TO_UPPER_CASE expects a single channel (one line of input), and expects the data to be filenames (of type path ). Notice that there is a difference between how the inputs and outputs are declared? The output is an explicit string ( i.e surrounded by quotes), while the input is a variable named file . This means inputs can be referenced in the process without naming the data explicitly, unlike the output where the name needs to be explicit. We'll get back to exactly how this works in just a moment. Let's move on to the first section: publishDir . This tells Nextflow where the output of the process should be stored when it is finished; setting mode to \"copy\" just means that we want to copy the output files to the publishing directory, rather than using a symbolic link (which is the default). Executing workflows # Let's try running the workflow we just created! Type the following in your terminal: nextflow run main.nf This will make Nextflow run the workflow specified in your main.nf file. You should see something along these lines: N E X T F L O W ~ version 21.04.0 Launching `./main.nf` [mad_legentil] - revision: 87f0c253ed executor > local (1) [32/9124a1] process > CONVERT_TO_UPPER_CASE (1) [100%] 1 of 1 \u2714 The first few lines are information about this particular run, including the Nextflow version used, which workflow definition file was used, a randomly generated run name (an adjective and a scientist), the revision ID as well as where the processes were executed (locally, in this case). What follows next is a list of all the various processes for this particular workflow. The order does not necessarily reflect the order of execution (depending on each process\u2019 input and output dependencies), but they are in the order they were defined in the workflow file - there's only the one process here, of course. The first part ( e.g [32/9124a1] ) is the process ID, which is also the first part of the subdirectory in which the process is run (before the outputs are transferred to the publish directory). We then get the process and its name. Lastly, we get how many instances of each process are currently running or have finished. Here we only have the one process, of course, but this will soon change. Let's check that everything worked: type ls results/ and see that it contains the output we expected. Let's explore the working directory: change into whatever directory is specified by the process ID (your equivalent to work/32/9124a1[...] ). What do you see when you list the contents of this directory? You should, hopefully, see a symbolic link named a.txt pointing to the real location of this file, plus a normal file a.upper.txt , which is the output of the process that was run in this directory. While it seems cumbersome to manually move into these work directories it is something you only do when debugging errors in your workflow, and Nextflow has some tricks to make this process a lot easier - more on this later. So, how does this all work? Well, we have three components: a set of inputs, a set of processes and a workflow that defines which processes should be run. We tell Nextflow to push the inputs through the entire workflow, so to speak. Now it's your turn! Move back to the workflow root and make it use only the b.txt input file and give you the b.upper.txt instead. Run your workflow and make sure it works before you move on. Files and sample names # Having to manually change inputs and outputs like you just did is not really ideal, is it? Hard-coding outputs is rarely good, so let's try to change that. One powerful feature of Nextflow is that it can handle complex data structures as input, and not only filenames. One strategy we can follow is to create a prefix for our output and pass it together with the filename. Change the channel definition to the following: ch_input = Channel .fromPath(\"a.txt\") .map{ file -> tuple(file.getBaseName(), file) } Okay, so what does that do, exactly? Well, the added line containing the .map{} statement changes the data stream to be [prefix, file] instead of just [file] - we generate the prefix from the base name of the file itself, i.e. the file without extension or directory. We now have to change the process itself to make use of this new information contained in the ch_input channel. Change the process definition to the following: process CONVERT_TO_UPPER_CASE { publishDir \"results/\", mode: \"copy\" input: tuple val(prefix), path(file) output: path(\"${prefix}.upper.txt\") script: \"\"\" tr [a-z] [A-Z] < ${file} > ${prefix}.upper.txt \"\"\" } Notice how the input now is aware that we're passing a tuple as input, which allows us to use both the file variable (as before) and the new prefix variable. All that's left now is to change the input to our pipeline! Change the channel definition line from .fromPath(\"a.txt\") to .fromPath([\"a.txt\", \"b.txt\"]) and try running the pipeline. Make sure it works before you move on! Adding more processes # It's time to add more processes to our workflow! We have the two files a.upper.txt and b.upper.txt ; the next part of the workflow is a step that concatenates the content of all these UPPERCASE files. We already have a channel containing the two files we need: the output of the CONVERT_TO_UPPER_CASE process called CONVERT_TO_UPPER_CASE.out . We can use this output as input to a new process using the syntax: CONVERT_TO_UPPER_CASE.out.collect() . The collect() operator, groups all the outputs in the channel into a single data object for the next process. This is a many-to-one type of operation: a stream with several files ( many ) is merged into a lone list of files ( one ). If collect() was not used, the next process would try to run a task for each file in the output channel. Let's put this in use by adding a new process to the workflow definition. We'll call this process CONCATENATE_FILES and it will take the output from CONVERT_TO_UPPER_CASE as input, grouped using the collect() operator. Add a line to your workflow definition for this new process with the appropriate input - click below if you're having trouble. Click to show CONCATENATE_FILES( CONVERT_TO_UPPER_CASE.out.collect() ) Now all we have to do is define the actual CONCATENATE_FILES process in the process definition section. Copy the following code as a new process into your workflow: process CONCATENATE_FILES { publishDir \"results/\", mode: \"copy\" input: path(files) output: path(\"*.txt\") script: \"\"\" cat ${files} > concat.txt \"\"\" } Run your workflow again and check the results/ directory. At this point you should have three files there: a.upper.txt , b.upper.txt and concat.txt . Inspect the contents of concat.txt - do you see everything as you expected? Note the use of path(files) as input. Although we pass a list of files as input, the list is considered a single object, and so the files variable references a list. Each file in that list can be individually accessed using an index e.g. ${files[0]} , or as we do here, use the variable without an index to list all the input files. Viewing channel contents # As our channels become more complicated it is useful to actually check out what's inside them: you can do this using the .view() operator. Add the following to your workflow definition (on a new line) and execute the workflow: ch_input.view() . What do you see? It can be quite useful to inspect channel contents like this when you are developing workflows, especially if you are working with tuples, maps and any transforming operators in general. Check the channel contents of the (1) raw and (2) collected output of the CONVERT_TO_UPPER_CASE process. How are they different? Quick recap In this section we've learnt: How to create and extend simple Nextflow workflows How to create channels for input data How to execute workflows How to explore Nextflow's work directory How to generalize workflows How to view channel contents","title":"The basics"},{"location":"nextflow/nextflow-2-the-basics/#workflow-definitions","text":"The workflow definition here has two parts, each doing an important job for any Nextflow workflow. The first part defines a channel , which is an asynchronous first-in-first-out stream of data that connect a workflow's various inputs and outputs. In this particular case, we define a Channel using the .fromPath channel factory on the specific file path a.txt , and name the channel ch_input . You can read this as \"create the channel ch_input and send the file a.txt into it\" . Naming channels A channel can be named anything you like, but it is good practice to prepend them with ch_ , as that makes it clear which variables are channels and which are just normal variables. How do we use these channels then? Channels pass data to and from processes through our workflow. By providing channels as arguments to processes, we describe how we want data to flow. This is exactly what we do in the second part: we call our CONVERT_TO_UPPER_CASE process with the ch_input as input argument - this is very similar to functional programming. This is our entire workflow, for now: the creation of a channel followed by using the contents of that channel as input to a single process. Let's look at how processes themselves are defined!","title":"Workflow definitions"},{"location":"nextflow/nextflow-2-the-basics/#process-definitions","text":"Looking at the process in the code above, we can see several parts. The process block starts with its name, in this case CONVERT_TO_UPPER_CASE , followed by several sections: publishDir , input , output and script . Naming processes A process can be named using any case, but a commonly used convention is to use UPPERCASE letters for processes to visually distinguish them in the workflow. You do not have to follow this if you don't want to, but we do so here. Let's ignore the first section for now and focus on the last three. The input and output sections describe the data expected to come through the channel for this specific process. Each line of input describes the data expected for each process argument, in the order used in the workflow. In this case, CONVERT_TO_UPPER_CASE expects a single channel (one line of input), and expects the data to be filenames (of type path ). Notice that there is a difference between how the inputs and outputs are declared? The output is an explicit string ( i.e surrounded by quotes), while the input is a variable named file . This means inputs can be referenced in the process without naming the data explicitly, unlike the output where the name needs to be explicit. We'll get back to exactly how this works in just a moment. Let's move on to the first section: publishDir . This tells Nextflow where the output of the process should be stored when it is finished; setting mode to \"copy\" just means that we want to copy the output files to the publishing directory, rather than using a symbolic link (which is the default).","title":"Process definitions"},{"location":"nextflow/nextflow-2-the-basics/#executing-workflows","text":"Let's try running the workflow we just created! Type the following in your terminal: nextflow run main.nf This will make Nextflow run the workflow specified in your main.nf file. You should see something along these lines: N E X T F L O W ~ version 21.04.0 Launching `./main.nf` [mad_legentil] - revision: 87f0c253ed executor > local (1) [32/9124a1] process > CONVERT_TO_UPPER_CASE (1) [100%] 1 of 1 \u2714 The first few lines are information about this particular run, including the Nextflow version used, which workflow definition file was used, a randomly generated run name (an adjective and a scientist), the revision ID as well as where the processes were executed (locally, in this case). What follows next is a list of all the various processes for this particular workflow. The order does not necessarily reflect the order of execution (depending on each process\u2019 input and output dependencies), but they are in the order they were defined in the workflow file - there's only the one process here, of course. The first part ( e.g [32/9124a1] ) is the process ID, which is also the first part of the subdirectory in which the process is run (before the outputs are transferred to the publish directory). We then get the process and its name. Lastly, we get how many instances of each process are currently running or have finished. Here we only have the one process, of course, but this will soon change. Let's check that everything worked: type ls results/ and see that it contains the output we expected. Let's explore the working directory: change into whatever directory is specified by the process ID (your equivalent to work/32/9124a1[...] ). What do you see when you list the contents of this directory? You should, hopefully, see a symbolic link named a.txt pointing to the real location of this file, plus a normal file a.upper.txt , which is the output of the process that was run in this directory. While it seems cumbersome to manually move into these work directories it is something you only do when debugging errors in your workflow, and Nextflow has some tricks to make this process a lot easier - more on this later. So, how does this all work? Well, we have three components: a set of inputs, a set of processes and a workflow that defines which processes should be run. We tell Nextflow to push the inputs through the entire workflow, so to speak. Now it's your turn! Move back to the workflow root and make it use only the b.txt input file and give you the b.upper.txt instead. Run your workflow and make sure it works before you move on.","title":"Executing workflows"},{"location":"nextflow/nextflow-2-the-basics/#files-and-sample-names","text":"Having to manually change inputs and outputs like you just did is not really ideal, is it? Hard-coding outputs is rarely good, so let's try to change that. One powerful feature of Nextflow is that it can handle complex data structures as input, and not only filenames. One strategy we can follow is to create a prefix for our output and pass it together with the filename. Change the channel definition to the following: ch_input = Channel .fromPath(\"a.txt\") .map{ file -> tuple(file.getBaseName(), file) } Okay, so what does that do, exactly? Well, the added line containing the .map{} statement changes the data stream to be [prefix, file] instead of just [file] - we generate the prefix from the base name of the file itself, i.e. the file without extension or directory. We now have to change the process itself to make use of this new information contained in the ch_input channel. Change the process definition to the following: process CONVERT_TO_UPPER_CASE { publishDir \"results/\", mode: \"copy\" input: tuple val(prefix), path(file) output: path(\"${prefix}.upper.txt\") script: \"\"\" tr [a-z] [A-Z] < ${file} > ${prefix}.upper.txt \"\"\" } Notice how the input now is aware that we're passing a tuple as input, which allows us to use both the file variable (as before) and the new prefix variable. All that's left now is to change the input to our pipeline! Change the channel definition line from .fromPath(\"a.txt\") to .fromPath([\"a.txt\", \"b.txt\"]) and try running the pipeline. Make sure it works before you move on!","title":"Files and sample names"},{"location":"nextflow/nextflow-2-the-basics/#adding-more-processes","text":"It's time to add more processes to our workflow! We have the two files a.upper.txt and b.upper.txt ; the next part of the workflow is a step that concatenates the content of all these UPPERCASE files. We already have a channel containing the two files we need: the output of the CONVERT_TO_UPPER_CASE process called CONVERT_TO_UPPER_CASE.out . We can use this output as input to a new process using the syntax: CONVERT_TO_UPPER_CASE.out.collect() . The collect() operator, groups all the outputs in the channel into a single data object for the next process. This is a many-to-one type of operation: a stream with several files ( many ) is merged into a lone list of files ( one ). If collect() was not used, the next process would try to run a task for each file in the output channel. Let's put this in use by adding a new process to the workflow definition. We'll call this process CONCATENATE_FILES and it will take the output from CONVERT_TO_UPPER_CASE as input, grouped using the collect() operator. Add a line to your workflow definition for this new process with the appropriate input - click below if you're having trouble. Click to show CONCATENATE_FILES( CONVERT_TO_UPPER_CASE.out.collect() ) Now all we have to do is define the actual CONCATENATE_FILES process in the process definition section. Copy the following code as a new process into your workflow: process CONCATENATE_FILES { publishDir \"results/\", mode: \"copy\" input: path(files) output: path(\"*.txt\") script: \"\"\" cat ${files} > concat.txt \"\"\" } Run your workflow again and check the results/ directory. At this point you should have three files there: a.upper.txt , b.upper.txt and concat.txt . Inspect the contents of concat.txt - do you see everything as you expected? Note the use of path(files) as input. Although we pass a list of files as input, the list is considered a single object, and so the files variable references a list. Each file in that list can be individually accessed using an index e.g. ${files[0]} , or as we do here, use the variable without an index to list all the input files.","title":"Adding more processes"},{"location":"nextflow/nextflow-2-the-basics/#viewing-channel-contents","text":"As our channels become more complicated it is useful to actually check out what's inside them: you can do this using the .view() operator. Add the following to your workflow definition (on a new line) and execute the workflow: ch_input.view() . What do you see? It can be quite useful to inspect channel contents like this when you are developing workflows, especially if you are working with tuples, maps and any transforming operators in general. Check the channel contents of the (1) raw and (2) collected output of the CONVERT_TO_UPPER_CASE process. How are they different? Quick recap In this section we've learnt: How to create and extend simple Nextflow workflows How to create channels for input data How to execute workflows How to explore Nextflow's work directory How to generalize workflows How to view channel contents","title":"Viewing channel contents"},{"location":"nextflow/nextflow-3-executing-workflows/","text":"It's time to start working with a more realistic workflow using the MRSA case study of this course! We've created a bare-bones version of this pipeline for you, but we'll work our way through it as we go along and learn more about Nextflow's features and functionality. The MRSA workflow looks like this: workflow { // Workflow for generating count data for the MRSA case study // Define SRA input data channel ch_sra_ids = Channel.fromList( [\"SRR935090\", \"SRR935091\", \"SRR935092\"] ) // Define the workflow GET_SRA_BY_ACCESSION ( ch_sra_ids ) RUN_FASTQC ( GET_SRA_BY_ACCESSION.out ) RUN_MULTIQC ( RUN_FASTQC.out[1].collect() ) GET_GENOME_FASTA () INDEX_GENOME ( GET_GENOME_FASTA.out.fasta ) ALIGN_TO_GENOME ( GET_SRA_BY_ACCESSION.out, INDEX_GENOME.out.index ) SORT_BAM ( ALIGN_TO_GENOME.out.bam ) GET_GENOME_GFF3 () GENERATE_COUNTS_TABLE ( SORT_BAM.out.bam.collect(), GET_GENOME_GFF3.out.gff ) } The workflow has one input channel named ch_sra_ids , which is a list of SRA IDs ( i.e. a list of strings). We then define the processes to be executed by this workflow, nine in total. The first process ( GET_SRA_BY_ACCESSION ) takes the ch_sra_ids channel as input, while the rest of the processes takes the output of previous processes as input. Before we go into more detail regarding the ins-and-outs of this workflow, let's start with some specifics of how workflows are executed and what you can get from them. Reports and visualisations # Let's start with running the workflow plus getting some reports and visualisation while we're at it! Run the workflow using the following command: nextflow run main_mrsa.nf -with-report -with-timeline -with-dag dag.png . After successful executing, you will find three more files in your current directory: report.html , timeline.html and dag.png . The first file contains a workflow report, which includes various information regarding execution such as runtime, resource usage and details about the different processes. The second file contains a timeline for how long each individual process took to execute, while the last contains a visualisation of the workflow itself. Take a few minutes to browse these files for yourself! When running a workflow you can of course choose which of these additional files you want to include by picking which ones are important or interesting to you - or don\u2019t include any! Re-running workflows # Something you often want to do in Nextflow (or any WfMS for that matter) is to re-run the workflow when you changed some input files or some of the code for its analyses, but you don't want to re-run the entire workflow from start to finish. Let\u2019s find out how this works in Nextflow! Run the same nextflow run main_mrsa.nf command again. What happened here? Nextflow actually re-ran the entire workflow from scratch, even though we didn't change anything. This is the default behaviour of Nextflow. Let\u2019s try that again: nextflow run main_mrsa.nf -resume instead. Now you can see that Nextflow didn't actually re-run anything. The -resume flag instructed Nextflow to use the cached results from the previous run! Nextflow automatically keeps track of not only changes to input files, but also changes to code, process definitions and scripts. You can thus change anything relating to your workflow and just re-run with the -resume flag and be sure that only processes relevant to your changes are executed again! Use tree work to list the contents of the work directory. Because Nextflow keeps track of all the runs, we've now got two sets of files in the work directory. One set from the first run, and another from the second run. This can take up valuable space, so let's clean that up. Use nextflow clean -n -before <run_name> to show which work directories will be cleaned up. Then delete those directories by changing -n to -f . Nextflow's clean subcommand can be used to clean up failed tasks and unused processes. Use nextflow help clean to see other options for cleaning. This is the preferred way to clean up the working directory. Remove the results directory and re-run the workflow again using the -resume flag. We removed all the results we used before, but we still managed to resume the workflow and use its cache - how come? Remember that Nextflow uses the work directory to run all of its tasks, while the results directory is just where we have chosen to publish our outputs. We can thus delete the results directory as often as we like (a necessity when output filenames are changed) and still get everything back without having to re-run anything. If we were to delete the work directory, however... Delete the work directory and re-run the workflow using the -resume flag. There is no longer any cache for Nextflow to use, so it re-runs from the start! This is good to keep in mind: you can always delete the output directories of your workflow, but if you mess with work you'll lose, well... work! Logs # Nextflow keeps a log of all the workflows that have been executed. Let's check it out! Type nextflow log to get a list of all the executions. Here we get information about when the workflow was executed, how long it ran, its run name, whether it succeeded or not and what command was used to run it. You can also use nextflow log <run name> to show each task's directory that was executed for that run. You can also supply the -f (or -fields ) flag along with additional fields to show. Run nextflow log <run name> -f hash,name,exit,status This shows us not only the beginning of each task's working directory, but also its name, exit code and status ( i.e. if it completed successfully or failed in some manner). Listing fields If you want to see a complete list of all the fields you might explore using the log, just type nextflow log -l or nextflow log -list-fields . This is highly useful for debugging when there's some specific information about a run you're particularly interested in! We can also get even more detailed information about the latest run by looking into the .nextflow.log file! Look into the latest log by typing less .nextflow.log . You'll be greeted by a wealth of debugging information, which may even seem a bit overkill at this point! This level of detail is, however, quite useful both as a history of what you've attempted and as an additional help when you run into errors! Also, it helps with advanced debugging - which we'll get into later. Quick recap In this section we've learnt: How to get automatic reports and visualisations How to re-run workflows How to clean the Nextflow cache How to check the Nextflow logs","title":"Running Nextflow"},{"location":"nextflow/nextflow-3-executing-workflows/#reports-and-visualisations","text":"Let's start with running the workflow plus getting some reports and visualisation while we're at it! Run the workflow using the following command: nextflow run main_mrsa.nf -with-report -with-timeline -with-dag dag.png . After successful executing, you will find three more files in your current directory: report.html , timeline.html and dag.png . The first file contains a workflow report, which includes various information regarding execution such as runtime, resource usage and details about the different processes. The second file contains a timeline for how long each individual process took to execute, while the last contains a visualisation of the workflow itself. Take a few minutes to browse these files for yourself! When running a workflow you can of course choose which of these additional files you want to include by picking which ones are important or interesting to you - or don\u2019t include any!","title":"Reports and visualisations"},{"location":"nextflow/nextflow-3-executing-workflows/#re-running-workflows","text":"Something you often want to do in Nextflow (or any WfMS for that matter) is to re-run the workflow when you changed some input files or some of the code for its analyses, but you don't want to re-run the entire workflow from start to finish. Let\u2019s find out how this works in Nextflow! Run the same nextflow run main_mrsa.nf command again. What happened here? Nextflow actually re-ran the entire workflow from scratch, even though we didn't change anything. This is the default behaviour of Nextflow. Let\u2019s try that again: nextflow run main_mrsa.nf -resume instead. Now you can see that Nextflow didn't actually re-run anything. The -resume flag instructed Nextflow to use the cached results from the previous run! Nextflow automatically keeps track of not only changes to input files, but also changes to code, process definitions and scripts. You can thus change anything relating to your workflow and just re-run with the -resume flag and be sure that only processes relevant to your changes are executed again! Use tree work to list the contents of the work directory. Because Nextflow keeps track of all the runs, we've now got two sets of files in the work directory. One set from the first run, and another from the second run. This can take up valuable space, so let's clean that up. Use nextflow clean -n -before <run_name> to show which work directories will be cleaned up. Then delete those directories by changing -n to -f . Nextflow's clean subcommand can be used to clean up failed tasks and unused processes. Use nextflow help clean to see other options for cleaning. This is the preferred way to clean up the working directory. Remove the results directory and re-run the workflow again using the -resume flag. We removed all the results we used before, but we still managed to resume the workflow and use its cache - how come? Remember that Nextflow uses the work directory to run all of its tasks, while the results directory is just where we have chosen to publish our outputs. We can thus delete the results directory as often as we like (a necessity when output filenames are changed) and still get everything back without having to re-run anything. If we were to delete the work directory, however... Delete the work directory and re-run the workflow using the -resume flag. There is no longer any cache for Nextflow to use, so it re-runs from the start! This is good to keep in mind: you can always delete the output directories of your workflow, but if you mess with work you'll lose, well... work!","title":"Re-running workflows"},{"location":"nextflow/nextflow-3-executing-workflows/#logs","text":"Nextflow keeps a log of all the workflows that have been executed. Let's check it out! Type nextflow log to get a list of all the executions. Here we get information about when the workflow was executed, how long it ran, its run name, whether it succeeded or not and what command was used to run it. You can also use nextflow log <run name> to show each task's directory that was executed for that run. You can also supply the -f (or -fields ) flag along with additional fields to show. Run nextflow log <run name> -f hash,name,exit,status This shows us not only the beginning of each task's working directory, but also its name, exit code and status ( i.e. if it completed successfully or failed in some manner). Listing fields If you want to see a complete list of all the fields you might explore using the log, just type nextflow log -l or nextflow log -list-fields . This is highly useful for debugging when there's some specific information about a run you're particularly interested in! We can also get even more detailed information about the latest run by looking into the .nextflow.log file! Look into the latest log by typing less .nextflow.log . You'll be greeted by a wealth of debugging information, which may even seem a bit overkill at this point! This level of detail is, however, quite useful both as a history of what you've attempted and as an additional help when you run into errors! Also, it helps with advanced debugging - which we'll get into later. Quick recap In this section we've learnt: How to get automatic reports and visualisations How to re-run workflows How to clean the Nextflow cache How to check the Nextflow logs","title":"Logs"},{"location":"nextflow/nextflow-4-working-with-processes/","text":"Now that we've gone through the specifics of executing workflows in a bit more detail, let's go through working with processes. While there are numerous process directives that can be used, we'll go through some of the more commonly used ones here. Tags # Let's look at the command line output we got during the workflow's execution, which should look something like this: N E X T F L O W ~ version 21.04.0 Launching `./main.nf` [friendly_bhaskara] - revision: b4490b9201 executor > local (17) [c9/e5f818] process > GET_SRA_BY_ACCESSION (SRR935092) [100%] 3 of 3 \u2714 [d5/b5f24e] process > RUN_FASTQC (SRR935092) [100%] 3 of 3 \u2714 [91/2cea54] process > RUN_MULTIQC [100%] 1 of 1 \u2714 [e0/b4fd37] process > GET_GENOME_FASTA [100%] 1 of 1 \u2714 [87/32ce10] process > INDEX_GENOME [100%] 1 of 1 \u2714 [56/e9a460] process > ALIGN_TO_GENOME (SRR935092) [100%] 3 of 3 \u2714 [ed/d8c223] process > SORT_BAM (SRR935092) [100%] 3 of 3 \u2714 [e7/4a6bda] process > GET_GENOME_GFF3 [100%] 1 of 1 \u2714 [e9/84f093] process > GENERATE_COUNTS_TABLE [100%] 1 of 1 \u2714 Have you noticed that there are SRA IDs after some of the processes? Well, if you look at which processes show these SRA IDs you might see that it's only those processes that are executed three times, i.e. once per SRA ID. This doesn't happen automatically, however, and comes from something called tags . Let's look at the GET_SRA_BY_ACCESSION process: process GET_SRA_BY_ACCESSION { // Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run // accession number. tag \"${sra_id}\" publishDir \"results/data/raw_internal\", mode: \"copy\" input: val(sra_id) output: tuple val(sra_id), path(\"*.fastq.gz\") script: \"\"\" fastq-dump ${sra_id} \\ -X 25000 \\ --readids \\ --dumpbase \\ --skip-technical \\ --gzip \\ -Z \\ > ${sra_id}.fastq.gz \"\"\" } You can see the tag directive at the very top of the process definition. Tags can be used to e.g. show information about the sample currently being analysed by the process. This is useful both during run-time (allowing you to see which sample is being processed) but also for debugging or finding problematic samples in case of errors or odd output. There is, naturally, no need to use tags for processes which are only run once. Comment out (prefix with // ) the tag directive from the GET_SRA_BY_ACCESSION process and run the workflow again. No more SRA IDs! Uncomment the tag directive before you move on. Named outputs # Let's move on to the next process! It looks like this: process RUN_FASTQC { // Run FastQC on a FASTQ file. tag \"${sample}\" publishDir \"results/\", mode: \"copy\" input: tuple val(sample), path(fastq) output: path(\"*.html\") path(\"*.zip\") script: \"\"\" # Run FastQC fastqc ${fastq} -q \"\"\" } Here is a process with two output channels! One contains all the .html files, while the other contains all the .zip files. How is this handled in the workflow definition of downstream processes that use the outputs? The RUN_MULTIQC process uses this output, and its part in the workflow definition looks like this: RUN_MULTIQC ( RUN_FASTQC.out[1].collect() ) We already know about .out and .collect() , but we have something new here: the RUN_MULTIQC process is taking the second channel of the output from the RUN_FASTQC process - [1] is the index for the second channel, as Groovy is zero-based (the first channel is indexed by [0] ). This comes with some issues, however. What if we accidentally changed the order of the outputs in the rule, or added a new one? Using positions like this is easy to mess up, but there is a better solution: named outputs! This can be achieved by adding the emit option for some or all of the outputs, like so: output: path(*.txt), emit: text Instead of referring to the output by its position in an array as previously, we refer to the channel with a label we choose ( .out.text ) instead. This benefits us in that we can infer more information about channel contents called text rather than [1] , and it is also allows us to be less error-prone when rewriting parts of a workflow. Your turn! Add named outputs to the RUN_FASTQC process and make RUN_MULTIQC use those outputs. You'll have to change both the output section of the RUN_FASTQC process, and the workflow definition section for RUN_MULTIQC . If you need help, see the hint below. Click to show // Workflow definition for RUN_MULTIQC RUN_MULTIQC ( RUN_FASTQC.out.zip.collect() // Output section of RUN_FASTC output: path(\"*.html\"), emit: html path(\"*.zip\"), emit: zip Check if it works by executing the workflow. Advanced publishing # So far we've only used the publishDir directive in a very simple way: specifying a directory and the mode to use when publishing (to copy the files rather than symbolically link them). There are more things you can do, however, especially for processes with more than one output. For example, we can publish outputs in separate directories, like so: publishDir \"results/tables/\", pattern: \"*.tsv\", mode: \"copy\" publishDir \"results/logs\", pattern: \"*.log\", mode: \"copy\" In this example, *.tsv files are copied to the folder results/tables/ , while *.log files are copied to the folder results/logs . The publishDir directive can be used multiple times in a single process, allowing one to separate output as above, or publish the same output to multiple folders. Edit the RUN_FASTQC process to place the HTML and compressed files in separate directories. Remove the results directory and re-run the workflow to check that it worked. Note that an output and a published output are different things: something can be an output of a process without being published. In fact, the RUN_FASTQC process is a prime example of this! Think about the compressed output: this output is only used by the downstream process RUN_MULTIQC and is never meant to be viewed by a human or used by a human in some downstream task not part of the pipeline itself. We would thus like to keep the compressed files as an output, but not publish said output. How do we do this? Just remove the corresponding publishDir directive! The MRSA workflow we've made here was refactored directly from its original version in the Snakemake tutorial of this course, which means that its output structure is not fully taking advantage of some of Nextflow's functionality. The compressed output we've already talked about above is, for example, put in the results/intermediate/ directory. This is required for Snakemake, but not so for Nextflow. See if you can find any other processes in the current implementation of the MRSA workflow that you could optimise like this! Think about whether all processes actually need to have published outputs. Make sure you test executing the workflow after you've made any changes. Debugging # It is, sadly, inevitable that we all make mistakes while coding - nobody's perfect! Nextflow helps you quite a bit when this happens, not just with its logs but also with informative error messages. Let's introduce an error and look at what we get: Change the name of the multiqc_general_stats.txt output in the RUN_MULTIQC process to something different and re-run the workflow. We got an error! We get a number of things, actually, including (in order from the top) the name of the process that gave the error, the likely cause, the command that was executed, along with its exit status, output, error and the work directory that the task was run in. Let's focus on the Caused by: part, which should look something like this: Caused by: Missing output file(s) `multiqc_general_stats.text` expected by process `RUN_MULTIQC` We can also see that the command's exit status is 0 , which means that the command was successful; any exit status other than 0 means there was an error of some kind. We can thus infer that the command (1) worked, (2) failed to give us the output expected by Nextflow. Thankfully, Nextflow graciously prints the work directory for us so that we may check out what happened in more detail. Copy the working directory path, cd into it and list its contents using ls . You might already have spotted the error in the message above! The error we introduced here was that the expected output file has a .text extension, rather than the correct .txt . Nextflow is expecting the .text output, but the process script directive is (correctly) giving us the .txt file. Go back to the root directory, revert the error you introduced and re-run the workflow to make sure it works again. This might have seemed like a trivial error, but a lot of errors in Nextflow can be solved in the same manner, i.e. by just following the debugging output reported by Nextflow and inspecting the specific subdirectory in question. A note about Bash If you are using Bash variables inside the script directive you have to be careful to prepend it with a backslash, e.g. \\${BASH_VARIABLE} . This is because the dollar-sign is used by Nextflow, so you have to tell Nextflow explicitly when you're using a Bash variable. This is a common source of errors when using Bash variables, so keeping it in mind can save you some debugging time! Quick recap In this section we've learnt: How to use the tag directive How to use named output How to separate process outputs into different directories How to debug errors and mistakes","title":"Processes"},{"location":"nextflow/nextflow-4-working-with-processes/#tags","text":"Let's look at the command line output we got during the workflow's execution, which should look something like this: N E X T F L O W ~ version 21.04.0 Launching `./main.nf` [friendly_bhaskara] - revision: b4490b9201 executor > local (17) [c9/e5f818] process > GET_SRA_BY_ACCESSION (SRR935092) [100%] 3 of 3 \u2714 [d5/b5f24e] process > RUN_FASTQC (SRR935092) [100%] 3 of 3 \u2714 [91/2cea54] process > RUN_MULTIQC [100%] 1 of 1 \u2714 [e0/b4fd37] process > GET_GENOME_FASTA [100%] 1 of 1 \u2714 [87/32ce10] process > INDEX_GENOME [100%] 1 of 1 \u2714 [56/e9a460] process > ALIGN_TO_GENOME (SRR935092) [100%] 3 of 3 \u2714 [ed/d8c223] process > SORT_BAM (SRR935092) [100%] 3 of 3 \u2714 [e7/4a6bda] process > GET_GENOME_GFF3 [100%] 1 of 1 \u2714 [e9/84f093] process > GENERATE_COUNTS_TABLE [100%] 1 of 1 \u2714 Have you noticed that there are SRA IDs after some of the processes? Well, if you look at which processes show these SRA IDs you might see that it's only those processes that are executed three times, i.e. once per SRA ID. This doesn't happen automatically, however, and comes from something called tags . Let's look at the GET_SRA_BY_ACCESSION process: process GET_SRA_BY_ACCESSION { // Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run // accession number. tag \"${sra_id}\" publishDir \"results/data/raw_internal\", mode: \"copy\" input: val(sra_id) output: tuple val(sra_id), path(\"*.fastq.gz\") script: \"\"\" fastq-dump ${sra_id} \\ -X 25000 \\ --readids \\ --dumpbase \\ --skip-technical \\ --gzip \\ -Z \\ > ${sra_id}.fastq.gz \"\"\" } You can see the tag directive at the very top of the process definition. Tags can be used to e.g. show information about the sample currently being analysed by the process. This is useful both during run-time (allowing you to see which sample is being processed) but also for debugging or finding problematic samples in case of errors or odd output. There is, naturally, no need to use tags for processes which are only run once. Comment out (prefix with // ) the tag directive from the GET_SRA_BY_ACCESSION process and run the workflow again. No more SRA IDs! Uncomment the tag directive before you move on.","title":"Tags"},{"location":"nextflow/nextflow-4-working-with-processes/#named-outputs","text":"Let's move on to the next process! It looks like this: process RUN_FASTQC { // Run FastQC on a FASTQ file. tag \"${sample}\" publishDir \"results/\", mode: \"copy\" input: tuple val(sample), path(fastq) output: path(\"*.html\") path(\"*.zip\") script: \"\"\" # Run FastQC fastqc ${fastq} -q \"\"\" } Here is a process with two output channels! One contains all the .html files, while the other contains all the .zip files. How is this handled in the workflow definition of downstream processes that use the outputs? The RUN_MULTIQC process uses this output, and its part in the workflow definition looks like this: RUN_MULTIQC ( RUN_FASTQC.out[1].collect() ) We already know about .out and .collect() , but we have something new here: the RUN_MULTIQC process is taking the second channel of the output from the RUN_FASTQC process - [1] is the index for the second channel, as Groovy is zero-based (the first channel is indexed by [0] ). This comes with some issues, however. What if we accidentally changed the order of the outputs in the rule, or added a new one? Using positions like this is easy to mess up, but there is a better solution: named outputs! This can be achieved by adding the emit option for some or all of the outputs, like so: output: path(*.txt), emit: text Instead of referring to the output by its position in an array as previously, we refer to the channel with a label we choose ( .out.text ) instead. This benefits us in that we can infer more information about channel contents called text rather than [1] , and it is also allows us to be less error-prone when rewriting parts of a workflow. Your turn! Add named outputs to the RUN_FASTQC process and make RUN_MULTIQC use those outputs. You'll have to change both the output section of the RUN_FASTQC process, and the workflow definition section for RUN_MULTIQC . If you need help, see the hint below. Click to show // Workflow definition for RUN_MULTIQC RUN_MULTIQC ( RUN_FASTQC.out.zip.collect() // Output section of RUN_FASTC output: path(\"*.html\"), emit: html path(\"*.zip\"), emit: zip Check if it works by executing the workflow.","title":"Named outputs"},{"location":"nextflow/nextflow-4-working-with-processes/#advanced-publishing","text":"So far we've only used the publishDir directive in a very simple way: specifying a directory and the mode to use when publishing (to copy the files rather than symbolically link them). There are more things you can do, however, especially for processes with more than one output. For example, we can publish outputs in separate directories, like so: publishDir \"results/tables/\", pattern: \"*.tsv\", mode: \"copy\" publishDir \"results/logs\", pattern: \"*.log\", mode: \"copy\" In this example, *.tsv files are copied to the folder results/tables/ , while *.log files are copied to the folder results/logs . The publishDir directive can be used multiple times in a single process, allowing one to separate output as above, or publish the same output to multiple folders. Edit the RUN_FASTQC process to place the HTML and compressed files in separate directories. Remove the results directory and re-run the workflow to check that it worked. Note that an output and a published output are different things: something can be an output of a process without being published. In fact, the RUN_FASTQC process is a prime example of this! Think about the compressed output: this output is only used by the downstream process RUN_MULTIQC and is never meant to be viewed by a human or used by a human in some downstream task not part of the pipeline itself. We would thus like to keep the compressed files as an output, but not publish said output. How do we do this? Just remove the corresponding publishDir directive! The MRSA workflow we've made here was refactored directly from its original version in the Snakemake tutorial of this course, which means that its output structure is not fully taking advantage of some of Nextflow's functionality. The compressed output we've already talked about above is, for example, put in the results/intermediate/ directory. This is required for Snakemake, but not so for Nextflow. See if you can find any other processes in the current implementation of the MRSA workflow that you could optimise like this! Think about whether all processes actually need to have published outputs. Make sure you test executing the workflow after you've made any changes.","title":"Advanced publishing"},{"location":"nextflow/nextflow-4-working-with-processes/#debugging","text":"It is, sadly, inevitable that we all make mistakes while coding - nobody's perfect! Nextflow helps you quite a bit when this happens, not just with its logs but also with informative error messages. Let's introduce an error and look at what we get: Change the name of the multiqc_general_stats.txt output in the RUN_MULTIQC process to something different and re-run the workflow. We got an error! We get a number of things, actually, including (in order from the top) the name of the process that gave the error, the likely cause, the command that was executed, along with its exit status, output, error and the work directory that the task was run in. Let's focus on the Caused by: part, which should look something like this: Caused by: Missing output file(s) `multiqc_general_stats.text` expected by process `RUN_MULTIQC` We can also see that the command's exit status is 0 , which means that the command was successful; any exit status other than 0 means there was an error of some kind. We can thus infer that the command (1) worked, (2) failed to give us the output expected by Nextflow. Thankfully, Nextflow graciously prints the work directory for us so that we may check out what happened in more detail. Copy the working directory path, cd into it and list its contents using ls . You might already have spotted the error in the message above! The error we introduced here was that the expected output file has a .text extension, rather than the correct .txt . Nextflow is expecting the .text output, but the process script directive is (correctly) giving us the .txt file. Go back to the root directory, revert the error you introduced and re-run the workflow to make sure it works again. This might have seemed like a trivial error, but a lot of errors in Nextflow can be solved in the same manner, i.e. by just following the debugging output reported by Nextflow and inspecting the specific subdirectory in question. A note about Bash If you are using Bash variables inside the script directive you have to be careful to prepend it with a backslash, e.g. \\${BASH_VARIABLE} . This is because the dollar-sign is used by Nextflow, so you have to tell Nextflow explicitly when you're using a Bash variable. This is a common source of errors when using Bash variables, so keeping it in mind can save you some debugging time! Quick recap In this section we've learnt: How to use the tag directive How to use named output How to separate process outputs into different directories How to debug errors and mistakes","title":"Debugging"},{"location":"nextflow/nextflow-5-workflow-configuration/","text":"We've so far been working with a relatively non-generalised workflow: it's got hard-coded inputs, paths and genome references. This is perfectly fine for a project that is purely aimed at getting reproducible results (which is the full extent of what you want in a lot of cases), but it can be made a lot more generalisable. Let's go through the MRSA workflow and see what can be improved! Configuration basics # One of the things that allow generalisability of Nextflow workflows is parameters , which hold information and values that can be changed directly on the command-line at the time of execution. One use of parameters in our MRSA workflow is to remove the hard-coded results output directory, for example. Parameters can be written in the following form: params { parameter_1 = \"some/data/path\" // A string parameter parameter_2 = 42 // A value parameter parameter_3 = [\"a\", \"b\", \"c\", \"d\"] // A list parameter } You would then refer to these parameters using e.g. params.parameter_1 anywhere you need to in the workflow. Although parameters can be defined in main_mrsa.nf , it is preferable to define them in a separate configuration file . The default name of this file is nextflow.config and if such a file is present it will be used automatically by Nextflow (to supply a config file with another name use nextflow -c <path-to-config-file> run main_mrsa.nf ) Create a configuration file and add a parameter for the results output directory. Use your newly created parameter in the publishDir directory of a process (it'll be in the form of ${params.resultsdir}/some/other/path , for example). Run your workflow to see if it worked. Tip Instead of manually changing all the hard-coded directories in your workflow you can use the following little sed command, which will do it for you: sed 's/\\\"results\\//\\\"${params.resultsdir}\\//g' main_mrsa.nf > tmp; mv tmp main_mrsa.nf . In case you used a parameter name other than resultsdir update the command accordingly. Command line parameters # Workflow parameters can be assigned on the command-line by executing workflows like so: nextflow run main_mrsa.nf --parameter_name 'some_value' . The workflow parameter parameter_name , is prefixed by a double dash -- to tell Nextflow this is a parameter to the workflow (a single dash is a parameter to Nextflow, e.g. -resume ). The value is also quoted (this is important for parameters that take file paths as values). Run your workflow using the parameter you previously created, but pick something other than the default value! You should now have a new directory containing all the results! This is highly useful if you want to keep track of separate runs of a workflow with different software parameters, for example: nextflow run main.nf --important_param 'value1' --resultsdir 'value1' , or simply want to keep the results of separate versions of the same workflow. You can also change parameters by using the -params-file option or by using another configuration file (and using -c ), rather than on the command line! Configuring inputs # Remember the input for the MRSA workflow, the ch_sra_ids channel? This input is also hard-coded inside the main_mrsa.nf file. This could also be made into a parameter! Add another parameter for the input SRA IDs and execute your workflow to check that it worked. Using lists for parameters has its problems though, as you won't be able to change it on the command line, since the command line doesn't know about Groovy lists. There are several other ways of specifying inputs in a command line-friendly way, one of which is to use sample sheets . Instead of specifying samples directly in the command line, you specify a file that lists the input samples instead; this is the standard that is used in e.g. nf-core pipelines . Such a sample sheet for the MRSA workflow might be stored in e.g. input.csv and look like this: sra_id SRR935090 SRR935091 SRR935092 Reading input from a CSV file can be done by combining the .splitCsv channel factory (splitting the rows to read each entry) and the .map operator (defining which columns should be used). Let's see if we can make it work! Create the input.csv file with the above shown content. Change the definition of the ch_sra_ids channel to take the value of a new parameter of your choice, defined in the configuration file. Add the .splitCsv(header: true) operator to the end of the channel definition, so that the input is read from the file contents. Add the .map{row -> row.sra_id} operator, which specifies that each row should contain the sra_id column, but no other columns. You should now have a more generalised input to your workflow! Try to run it to make sure it works - look below if you need some help. Click to show // Channel definition ch_sra_ids = Channel .fromPath ( params.sra_ids ) .splitCsv ( header: true ) .map { row -> row.sra_id } // Configuration file sra_ids = \"input.csv\" By specifying inputs from sample sheets like this we can change inputs of a workflow execution by creating another sample sheet and specifying e.g. , --sra_ids input-2.csv on the command line. This is highly useful when you want to run a single sample e.g. , when testing a workflow, or when you want to keep track of all the different inputs you've used historically. Sample sheets are also useful for keeping other metadata, such as custom sample names, sample groups, location of files, etc. For example: sample-1,case,data/sample-1.fastq.gz sample-2,ctrl,data/sample-2.fastq.gz sample-3,case,data/sample-3.fastq.gz sample-4,ctrl,data/sample-4.fastq.gz Here we have not only names and file paths but also to which group each sample belongs, i.e. case or control. Such metadata can be highly useful for more advanced workflows to use in downstream analyses, such as differential gene expression! We could create a tuple based on this metadata like so: ch_input = Channel .fromPath(\"metadata.csv\") .splitCsv(header: ['id', 'group', 'fastq']) .view() Input file formatting The input file may also have headers, in which case you can use header: true to use the column headers defined in the file. You can also read e.g. tab-separated files by using the sep field: .splitCsv(sep: \"\\t\") . Other configuration scopes # There are lots of things that you might want to add to your configuration, not just parameters! The workflow manifest , for example, which might look like this: manifest { name = \"My Workflow\" description = \"My workflow, created by me\" author = \"Me\" mainScript = \"main.nf\" version = \"1.0.0\" } Go ahead and add a workflow manifest to your nextflow.config file! The manifest is useful when you're publishing or sharing the workflow through e.g. GitHub or similar. There are many more such configuration scopes that you might want to use - read more about them in the documentation . Quick recap In this section we learnt: How to create parameters in a configuration file How to specify parameters on the command line How to add workflow manifest and other configuration scopes","title":"Configuration"},{"location":"nextflow/nextflow-5-workflow-configuration/#configuration-basics","text":"One of the things that allow generalisability of Nextflow workflows is parameters , which hold information and values that can be changed directly on the command-line at the time of execution. One use of parameters in our MRSA workflow is to remove the hard-coded results output directory, for example. Parameters can be written in the following form: params { parameter_1 = \"some/data/path\" // A string parameter parameter_2 = 42 // A value parameter parameter_3 = [\"a\", \"b\", \"c\", \"d\"] // A list parameter } You would then refer to these parameters using e.g. params.parameter_1 anywhere you need to in the workflow. Although parameters can be defined in main_mrsa.nf , it is preferable to define them in a separate configuration file . The default name of this file is nextflow.config and if such a file is present it will be used automatically by Nextflow (to supply a config file with another name use nextflow -c <path-to-config-file> run main_mrsa.nf ) Create a configuration file and add a parameter for the results output directory. Use your newly created parameter in the publishDir directory of a process (it'll be in the form of ${params.resultsdir}/some/other/path , for example). Run your workflow to see if it worked. Tip Instead of manually changing all the hard-coded directories in your workflow you can use the following little sed command, which will do it for you: sed 's/\\\"results\\//\\\"${params.resultsdir}\\//g' main_mrsa.nf > tmp; mv tmp main_mrsa.nf . In case you used a parameter name other than resultsdir update the command accordingly.","title":"Configuration basics"},{"location":"nextflow/nextflow-5-workflow-configuration/#command-line-parameters","text":"Workflow parameters can be assigned on the command-line by executing workflows like so: nextflow run main_mrsa.nf --parameter_name 'some_value' . The workflow parameter parameter_name , is prefixed by a double dash -- to tell Nextflow this is a parameter to the workflow (a single dash is a parameter to Nextflow, e.g. -resume ). The value is also quoted (this is important for parameters that take file paths as values). Run your workflow using the parameter you previously created, but pick something other than the default value! You should now have a new directory containing all the results! This is highly useful if you want to keep track of separate runs of a workflow with different software parameters, for example: nextflow run main.nf --important_param 'value1' --resultsdir 'value1' , or simply want to keep the results of separate versions of the same workflow. You can also change parameters by using the -params-file option or by using another configuration file (and using -c ), rather than on the command line!","title":"Command line parameters"},{"location":"nextflow/nextflow-5-workflow-configuration/#configuring-inputs","text":"Remember the input for the MRSA workflow, the ch_sra_ids channel? This input is also hard-coded inside the main_mrsa.nf file. This could also be made into a parameter! Add another parameter for the input SRA IDs and execute your workflow to check that it worked. Using lists for parameters has its problems though, as you won't be able to change it on the command line, since the command line doesn't know about Groovy lists. There are several other ways of specifying inputs in a command line-friendly way, one of which is to use sample sheets . Instead of specifying samples directly in the command line, you specify a file that lists the input samples instead; this is the standard that is used in e.g. nf-core pipelines . Such a sample sheet for the MRSA workflow might be stored in e.g. input.csv and look like this: sra_id SRR935090 SRR935091 SRR935092 Reading input from a CSV file can be done by combining the .splitCsv channel factory (splitting the rows to read each entry) and the .map operator (defining which columns should be used). Let's see if we can make it work! Create the input.csv file with the above shown content. Change the definition of the ch_sra_ids channel to take the value of a new parameter of your choice, defined in the configuration file. Add the .splitCsv(header: true) operator to the end of the channel definition, so that the input is read from the file contents. Add the .map{row -> row.sra_id} operator, which specifies that each row should contain the sra_id column, but no other columns. You should now have a more generalised input to your workflow! Try to run it to make sure it works - look below if you need some help. Click to show // Channel definition ch_sra_ids = Channel .fromPath ( params.sra_ids ) .splitCsv ( header: true ) .map { row -> row.sra_id } // Configuration file sra_ids = \"input.csv\" By specifying inputs from sample sheets like this we can change inputs of a workflow execution by creating another sample sheet and specifying e.g. , --sra_ids input-2.csv on the command line. This is highly useful when you want to run a single sample e.g. , when testing a workflow, or when you want to keep track of all the different inputs you've used historically. Sample sheets are also useful for keeping other metadata, such as custom sample names, sample groups, location of files, etc. For example: sample-1,case,data/sample-1.fastq.gz sample-2,ctrl,data/sample-2.fastq.gz sample-3,case,data/sample-3.fastq.gz sample-4,ctrl,data/sample-4.fastq.gz Here we have not only names and file paths but also to which group each sample belongs, i.e. case or control. Such metadata can be highly useful for more advanced workflows to use in downstream analyses, such as differential gene expression! We could create a tuple based on this metadata like so: ch_input = Channel .fromPath(\"metadata.csv\") .splitCsv(header: ['id', 'group', 'fastq']) .view() Input file formatting The input file may also have headers, in which case you can use header: true to use the column headers defined in the file. You can also read e.g. tab-separated files by using the sep field: .splitCsv(sep: \"\\t\") .","title":"Configuring inputs"},{"location":"nextflow/nextflow-5-workflow-configuration/#other-configuration-scopes","text":"There are lots of things that you might want to add to your configuration, not just parameters! The workflow manifest , for example, which might look like this: manifest { name = \"My Workflow\" description = \"My workflow, created by me\" author = \"Me\" mainScript = \"main.nf\" version = \"1.0.0\" } Go ahead and add a workflow manifest to your nextflow.config file! The manifest is useful when you're publishing or sharing the workflow through e.g. GitHub or similar. There are many more such configuration scopes that you might want to use - read more about them in the documentation . Quick recap In this section we learnt: How to create parameters in a configuration file How to specify parameters on the command line How to add workflow manifest and other configuration scopes","title":"Other configuration scopes"},{"location":"nextflow/nextflow-6-optimising-the-mrsa-workflow/","text":"We just added several parameters and configurations to our MRSA workflow, but we didn't do anything about the reference genomes: those are still hard-coded. How come? Well, the current MRSA workflow is, in fact, not very well-optimised for Nextflow at all, being a refactor from the Snakemake tutorial of this course. All of the processes are basically unchanged, excluding some minor alterations. For example, the run_fastqc rule in Snakemake used the -o flag to specify that the results should be in the current directory, followed by moving the output files to their respective output directory. The first part is not needed in Nextflow (as everything is run in its own subdirectory), and the second part is done by the publishDir directive. These are just minor alterations, though, but we can do much more if we fully utilise Nextflow's features! Remote files # One of these features is the ability to automatically download remote files, without needing to explicitly do so! The path input type can handle either file paths (like we've done so far) or a URI-supported protocol (such as http:// , s3:// , ftp:// , etc. ). This would be highly useful for e.g. the GET_GENOME_FASTA process - in fact, we don't even need that process at all! All we need to do is to change the input to the INDEX_GENOME and ALIGN_TO_GENOME processes. Create a new input channel using the fromPath() channel factory and the absolute path to the genome FASTA. Make the INDEX_GENOME process use that input channel instead of the previously used output of the GET_GENOME_FASTA process. Remove the GET_GENOME_FASTA process, as it is not needed anymore. Re-run the workflow to see if it worked. Check the code below for an example if you're stuck: Click to show # Channel creation ch_genome_fasta = Channel.fromPath( \"ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna/Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\" ) # Workflow definition INDEX_GENOME ( ch_genome_fasta ) If we want to get detailed we can also change the hard-coded \"NCT8325\" naming in e.g. the INDEX_GENOME process and put that in another parameter, or grab the baseName() from the channel and make a [prefix, file] tuple using the map{} operator like we did previously; check below if you're curious of how this could be done. Click to show // Channel definition ch_genome_fasta = Channel .fromPath( \"ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna/Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\" ) .map{ file -> tuple(file.getBaseName(), file) } // INDEX_GENOME process definition process INDEX_GENOME { publishDir \"results/intermediate/\", mode: \"copy\" input: tuple val(fasta_name), path(fasta) output: path(\"*.b2t\"), emit: index script: \"\"\" # Bowtie2 cannot use .gz, so unzip to a temporary file first gunzip -c ${fasta} > tempfile bowtie2-build tempfile ${fasta_name} \"\"\" } Subworkflows # The DSL2 allows highly modular workflow design, where a workflow may contain multiple subworkflows . A subworkflow is just like a normal workflow, but it can be called inside other workflows, similar to a process. There is thus no special difference between a subworkflow and a workflow; the only difference is how you use them in practice. Let's take a look at a toy example: workflow { ch_input = Channel.fromPath( params.input ) SUBWORKFLOW( ch_input ) } workflow SUBWORKFLOW { take: input_file main: ALIGN_READS( input_file ) emit: bam = ALIGN_READS.out.bam } Here we have an unnamed, main workflow like before, plus a named subworkflow. A workflow can have inputs specified by the take directive, which is the equivalent of process input for workflows. The main part is the workflow body, which contains how to run which processes in which order. The last part, emit , also works the same as for processes, in that we name the different outputs of the workflow so that we may use them in other workflows or processes. Nextflow will run the unnamed workflow by default, unless the -entry flag is specified, like so: nextflow run main.nf -entry SUBWORKFLOW This will run the workflow named SUBWORKFLOW , but nothing else. You can also store subworkflows in separate files, so that everything doesn't have to be crammed into a single main.nf file. A subworkflow named SUBWORKFLOW contained in the file subworkflow.nf can be loaded into a main.nf file like so: include { SUBWORKFLOW } from \"./subworkflow.nf\" If you have a complex workflow with several subworkflows you might thus store them in a separate directory, e.g. subworkflows . This allows you to have fine-grained control over the general architecture of your Nextflow workflows, organising them in a manner that is easy to code and maintain. A process can also be treated in the same manner, and defined separately in another file. Now it's your turn! Separate the RUN_FASTQC and RUN_MULTIQC processes out of the main workflow and into a subworkflow. Check below if you're having trouble. Click to show // In the main workflow: RUN_QC ( GET_SRA_BY_ACCESSION.out ) // A new subworkflow workflow RUN_QC { take: fastq main: RUN_FASTQC ( fastq ) RUN_MULTIQC ( RUN_FASTQC.out.zip.collect() ) } If you want to challenge yourself, try to do the same with the INDEX_GENOME , ALIGN_TO_GENOME and SORT_BAM processes! Be careful of where you get your inputs and outputs. Quick recap In this section we learnt: How to automatically download remote files How to create and work with subworkflows","title":"The MRSA Workflow"},{"location":"nextflow/nextflow-6-optimising-the-mrsa-workflow/#remote-files","text":"One of these features is the ability to automatically download remote files, without needing to explicitly do so! The path input type can handle either file paths (like we've done so far) or a URI-supported protocol (such as http:// , s3:// , ftp:// , etc. ). This would be highly useful for e.g. the GET_GENOME_FASTA process - in fact, we don't even need that process at all! All we need to do is to change the input to the INDEX_GENOME and ALIGN_TO_GENOME processes. Create a new input channel using the fromPath() channel factory and the absolute path to the genome FASTA. Make the INDEX_GENOME process use that input channel instead of the previously used output of the GET_GENOME_FASTA process. Remove the GET_GENOME_FASTA process, as it is not needed anymore. Re-run the workflow to see if it worked. Check the code below for an example if you're stuck: Click to show # Channel creation ch_genome_fasta = Channel.fromPath( \"ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna/Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\" ) # Workflow definition INDEX_GENOME ( ch_genome_fasta ) If we want to get detailed we can also change the hard-coded \"NCT8325\" naming in e.g. the INDEX_GENOME process and put that in another parameter, or grab the baseName() from the channel and make a [prefix, file] tuple using the map{} operator like we did previously; check below if you're curious of how this could be done. Click to show // Channel definition ch_genome_fasta = Channel .fromPath( \"ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna/Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\" ) .map{ file -> tuple(file.getBaseName(), file) } // INDEX_GENOME process definition process INDEX_GENOME { publishDir \"results/intermediate/\", mode: \"copy\" input: tuple val(fasta_name), path(fasta) output: path(\"*.b2t\"), emit: index script: \"\"\" # Bowtie2 cannot use .gz, so unzip to a temporary file first gunzip -c ${fasta} > tempfile bowtie2-build tempfile ${fasta_name} \"\"\" }","title":"Remote files"},{"location":"nextflow/nextflow-6-optimising-the-mrsa-workflow/#subworkflows","text":"The DSL2 allows highly modular workflow design, where a workflow may contain multiple subworkflows . A subworkflow is just like a normal workflow, but it can be called inside other workflows, similar to a process. There is thus no special difference between a subworkflow and a workflow; the only difference is how you use them in practice. Let's take a look at a toy example: workflow { ch_input = Channel.fromPath( params.input ) SUBWORKFLOW( ch_input ) } workflow SUBWORKFLOW { take: input_file main: ALIGN_READS( input_file ) emit: bam = ALIGN_READS.out.bam } Here we have an unnamed, main workflow like before, plus a named subworkflow. A workflow can have inputs specified by the take directive, which is the equivalent of process input for workflows. The main part is the workflow body, which contains how to run which processes in which order. The last part, emit , also works the same as for processes, in that we name the different outputs of the workflow so that we may use them in other workflows or processes. Nextflow will run the unnamed workflow by default, unless the -entry flag is specified, like so: nextflow run main.nf -entry SUBWORKFLOW This will run the workflow named SUBWORKFLOW , but nothing else. You can also store subworkflows in separate files, so that everything doesn't have to be crammed into a single main.nf file. A subworkflow named SUBWORKFLOW contained in the file subworkflow.nf can be loaded into a main.nf file like so: include { SUBWORKFLOW } from \"./subworkflow.nf\" If you have a complex workflow with several subworkflows you might thus store them in a separate directory, e.g. subworkflows . This allows you to have fine-grained control over the general architecture of your Nextflow workflows, organising them in a manner that is easy to code and maintain. A process can also be treated in the same manner, and defined separately in another file. Now it's your turn! Separate the RUN_FASTQC and RUN_MULTIQC processes out of the main workflow and into a subworkflow. Check below if you're having trouble. Click to show // In the main workflow: RUN_QC ( GET_SRA_BY_ACCESSION.out ) // A new subworkflow workflow RUN_QC { take: fastq main: RUN_FASTQC ( fastq ) RUN_MULTIQC ( RUN_FASTQC.out.zip.collect() ) } If you want to challenge yourself, try to do the same with the INDEX_GENOME , ALIGN_TO_GENOME and SORT_BAM processes! Be careful of where you get your inputs and outputs. Quick recap In this section we learnt: How to automatically download remote files How to create and work with subworkflows","title":"Subworkflows"},{"location":"nextflow/nextflow-7-extra-material/","text":"There are many more things you can do with Nextflow than covered here. If you are interested to learn more details about Nextflow, we will briefly show some of its advanced features in this section. But first, here are some links to additional resources on Nextflow: Nextflow patterns that can help with common operations and concepts The Nextflow documentation Learning Nextflow in 2020 Nextflow training at Seqera A work-in-progress Nextflow Carpentry course Community help from Nextflow's Slack channel Using containers in Nextflow # Nextflow has built-in support for using both Docker and Singularity containers (and others too), either with a single container for the workflow as a whole or separate containers for each individual process. The simplest way to do it is to have a single container for your entire workflow, in which case you simply run the workflow and specify the image you want to use, like so: # Run with docker nextflow run main.nf -with-docker [image] # Run with Singularity nextflow run main.nf -with-singularity [image].sif If you don't want to supply this at every execution, you can also add it directly to your configuration file: # Docker configuration process.container = 'image' docker.enabled = true # Singularity configuration process.container = 'path/to/image.sif' singularity.enabled = true If you instead would like to have each process use a different container you can use the container directive in your processes: process PROCESS_01 { (...) container: 'image_01' (...) } process PROCESS_02 { (...) container: 'image_02' (...) } Regardless of which solution you go for, Nextflow will execute all the processes inside the specified container. In practice, this means that Nextflow will automatically wrap your processes and run them by executing the Docker or Singularity command with the image you have provided. Using Conda in Nextflow # While you can execute Nextflow inside Conda environments just like you would any other type of software, you can also use Conda with Nextflow in the same way as for Docker and Singularity above. You can either supply an environment.yml file, the path to an existing environment or the packages and their versions directly in the conda directive, like so: process PROCESS_01 { (...) conda: 'mrsa-environment.yml' (...) } process PROCESS_02 { (...) conda: 'path/to/mrsa-env' (...) } process PROCESS_03 { (...) conda: 'bioconda::bwa=0.7.17 bioconda::samtools=1.13' (...) } You can use either of the methods described above with your configuration file as well, here exemplified using an environment.yml file: process.conda = 'mrsa-environment.yml' Running Nextflow on Uppmax # A lot of researchers in Sweden are using the Uppmax computer cluster in Uppsala, which is easily handled by Nextflow. What you need to do is to add the following profile to your nextflow.config file: profiles { // Uppmax general profile uppmax { params{ account = null } process { executor = 'slurm' clusterOptions = \"-A '${params.account}'\" memory = { 6.GB * task.attempt } cpus = { 1 * task.attempt } time = { 10.h * task.attempt } scratch = '$SNIC_TMP' errorStrategy = 'retry' maxRetries = 1 } } } This will add a profile to your workflow, which you can access by running the workflow with -profile uppmax . You will also have to supply an extra parameter account which corresponds to your SNIC project account, but the rest you can leave as-is, unless you want to tinker with e.g. compute resource specifications. That's all you need! Nextflow will take care of communications with SLURM (the system used by Uppmax, specified by the executor line) and will send off jobs to the cluster for you, and everything will look exactly the same way as if you were executing the pipeline locally. The memory , cpus and time lines define the various resources Nextflow will use as well as how much to automatically increase them by if re-trying failed tasks; this, in turn, is specified by the errorStrategy and maxRetries variables. The scratch variable defines where each node's local storage is situated, which gives Nextflow the most optimal access to the Uppmax file system for temporary files. Advanced channel creation # The input data shown in the MRSA example workflow is not that complex, but Nextflow channels can do much more than that. A common scenario in high-throughput sequencing is that you have pairs of reads for each sample. Nextflow has a special, built-in way to create channels for this data type: the fromFilePairs channel factory: Channel .fromFilePairs ( \"data/*_R{1,2}.fastq.gz\" ) .set { ch_raw_reads } This will create a channel containing all the reads in the data/ directory in the format <sample>_R1.fastq.gz and <sample>_R2.fastq.gz and will pair them together into a nested tuple looking like this: [sample, [data/sample_R1.fastq.gz, data/sample_R2.fastq.gz]] The first element of the tuple ( [0] ) thus contains the value sample , while the second element ( [1] ) contains another tuple with paths to both read files. This nested tuple can be passed into processes for e.g. read alignment, and it makes the entire procedure of going from read pairs ( i.e. two separate files, one sample) into a single alignment file (one file, one sample) very simple. For more methods of reading in data see the Nextflow documentation on Channel Factories . We can also do quite advanced things to manipuate data in channels, such as this: Channel .fromPath ( params.metadata ) .splitCsv ( sep: \"\\t\", header: true ) .map { row -> tuple(\"${row.sample_id}\", \"${row.treatment}\") } .filter { id, treatment -> treatment != \"DMSO\" } .unique ( ) .set { samples_and_treatments } That's a bit of a handful! But what does it do? The first line specifies that we want to read some data from a file specified by the metadata parameter, and the second line actually reads that data using tab as delimiter, including a header. The map operator takes each entire row and subsets it to only two columns: the sample_id and treatment columns. This subset is stored as a tuple. The filter operator is then used to remove any tuples where the second entry, treatment , is not equal to the string \"DMSO\" ( i.e. untreated cells, in this example). We then only take the unique tuples and set the results as the new channel samples_and_treatments . Let's say that this is the metadata we're reading: sample_id dose group treatment sample_1 0.1 control DMSO sample_1 1.0 control DMSO sample_1 2.0 control DMSO sample_2 0.1 case vorinostat sample_2 1.0 case vorinostat sample_2 2.0 case vorinostat sample_3 0.1 case fulvestrant sample_3 1.0 case fulvestrant sample_3 2.0 case fulvestrant Given the channel creation strategy above, we would get the following result: [sample_2, vorinostat] [sample_3, fulvestrant] In this way, you can perform complex operations on input files or input metadata and send the resulting content to your downstream processes in a simple way. Composing data manipuations in Nextflow like this can be half the fun of writing the workflow. Check out Nextflow's documentation on Channel operators to see the full list of channel operations at your disposal. Using Groovy in processes # You don't have to use bash or external scripts inside your processes all the time unless you want to: Nextflow is based on Groovy, which allows you to use both Groovy and Bash in the same process. For example, have a look at this: process index_fasta { tag \"${fasta_name}\" input: tuple val(fasta), path(fasta_file) output: path(\"${fasta_name}.idx\"), emit: fasta script: fasta_name = fasta.substring(0, fasta.lastIndexOf(\".\")) \"\"\" index --ref ${fasta_file},${fasta_name} \"\"\" } Here we have some command index that, for whatever reason, requires both the path to a FASTA file and the name of that file without the .fasta extension. We can use Groovy in the script directive together with normal Bash, mixing and matching as we like. The first line of the script directive gets the name of the FASTA file without the extension by removing anything after the dot, while the second calls the index command like normal using bash. The nf-core pipeline collection # You may have heard of the nf-core pipeline collection previously, which is a large, collaborative bioinformatics community dedicated to building, developing and maintaining Nextflow workflows. In fact, if you have sequenced data at e.g. the National Genomics Infrastructure ( NGI ), you can be sure that the data processing has been run using one of the nf-core pipelines! While the community only started in 2018 (with a Nature Biotechnology paper in 2020), it already has over 30 production-ready pipelines with everything from genomics, transcriptomics, proteomics and metagenomics - and more being developed all the time. The nf-core pipelines all work in the same way, in that they have the same exact base for inputs, parameters and arguments, making them all highly similar to run. Since you've already learnt the basics of Nextflow in this course, you should now be able to also run the nf-core pipelines! It might be that you have a data type that you can analyse using one of the pipelines in nf-core, meaning you don't need to do anything other than find out what parameters you should run it with. Each pipeline comes with extensive documentation, test datasets that you can use to practice on, can be run on both HPCs like Uppmax, cloud services like AWS or locally on your own computer. All pipelines support both Conda and Docker/Singularity, and you can additionally run specific versions of the pipelines, allowing for full reproducibility of your analyses. If you want to check nf-core out, simply head over to their list of pipelines and see what's available! Who knows, you might even write your own nf-core pipeline in the future?","title":"Extra materiel"},{"location":"nextflow/nextflow-7-extra-material/#using-containers-in-nextflow","text":"Nextflow has built-in support for using both Docker and Singularity containers (and others too), either with a single container for the workflow as a whole or separate containers for each individual process. The simplest way to do it is to have a single container for your entire workflow, in which case you simply run the workflow and specify the image you want to use, like so: # Run with docker nextflow run main.nf -with-docker [image] # Run with Singularity nextflow run main.nf -with-singularity [image].sif If you don't want to supply this at every execution, you can also add it directly to your configuration file: # Docker configuration process.container = 'image' docker.enabled = true # Singularity configuration process.container = 'path/to/image.sif' singularity.enabled = true If you instead would like to have each process use a different container you can use the container directive in your processes: process PROCESS_01 { (...) container: 'image_01' (...) } process PROCESS_02 { (...) container: 'image_02' (...) } Regardless of which solution you go for, Nextflow will execute all the processes inside the specified container. In practice, this means that Nextflow will automatically wrap your processes and run them by executing the Docker or Singularity command with the image you have provided.","title":"Using containers in Nextflow"},{"location":"nextflow/nextflow-7-extra-material/#using-conda-in-nextflow","text":"While you can execute Nextflow inside Conda environments just like you would any other type of software, you can also use Conda with Nextflow in the same way as for Docker and Singularity above. You can either supply an environment.yml file, the path to an existing environment or the packages and their versions directly in the conda directive, like so: process PROCESS_01 { (...) conda: 'mrsa-environment.yml' (...) } process PROCESS_02 { (...) conda: 'path/to/mrsa-env' (...) } process PROCESS_03 { (...) conda: 'bioconda::bwa=0.7.17 bioconda::samtools=1.13' (...) } You can use either of the methods described above with your configuration file as well, here exemplified using an environment.yml file: process.conda = 'mrsa-environment.yml'","title":"Using Conda in Nextflow"},{"location":"nextflow/nextflow-7-extra-material/#running-nextflow-on-uppmax","text":"A lot of researchers in Sweden are using the Uppmax computer cluster in Uppsala, which is easily handled by Nextflow. What you need to do is to add the following profile to your nextflow.config file: profiles { // Uppmax general profile uppmax { params{ account = null } process { executor = 'slurm' clusterOptions = \"-A '${params.account}'\" memory = { 6.GB * task.attempt } cpus = { 1 * task.attempt } time = { 10.h * task.attempt } scratch = '$SNIC_TMP' errorStrategy = 'retry' maxRetries = 1 } } } This will add a profile to your workflow, which you can access by running the workflow with -profile uppmax . You will also have to supply an extra parameter account which corresponds to your SNIC project account, but the rest you can leave as-is, unless you want to tinker with e.g. compute resource specifications. That's all you need! Nextflow will take care of communications with SLURM (the system used by Uppmax, specified by the executor line) and will send off jobs to the cluster for you, and everything will look exactly the same way as if you were executing the pipeline locally. The memory , cpus and time lines define the various resources Nextflow will use as well as how much to automatically increase them by if re-trying failed tasks; this, in turn, is specified by the errorStrategy and maxRetries variables. The scratch variable defines where each node's local storage is situated, which gives Nextflow the most optimal access to the Uppmax file system for temporary files.","title":"Running Nextflow on Uppmax"},{"location":"nextflow/nextflow-7-extra-material/#advanced-channel-creation","text":"The input data shown in the MRSA example workflow is not that complex, but Nextflow channels can do much more than that. A common scenario in high-throughput sequencing is that you have pairs of reads for each sample. Nextflow has a special, built-in way to create channels for this data type: the fromFilePairs channel factory: Channel .fromFilePairs ( \"data/*_R{1,2}.fastq.gz\" ) .set { ch_raw_reads } This will create a channel containing all the reads in the data/ directory in the format <sample>_R1.fastq.gz and <sample>_R2.fastq.gz and will pair them together into a nested tuple looking like this: [sample, [data/sample_R1.fastq.gz, data/sample_R2.fastq.gz]] The first element of the tuple ( [0] ) thus contains the value sample , while the second element ( [1] ) contains another tuple with paths to both read files. This nested tuple can be passed into processes for e.g. read alignment, and it makes the entire procedure of going from read pairs ( i.e. two separate files, one sample) into a single alignment file (one file, one sample) very simple. For more methods of reading in data see the Nextflow documentation on Channel Factories . We can also do quite advanced things to manipuate data in channels, such as this: Channel .fromPath ( params.metadata ) .splitCsv ( sep: \"\\t\", header: true ) .map { row -> tuple(\"${row.sample_id}\", \"${row.treatment}\") } .filter { id, treatment -> treatment != \"DMSO\" } .unique ( ) .set { samples_and_treatments } That's a bit of a handful! But what does it do? The first line specifies that we want to read some data from a file specified by the metadata parameter, and the second line actually reads that data using tab as delimiter, including a header. The map operator takes each entire row and subsets it to only two columns: the sample_id and treatment columns. This subset is stored as a tuple. The filter operator is then used to remove any tuples where the second entry, treatment , is not equal to the string \"DMSO\" ( i.e. untreated cells, in this example). We then only take the unique tuples and set the results as the new channel samples_and_treatments . Let's say that this is the metadata we're reading: sample_id dose group treatment sample_1 0.1 control DMSO sample_1 1.0 control DMSO sample_1 2.0 control DMSO sample_2 0.1 case vorinostat sample_2 1.0 case vorinostat sample_2 2.0 case vorinostat sample_3 0.1 case fulvestrant sample_3 1.0 case fulvestrant sample_3 2.0 case fulvestrant Given the channel creation strategy above, we would get the following result: [sample_2, vorinostat] [sample_3, fulvestrant] In this way, you can perform complex operations on input files or input metadata and send the resulting content to your downstream processes in a simple way. Composing data manipuations in Nextflow like this can be half the fun of writing the workflow. Check out Nextflow's documentation on Channel operators to see the full list of channel operations at your disposal.","title":"Advanced channel creation"},{"location":"nextflow/nextflow-7-extra-material/#using-groovy-in-processes","text":"You don't have to use bash or external scripts inside your processes all the time unless you want to: Nextflow is based on Groovy, which allows you to use both Groovy and Bash in the same process. For example, have a look at this: process index_fasta { tag \"${fasta_name}\" input: tuple val(fasta), path(fasta_file) output: path(\"${fasta_name}.idx\"), emit: fasta script: fasta_name = fasta.substring(0, fasta.lastIndexOf(\".\")) \"\"\" index --ref ${fasta_file},${fasta_name} \"\"\" } Here we have some command index that, for whatever reason, requires both the path to a FASTA file and the name of that file without the .fasta extension. We can use Groovy in the script directive together with normal Bash, mixing and matching as we like. The first line of the script directive gets the name of the FASTA file without the extension by removing anything after the dot, while the second calls the index command like normal using bash.","title":"Using Groovy in processes"},{"location":"nextflow/nextflow-7-extra-material/#the-nf-core-pipeline-collection","text":"You may have heard of the nf-core pipeline collection previously, which is a large, collaborative bioinformatics community dedicated to building, developing and maintaining Nextflow workflows. In fact, if you have sequenced data at e.g. the National Genomics Infrastructure ( NGI ), you can be sure that the data processing has been run using one of the nf-core pipelines! While the community only started in 2018 (with a Nature Biotechnology paper in 2020), it already has over 30 production-ready pipelines with everything from genomics, transcriptomics, proteomics and metagenomics - and more being developed all the time. The nf-core pipelines all work in the same way, in that they have the same exact base for inputs, parameters and arguments, making them all highly similar to run. Since you've already learnt the basics of Nextflow in this course, you should now be able to also run the nf-core pipelines! It might be that you have a data type that you can analyse using one of the pipelines in nf-core, meaning you don't need to do anything other than find out what parameters you should run it with. Each pipeline comes with extensive documentation, test datasets that you can use to practice on, can be run on both HPCs like Uppmax, cloud services like AWS or locally on your own computer. All pipelines support both Conda and Docker/Singularity, and you can additionally run specific versions of the pipelines, allowing for full reproducibility of your analyses. If you want to check nf-core out, simply head over to their list of pipelines and see what's available! Who knows, you might even write your own nf-core pipeline in the future?","title":"The nf-core pipeline collection"},{"location":"project/project/","text":"Putting everything together # It is time to try to set up a project from scratch and use the different tools that we have covered during the course together! This exercise is very open-ended and you have free hands to try out a bit of what you want. But you should aim to use what you've learned to do the following: Create a new git repository for the project (either on BitBucket or GitHub) Add a README file which should contain the required information on how to run the project Create a Conda environment.yml file with the required dependencies Create a R Markdown or Jupyter notebook to run your code Alternatively, create a Snakefile to run your code as a workflow and use a config.yml file to add settings to the workflow Use git to continuously commit changes to the repository Possibly make a Docker or Singularity image for your project This is not a small task and may seem overwhelming! Don't worry if you feel lost or if the task seems daunting. To get the most out of the exercise, take one step at a time and go back to the previous tutorials for help and inspiration. The goal is not necessarily for you to finish the whole exercise, but to really think about each step and how it all fits together in practice. Recommendation We recommend to start with git, Conda and a notebook, as we would see these as the core tools to make a research project reproducible. We suggest to keep the analysis for this exercise short so that you have time to try out the different tools together while you have the opportunity to ask for help. Your own project # This is a great opportunity for you to try to implement these methods on one of your current research projects. It is of course up to you which tools to include in making your research project reproducible, but we suggest to aim for at least git and Conda. Tip If your analysis project contains computationally intense steps it may be good to scale them down for the sake of the exercise. You might, for example, subset your raw data to only contain a minuscule part of its original size. You can then test your implementation on the subset and only run it on the whole dataset once everything works to your satisfaction. Alternative: student experience project # If you don't want to use a project you're currently working on we have a suggestion for a small-scale project for you. The idea is to analyze students' experiences at this Reproducible Research course. For this you will use responses from students to the registration form for the course. Below you'll find links to files in *.csv format with answers from 3 course instances: 2018-11 https://docs.google.com/spreadsheets/d/1yLcJL-rIAO51wWCPrAdSqZvCJswTqTSt4cFFe_eTjlQ/export?format=csv 2019-05 https://docs.google.com/spreadsheets/d/1mBp857raqQk32xGnQHd6Ys8oZALgf6KaFehfdwqM53s/export?format=csv 2019-11 https://docs.google.com/spreadsheets/d/1aLGpS9WKvmYRnsdmvvgX_4j9hyjzJdJCkkQdqWq-uvw/export?format=csv The goal here is to create a Snakemake workflow, which contains the following: Has a rule that downloads the csv files (making use of a config.yml file to pass the URLs and file names) Has a rule that cleans the files (making use of wildcards so that the same rule can be run on each file) The final step is to plot the student experience in some way Remember to Keep everything versioned controlled with git Add information to the README file so others know how to re-run the project Add required software to the Conda environment.yml file Inspiration and tips for the student experience workflow # The first two steps should be part of the Snakemake workflow. If you need some help with the cleaning step, see below for a Python script that you can save to a file and run in the second Snakemake rule. ??? note \"Click to show a script for cleaning column names\" The script ( e.g. clean_csv.py ): ```python #!/usr/bin/env python import pandas as pd from argparse import ArgumentParser def main(args): df = pd.read_csv(args.input, header=0) df.rename(columns=lambda x: x.split(\"[\")[-1].rstrip(\"]\"), inplace=True) df.rename(columns={'R Markdown': 'RMarkdown'}, inplace=True) df.to_csv(args.output, index=False) if __name__ == '__main__': parser = ArgumentParser() parser.add_argument(\"input\", type=str, help=\"Input csv file\") parser.add_argument(\"output\", type=str, help=\"Output csv file cleaned\") args = parser.parse_args() main(args) ``` Command to execute the script: ``` python clean_csv.py input_file.csv output_file.csv ``` The third step is really up to you how to implement. You could: Include the plotting in the workflow using an RMarkdown document that gets rendered into a report Have a script that produces separate figures ( e.g. png files) Create a jupyter notebook that reads the cleaned output from the workflow and generates some plot or does other additional analyses If you need some help/inspiration with plotting the results, click below to see an example Python script that you can save to file and run with the cleaned files as input. ??? note \"Click to show a script for plotting the student experience\" The script ( e.g. plot.py ): ```python #!/usr/bin/env python import matplotlib as mpl import matplotlib.pyplot as plt plt.style.use('ggplot') mpl.use('agg') import pandas as pd import seaborn as sns import numpy as np from argparse import ArgumentParser def read_files(files): \"\"\"Reads experience counts and concatenates into one dataframe\"\"\" df = pd.DataFrame() for i, f in enumerate(files): # Extract date d = f.split(\".\")[0] _df = pd.read_csv(f, sep=\",\", header=0) # Assign date _df = _df.assign(Date=pd.Series([d]*len(_df), index=_df.index)) if i==0: df = _df.copy() else: df = pd.concat([df,_df], sort=True) return df.reset_index().drop(\"index\",axis=1).fillna(0) def count_experience(df, normalize=False): \"\"\"Generates long format dataframe of counts\"\"\" df_l = pd.DataFrame() for software in df.columns: if software==\"Date\": continue # Groupby software and count _df = df.groupby([\"Date\",software]).count().iloc[:,0].reset_index() _df.columns = [\"Date\",\"Experience\",\"Count\"] _df = _df.assign(Software=pd.Series([software]*len(_df), index=_df.index)) if normalize: _df = pd.merge(_df.groupby(\"Date\").sum().rename(columns={'Count':'Tot'}),_df, left_index=True, right_on=\"Date\") _df.Count = _df.Count.div(_df.Tot)*100 _df.rename(columns={'Count': '%'}, inplace=True) df_l = pd.concat([df_l, _df], sort=True) df_l.loc[df_l.Experience==0,\"Experience\"] = np.nan return df_l def plot_catplot(df, outdir, figname, y, palette=\"Blues\"): \"\"\"Plot barplots of user experience per software\"\"\" ax = sns.catplot(data=df, x=\"Date\", col=\"Software\", col_wrap=3, y=y, hue=\"Experience\", height=2.8, kind=\"bar\", hue_order=[\"Never heard of it\", \"Heard of it but haven't used it\", \"Tried it once or twice\", \"Use it\"], col_order=[\"Conda\", \"Git\", \"Snakemake\", \"Jupyter\", \"RMarkdown\", \"Docker\", \"Singularity\"], palette=palette) ax.set_titles(\"{col_name}\") plt.savefig(\"{}/{}\".format(outdir, figname), bbox_to_inches=\"tight\", dpi=300) plt.close() def plot_barplot(df, outdir, figname, x): \"\"\"Plot a barplot summarizing user experience over all software\"\"\" ax = sns.barplot(data=df, hue=\"Date\", y=\"Experience\", x=x, errwidth=.5, order=[\"Never heard of it\", \"Heard of it but haven't used it\", \"Tried it once or twice\", \"Use it\"]) plt.savefig(\"{}/{}\".format(outdir, figname), bbox_inches=\"tight\", dpi=300) plt.close() def main(args): # Read all csv files df = read_files(args.files) # Count experience df_l = count_experience(df) # Count and normalize experience df_lp = count_experience(df, normalize=True) # Plot catplot of student experience plot_catplot(df_l, args.outdir, \"exp_counts.png\", y=\"Count\") # Plot catplot of student experience in % plot_catplot(df_lp, args.outdir, \"exp_percent.png\", y=\"%\", palette=\"Reds\") # Plot barplot of experience plot_barplot(df_lp, args.outdir, \"exp_barplot.png\", x=\"%\") if __name__ == '__main__': parser = ArgumentParser() parser.add_argument(\"files\", nargs=\"+\", help=\"CSV files with student experience to produce plots for\") parser.add_argument(\"--outdir\", type=str, default=\".\", help=\"Output directory for plots (defaults to current directory)\") args = parser.parse_args() main(args) ``` Command to execute the script: ``` python plot.py file1.csv file2.csv file3.csv --outdir results/ ```","title":"Making a project reproducible"},{"location":"project/project/#putting-everything-together","text":"It is time to try to set up a project from scratch and use the different tools that we have covered during the course together! This exercise is very open-ended and you have free hands to try out a bit of what you want. But you should aim to use what you've learned to do the following: Create a new git repository for the project (either on BitBucket or GitHub) Add a README file which should contain the required information on how to run the project Create a Conda environment.yml file with the required dependencies Create a R Markdown or Jupyter notebook to run your code Alternatively, create a Snakefile to run your code as a workflow and use a config.yml file to add settings to the workflow Use git to continuously commit changes to the repository Possibly make a Docker or Singularity image for your project This is not a small task and may seem overwhelming! Don't worry if you feel lost or if the task seems daunting. To get the most out of the exercise, take one step at a time and go back to the previous tutorials for help and inspiration. The goal is not necessarily for you to finish the whole exercise, but to really think about each step and how it all fits together in practice. Recommendation We recommend to start with git, Conda and a notebook, as we would see these as the core tools to make a research project reproducible. We suggest to keep the analysis for this exercise short so that you have time to try out the different tools together while you have the opportunity to ask for help.","title":"Putting everything together"},{"location":"project/project/#your-own-project","text":"This is a great opportunity for you to try to implement these methods on one of your current research projects. It is of course up to you which tools to include in making your research project reproducible, but we suggest to aim for at least git and Conda. Tip If your analysis project contains computationally intense steps it may be good to scale them down for the sake of the exercise. You might, for example, subset your raw data to only contain a minuscule part of its original size. You can then test your implementation on the subset and only run it on the whole dataset once everything works to your satisfaction.","title":"Your own project"},{"location":"project/project/#alternative-student-experience-project","text":"If you don't want to use a project you're currently working on we have a suggestion for a small-scale project for you. The idea is to analyze students' experiences at this Reproducible Research course. For this you will use responses from students to the registration form for the course. Below you'll find links to files in *.csv format with answers from 3 course instances: 2018-11 https://docs.google.com/spreadsheets/d/1yLcJL-rIAO51wWCPrAdSqZvCJswTqTSt4cFFe_eTjlQ/export?format=csv 2019-05 https://docs.google.com/spreadsheets/d/1mBp857raqQk32xGnQHd6Ys8oZALgf6KaFehfdwqM53s/export?format=csv 2019-11 https://docs.google.com/spreadsheets/d/1aLGpS9WKvmYRnsdmvvgX_4j9hyjzJdJCkkQdqWq-uvw/export?format=csv The goal here is to create a Snakemake workflow, which contains the following: Has a rule that downloads the csv files (making use of a config.yml file to pass the URLs and file names) Has a rule that cleans the files (making use of wildcards so that the same rule can be run on each file) The final step is to plot the student experience in some way Remember to Keep everything versioned controlled with git Add information to the README file so others know how to re-run the project Add required software to the Conda environment.yml file","title":"Alternative: student experience project"},{"location":"project/project/#inspiration-and-tips-for-the-student-experience-workflow","text":"The first two steps should be part of the Snakemake workflow. If you need some help with the cleaning step, see below for a Python script that you can save to a file and run in the second Snakemake rule. ??? note \"Click to show a script for cleaning column names\" The script ( e.g. clean_csv.py ): ```python #!/usr/bin/env python import pandas as pd from argparse import ArgumentParser def main(args): df = pd.read_csv(args.input, header=0) df.rename(columns=lambda x: x.split(\"[\")[-1].rstrip(\"]\"), inplace=True) df.rename(columns={'R Markdown': 'RMarkdown'}, inplace=True) df.to_csv(args.output, index=False) if __name__ == '__main__': parser = ArgumentParser() parser.add_argument(\"input\", type=str, help=\"Input csv file\") parser.add_argument(\"output\", type=str, help=\"Output csv file cleaned\") args = parser.parse_args() main(args) ``` Command to execute the script: ``` python clean_csv.py input_file.csv output_file.csv ``` The third step is really up to you how to implement. You could: Include the plotting in the workflow using an RMarkdown document that gets rendered into a report Have a script that produces separate figures ( e.g. png files) Create a jupyter notebook that reads the cleaned output from the workflow and generates some plot or does other additional analyses If you need some help/inspiration with plotting the results, click below to see an example Python script that you can save to file and run with the cleaned files as input. ??? note \"Click to show a script for plotting the student experience\" The script ( e.g. plot.py ): ```python #!/usr/bin/env python import matplotlib as mpl import matplotlib.pyplot as plt plt.style.use('ggplot') mpl.use('agg') import pandas as pd import seaborn as sns import numpy as np from argparse import ArgumentParser def read_files(files): \"\"\"Reads experience counts and concatenates into one dataframe\"\"\" df = pd.DataFrame() for i, f in enumerate(files): # Extract date d = f.split(\".\")[0] _df = pd.read_csv(f, sep=\",\", header=0) # Assign date _df = _df.assign(Date=pd.Series([d]*len(_df), index=_df.index)) if i==0: df = _df.copy() else: df = pd.concat([df,_df], sort=True) return df.reset_index().drop(\"index\",axis=1).fillna(0) def count_experience(df, normalize=False): \"\"\"Generates long format dataframe of counts\"\"\" df_l = pd.DataFrame() for software in df.columns: if software==\"Date\": continue # Groupby software and count _df = df.groupby([\"Date\",software]).count().iloc[:,0].reset_index() _df.columns = [\"Date\",\"Experience\",\"Count\"] _df = _df.assign(Software=pd.Series([software]*len(_df), index=_df.index)) if normalize: _df = pd.merge(_df.groupby(\"Date\").sum().rename(columns={'Count':'Tot'}),_df, left_index=True, right_on=\"Date\") _df.Count = _df.Count.div(_df.Tot)*100 _df.rename(columns={'Count': '%'}, inplace=True) df_l = pd.concat([df_l, _df], sort=True) df_l.loc[df_l.Experience==0,\"Experience\"] = np.nan return df_l def plot_catplot(df, outdir, figname, y, palette=\"Blues\"): \"\"\"Plot barplots of user experience per software\"\"\" ax = sns.catplot(data=df, x=\"Date\", col=\"Software\", col_wrap=3, y=y, hue=\"Experience\", height=2.8, kind=\"bar\", hue_order=[\"Never heard of it\", \"Heard of it but haven't used it\", \"Tried it once or twice\", \"Use it\"], col_order=[\"Conda\", \"Git\", \"Snakemake\", \"Jupyter\", \"RMarkdown\", \"Docker\", \"Singularity\"], palette=palette) ax.set_titles(\"{col_name}\") plt.savefig(\"{}/{}\".format(outdir, figname), bbox_to_inches=\"tight\", dpi=300) plt.close() def plot_barplot(df, outdir, figname, x): \"\"\"Plot a barplot summarizing user experience over all software\"\"\" ax = sns.barplot(data=df, hue=\"Date\", y=\"Experience\", x=x, errwidth=.5, order=[\"Never heard of it\", \"Heard of it but haven't used it\", \"Tried it once or twice\", \"Use it\"]) plt.savefig(\"{}/{}\".format(outdir, figname), bbox_inches=\"tight\", dpi=300) plt.close() def main(args): # Read all csv files df = read_files(args.files) # Count experience df_l = count_experience(df) # Count and normalize experience df_lp = count_experience(df, normalize=True) # Plot catplot of student experience plot_catplot(df_l, args.outdir, \"exp_counts.png\", y=\"Count\") # Plot catplot of student experience in % plot_catplot(df_lp, args.outdir, \"exp_percent.png\", y=\"%\", palette=\"Reds\") # Plot barplot of experience plot_barplot(df_lp, args.outdir, \"exp_barplot.png\", x=\"%\") if __name__ == '__main__': parser = ArgumentParser() parser.add_argument(\"files\", nargs=\"+\", help=\"CSV files with student experience to produce plots for\") parser.add_argument(\"--outdir\", type=str, default=\".\", help=\"Output directory for plots (defaults to current directory)\") args = parser.parse_args() main(args) ``` Command to execute the script: ``` python plot.py file1.csv file2.csv file3.csv --outdir results/ ```","title":"Inspiration and tips for the student experience workflow"},{"location":"rmarkdown/r-markdown-1-introduction/","text":"A markup language is a system for annotating text documents in order to e.g. define formatting. HTML, if you are familiar with that, is an example of a markup language. HTML uses tags, such as: <h1>Heading</h1> <h2>Sub-heading</h2> <a href=\"www.webpage.com\">Link</a> <ul> <li>List-item1</li> <li>List-item2</li> <li>List-item3</li> </ul> Markdown is a lightweight markup language which uses plain-text syntax in order to be as unobtrusive as possible, so that a human can easily read it. Some examples: # Heading ## Sub-heading ### Another deeper heading A [link](http://example.com). Text attributes _italic_, *italic*, **bold**, `monospace`. Bullet list: * apples * oranges * pears A markdown document can be converted to other formats, such as HTML or PDF, for viewing in a browser or a PDF reader; the page you are reading right now is written in markdown. Markdown is somewhat ill-defined, and as a consequence of that there exist many implementations and extensions, although they share most of the syntax. R Markdown is one such implementation/extension. R Markdown documents can be used both to save and execute code and to generate reports in various formats. This is done by mixing markdown (as in the example above), and so-called code chunks in the same document. The code itself, as well as the output it generates, can be included in the final report. R Markdown makes your analysis more reproducible by connecting your code, figures and descriptive text. You can use it to make reproducible reports, rather than e.g. copy-pasting figures into a Word document. You can also use it as a notebook, in the same way as lab notebooks are used in a wet lab setting (or as we utilise Jupyter notebooks in the tutorial after this one). This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. Place yourself in the workshop-reproducible-research/tutorials/rmarkdown/ directory, activate your rmarkdown-env Conda environment and start RStudio from the command line (type rstudio & ).","title":"Introduction"},{"location":"rmarkdown/r-markdown-2-the-basics/","text":"Let's begin with starting RStudio and opening a new file ( File --> New File --> R Markdown ). If you're using Conda you should have all the packages needed, but install anything that RStudio prompts you to. In the window that opens, select Document and HTML (which should be the default), and click Ok . This will open a template R Markdown document for you. On the top is a so called YAML header: --- title: \"Untitled\" output: html_document: toc: true --- Attention! The header might look slightly different depending on your version of RStudio. If so, replace the default with the header above. Here we can specify settings for the document, like the title and the output format. Change the title to My first R Markdown document Now, read through the rest of the template R Markdown document to get a feeling for the format. As you can see, there are essentially three types of components in an R Markdown document: Text (written in R Markdown) Code chunks (written in R or another supported language ) The YAML header Let's dig deeper into each of these in the following sections! But first, just to get the flavor for things to come: press the little Knit -button located at the top of the text editor panel in RStudio. This will prompt you to save the Rmd file (do that), and generate the output file (an HTML file in this case). It will also open up a preview of this file for you. Some commonly used formatting written in markdown is shown below, which you may recognize from the Git tutorial : # This is a heading This is a paragraph. This line-break will not appear in the output file.\\ But this will (since the previous line ends with a backslash). This is a new paragraph. ## This is a sub-heading This is **bold text**, this is *italic text*, this is `monospaced text`, and this is [a link](http://rmarkdown.rstudio.com/lesson-1.html). An important feature of R Markdown is that you are allowed to use R code inline to generate text by enclosing it with `r `. As an example: 112/67 is equal to `r round(112/67, 2)`. You can also use multiple commands like this: I like `r fruits <- c(\"apples\",\"bananas\"); paste(fruits, collapse = \" and \")`! The above markdown would generate something like this: { width=600px } Instead of reiterating information here, take a look on the first page (only the first page!) of this reference . This will show you how to write more stuff in markdown and how it will be displayed once the markdown document is converted to an output file ( e.g. HTML or PDF). An even more complete guide is available here . Try out some of the markdown described above (and in the links) in your template R Markdown document! Press Knit to see the effect of your changes. Don't worry about the code chunks just yet, we'll come to that in a second. Quick recap In this section you learned and tried out some of the basic markdown syntax.","title":"The basics"},{"location":"rmarkdown/r-markdown-3-code-chunks/","text":"Enough about markdown, let's get to the fun part and include some code! Look at the last code chunk in the template R Markdown document that you just created, as an example: ```{r pressure, echo = FALSE} plot(pressure) ``` The R code is surrounded by: ```{r} and ``` . The r indicates that the code chunk contains R code (it is possible to add code chunks using other languages, e.g. Python). After that comes an optional chunk name, pressure in this case (this can be used to reference the code chunk as well as alleviate debugging). Last comes chunk options, separated by commas (in this case there is only one option: echo = FALSE ). Note The code chunk name pressure has nothing to do with the code plot(pressure) . In the latter case, pressure is a default R dataframe that is used in examples. The chunk name happened to be set to the string pressure as well, but could just as well have been called something else, e.g. \"Plot pressure data\". Below are listed some useful chunk options related to evaluating and displaying code chunks in the final file: Chunk option Effect `echo = FALSE` Prevents code, but not the results, from appearing in the finished file. This is a useful way to embed figures. `include = FALSE` Prevents both code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks. `eval = FALSE` The code in the code chunk will not be run (but the code can be displayed in the finished file). Since the code is not evaluated, no results can be shown. `results = \"hide\"` Evaluate (and display) the code, but don't show the results. `message = FALSE` Prevents messages that are generated by code from appearing in the finished file. `warning = FALSE` Prevents warnings that are generated by code from appearing in the finished file. Go back to your template R Markdown document in RStudio and locate the cars code chunk. Add the option echo = FALSE : ```{r cars, echo = FALSE} summary(cars) ``` How do you think this will affect the rendered file? Press Knit and check if you were right. Remove the echo = FALSE option and add eval = FALSE instead: ```{r cars, eval = FALSE} summary(cars) ``` How do you think this will affect the rendered file? Press Knit and check if you were right. Remove the eval = FALSE option and add include = FALSE instead: ```{r cars, include = FALSE} summary(cars) ``` There are also some chunk options related to plots: Chunk option Effect `fig.height = 9, fig.width = 6` Set plot dimensions to 9x6 inches (the default is 7x7.) `out.height = \"10cm\", out.width = \"8cm\"` Scale plot to 10x8 cm in the final output file. `fig.cap = \"This is a plot.\"` Add a figure caption. Go back to your template R Markdown document in RStudio and locate the pressure code chunk. Add the fig.width and fig.height options as below: ```{r pressure, echo = FALSE, fig.width = 6, fig.height = 4} plot(pressure) ``` Press Knit and look at the output. Can you see any differences? Now add a whole new code chunk to the end of the document. Give it the name pressure 2 (code chunks have to have unique names, or no name). Add the fig.width and out.width options like this: ```{r pressure 2, echo = FALSE, fig.width = 9, out.width = \"560px\"} plot(pressure) ``` Press Knit and look at the output. Notice the difference between the two plots? In the second chunk we have first plotted a figure that is a fair bit larger (9 inches wide) than that in the first chunk. Next we have down-sized it in the final output, using the out.width option (where we need to use a size metric recognized by the output format, in this case \"560px\" which works for HTML). Have you noticed the first chunk? ```{r setup, include = FALSE} knitr::opts_chunk$set(echo = TRUE) ``` In this way we can set global chunk options, i.e. defaults for all chunks. In this example, echo will always be set to TRUE , unless otherwise specified in individual chunks. Tip For more chunk options, have a look at page 2-3 of this reference . It is also possible to create different types of interactive plots using R Markdown. You can see some examples of this here . If you want to try it out you can add the following code chunk to your document: ```{r} library(networkD3) data(MisLinks, MisNodes) forceNetwork(Links = MisLinks, Nodes = MisNodes, Source = \"source\", Target = \"target\", Value = \"value\", NodeID = \"name\", Group = \"group\", opacity = 0.4) ``` Quick recap In this section you learned how to include code chunks and how to use chunk options to control how the output (code, results and figures) is displayed.","title":"Code chunks"},{"location":"rmarkdown/r-markdown-4-the-yaml-header/","text":"Last but not least, we have the YAML header. Here is where you configure general settings for the final output file, and a few other things. The settings are written in YAML format in the form key: value . Nested settings or sub-settings are indented with spaces. In the template R Markdown document you can see that html_document is nested under output , and in turn, toc is nested under html_document since it is a setting for the HTML output. The table of contents (TOC) is automatically compiled from the section headers (marked by #). Add a subsection header somewhere in your document using three ### . Knit and look at how the table of contents is structured. Now set toc: false and knit again. What happened? A setting that works for HTML output is toc_float: true . Add that to your document (same indentation level as toc: true ) and knit. What happened? In the same way, add the option number_sections: true . What happened? Do you think it looks weird with sections numbered with 0, e.g. 0.1? That is because the document does not contain any level-1-header. Add a header using only one # at the top of the document, just after the setup chunk. Knit and see what happens! We can also set parameters in the YAML header. These are either character strings, numerical values, or logicals, and they can be used in the R code in the code chunks. Let's try it out: Add two parameters, data and color , to the YAML header. It should now look something like this: --- title: \"Untitled\" output: html_document: toc: true toc_float: true number_sections: true params: data: cars color: blue --- So now we have two parameters that we can use in the code! Modify the pressure code chunk so that it looks like this: ```{r pressure, fig.width = 6, fig.height = 4} plot(get(params$data), col = params$color) ``` This will plot the dataset cars using the color blue , based on the parameters we set in the YAML header. Knit and see what happens! Later, we will learn how to set parameters using an external command. We have up until now mainly been using html_document as an output format. There are however a range of different available formats to choose between. What is important to know, is that not all chunk settings work for all output formats (this mainly regards settings related to rendering plots and figures), and some YAML settings are specific for the given output format chosen. Take a look at this gallery of R Markdown documents to see what different kinds of output formats are possible to generate. Take a look at the last page of this reference for a list of YAML header options, and what output formats they are available for. Quick recap In this section you learned how to set document-wide settings in the YAML header, including document output type and user defined parameters.","title":"The YAML header"},{"location":"rmarkdown/r-markdown-5-rendering/","text":"You can render (sometimes called \"knitting\") reports in several different ways: Pressing the Knit button in RStudio (as we have done this far) Running the R command render : to Knit the file my_file.Rmd run rmarkdown::render(\"my_file.Rmd\") in the R console. Running from the command line: R -e 'rmarkdown::render(\"my_file.Rmd\")' Using the render command, we can also set YAML header options and change defaults ( i.e. override those specified in the R Markdown document itself). Here are a few useful arguments (see ?rmarkdown::render for a full list): output_format : change output format, e.g. html_document or pdf_document output_file and output_dir : change directory and file name of the generated report file (defaults to the same name and directory as the .Rmd file) params : change parameter defaults. Note that only parameters already listed in the YAML header can be set, no new parameters can be defined Try to use the render command to knit your template R Markdown document and set the two parameters data and color . Hint: the params argument should be a list, e.g. : rmarkdown::render(\"my_file.Rmd\", params = list(data = \"cars\", color = \"green\")) You might already have noticed the various ways in which you can run code chunks directly in RStudio: Place the cursor on an R command and press CTRL + Enter (Windows) or Cmd + Enter (Mac) to run that line in R. Select several R command lines and use the same keyboard shortcut as above to run those lines. To the right in each chunk there are two buttons; one for running the code in all chunks above the current chunk and one for running the code in the current chunk (depending on your layout, otherwise you can find the options in the Run drop-down). You can easily insert an empty chunk in your Rmd document in RStudio by pressing Code --> Insert Chunk in the menu. Depending on your settings, the output of the chunk code will be displayed inline in the Rmd document, or in RStudio's Console and Plot panels. To customize this setting, press the cog-wheel next to the Knit button and select either \"Chunk Output Inline\" or \"Chunk Output in Console\". Additionally, in the top right in the editor panel in RStudio there is a button to toggle the document outline. By making that visible you can click and jump between sections (headers and named code chunks) in your R Markdown document. Quick recap In this section you learned how to render R Markdown documents into HTML documents using several different methods.","title":"Rendering"},{"location":"rmarkdown/r-markdown-6-the-mrsa-case-study/","text":"As you might remember from the intro , we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus ). In this exercise we will use R Markdown to make a report in form of a Supplementary Material HTML based on the outputs from the Snakemake tutorial . Among the benefits of having the supplementary material (or even the full manuscript) in R Markdown format are: It is fully transparent how the text, tables and figures were produced. If you get reviewer comments, or realize you've made a mistake somewhere, you can easily update your code and regenerate the document with the push of a button. By making report generation part of your workflow early on in a project, much of the eventual manuscript \"writes itself\". You no longer first have to finish the research part and then start creating the tables and figures for the paper. Before you start: Make sure that your working directory in R is workshop-reproducible-research/tutorials/rmarkdown in the course directory (Session > Set Working Directory). Open the file code/supplementary_material.Rmd . Note In this tutorial we have used Conda to install all the R packages we need, so that you get to practice how you can actually do this in projects of your own. You can, however, install things using install.packages() or BiocManager::install() as well, even though this makes it both less reproducible and more complicated in most cases. Overview # Let's start by taking a look at the YAML header at the top of the file. The parameters correspond to files (and sample IDs) that are generated by the MRSA analysis workflow (see the Snakemake tutorial ) and contain results that we want to include in the supplementary material document. We've also specified that we want to render to HTML. --- title: \"Supplementary Materials\" output: html_document params: counts_file: \"results/tables/counts.tsv\" multiqc_file: \"intermediate/multiqc_general_stats.txt\" summary_file: \"results/tables/counts.tsv.summary\" rulegraph_file: \"results/rulegraph.png\" SRR_IDs: \"SRR935090 SRR935091 SRR935092\" GSM_IDs: \"GSM1186459 GSM1186460 GSM1186461\" --- From a reproducibility perspective it definitely makes sense to include information about who authored the document and the date it was generated. Add the two lines below to the YAML header. Note that we can include inline R code by using `r some_code` . author: John Doe, Joan Dough, Jan Doh, Dyon Do date: \"`r format(Sys.time(), '%d %B, %Y')`\" Tip Make it a practice to keep track of all input files and add them as parameters rather than hard-coding them later in the R code. Next, take a look at the dependencies , read_params , and read_data chunks. They 1) load the required packages, 2) read the parameters and store them in R objects to be used later in the code, and 3) read the data in the counts file, the multiqc file, as well as fetch meta data from GEO. These chunks are provided as is, and you do not need to edit them. Below these chunks there is some markdown text that contains the Supplementary Methods section. Note the use of section headers using # and ## . Then there is a Supplementary Tables and Figures section. This contains four code chunks, each for a specific table or figure. Have a quick look at the code and see if you can figure out what it does, but don't worry if you can't understand everything. Finally, there is a Reproducibility section which describes how the results in the report can be reproduced. The session_info chunk prints information regarding R version and which packages and versions that are used. We highly encourage you to include this chunk in all your R Markdown reports: it's an effortless way to increase reproducibility. Rendering options and paths # Now that you have had a look at the R Markdown document, it is time to Knit! We will do this from the R terminal (rather than pressing Knit ). rmarkdown::render(\"code/supplementary_material.Rmd\", output_dir = \"results\") The reason for this is that we can then redirect the output html file to be saved in the results/ directory. Normally, while rendering, R code in the Rmd file will be executed using the directory of the Rmd file as working directory ( rmarkdown/code in this case). However, it is good practice to write all code as if it would be executed from the project root directory ( rmarkdown/ in this case). For instance, you can see that we have specified the files in params with relative paths from the project root directory. To set a different directory as working directory for all chunks one modifies the knit options like this: knitr::opts_knit$set(root.dir = '../') Here we set the working directory to the parent directory of the Rmd file ( ../ ), in other words, the project root. Use this rather than setwd() while working with Rmd files. Take a look at the output. You should find the html file in the results directory. Formatting tables and figures # You will probably get a good idea of the contents of the file, but the tables look weird and the figures could be better formatted. Let's start by adjusting the figures! Locate the Setup chunk. Here, we have already set echo = FALSE . Let's add some default figure options: fig.height = 6, fig.width = 6, fig.align = 'center' . This will make the figures slightly smaller than default and center them. Knit again, using the same R command as above. Do you notice any difference? Better, but still not perfect! Let's improve the tables! We have not talked about tables before. There are several options to print tables, here we will use the kable function which is part of the knitr package. Go to the Sample info chunk. Replace the last line, sample_info , with: knitr::kable(sample_info) Knit again and look at the result. You should see a formatted table. The column names can be improved, and we could use a table legend. Change to use the following: knitr::kable(sample_info, caption = \"Sample info\", col.names = c(\"SRR\", \"GEO\", \"Strain\", \"Treatment\")) Knit and check the result. Try to fix the table in the QC statistics chunk in the same manner. The column names are fine here so no need to change them, but add a table legend: \"QC stats from FastQC\". Knit and check your results. Let's move on to the figures! Go to the Counts barplot chunk. To add a figure legend we have to use a chunk option (so not in the same way as for tables). Add the chunk option: fig.cap = \"Counting statistics per sample, in terms of read counts for genes and reads not counted for various reasons.\" Knit and check the outcome! Next, add a figure legend to the figure in the gene-heatmap chunk. Here we can try out the possibility to add R code to generate the legend: fig.cap = paste0(\"Expression (log-10 counts) of genes with at least \", max_cutoff, \" counts in one sample and a CV>\", cv_cutoff, \".\") This will use the cv_cutoff and max_cutoff variables to ensure that the figure legend gives the same information as was used to generate the plot. Note that figure legends are generated after the corresponding code chunk is evaluated. This means we can use objects defined in the code chunk in the legend. Knit and have a look at the results. The heatmap still looks a bit odd. Let's play with the fig.height and out.height options, like we did above, to scale the figure in a more appropriate way. Add this to the chunk options: fig.height = 10, out.height = \"22cm\" . Knit and check the results. Does it look better now? Now let's add a third figure! This time we will not plot a figure in R, but use an available image file showing the structure of the Snakemake workflow used to generate the inputs for this report. Add a new chunk at the end of the Supplementary Tables and Figures section containing this code: knitr::include_graphics(normalizePath(rulegraph_file)) Knitr paths Just like for R Markdown paths in general, Knitr needs everything to be relative to the directory in which the .Rmd file is situated. Just like setting the root.dir chunk option can help with this for the overall rendering, the normalizePath() function is needed when using include_graphics() for adding images with Knitr. Also, add the chunk options: fig.cap = \"A rule graph showing the different steps of the bioinformatic analysis that is included in the Snakemake workflow.\" and: out.height = \"11cm\" Knit and check the results. Note It is definitely possible to render R Markdown documents as part of a Snakemake or Nextflow workflow. This is something we do for the final version of the MRSA project (in the Containers tutorial). In such cases it is advisable to manage the installation of R and required R packages through your conda environment file and use the rmarkdown::render() command from the shell section of your Snakemake rule or Nexflow process. Quick recap In this section you learned some additional details for making nice R Markdown reports in a reproducible research project setting, including setting the root directory, adding tables as well as setting figure and table captions.","title":"The MRSA case study"},{"location":"rmarkdown/r-markdown-6-the-mrsa-case-study/#overview","text":"Let's start by taking a look at the YAML header at the top of the file. The parameters correspond to files (and sample IDs) that are generated by the MRSA analysis workflow (see the Snakemake tutorial ) and contain results that we want to include in the supplementary material document. We've also specified that we want to render to HTML. --- title: \"Supplementary Materials\" output: html_document params: counts_file: \"results/tables/counts.tsv\" multiqc_file: \"intermediate/multiqc_general_stats.txt\" summary_file: \"results/tables/counts.tsv.summary\" rulegraph_file: \"results/rulegraph.png\" SRR_IDs: \"SRR935090 SRR935091 SRR935092\" GSM_IDs: \"GSM1186459 GSM1186460 GSM1186461\" --- From a reproducibility perspective it definitely makes sense to include information about who authored the document and the date it was generated. Add the two lines below to the YAML header. Note that we can include inline R code by using `r some_code` . author: John Doe, Joan Dough, Jan Doh, Dyon Do date: \"`r format(Sys.time(), '%d %B, %Y')`\" Tip Make it a practice to keep track of all input files and add them as parameters rather than hard-coding them later in the R code. Next, take a look at the dependencies , read_params , and read_data chunks. They 1) load the required packages, 2) read the parameters and store them in R objects to be used later in the code, and 3) read the data in the counts file, the multiqc file, as well as fetch meta data from GEO. These chunks are provided as is, and you do not need to edit them. Below these chunks there is some markdown text that contains the Supplementary Methods section. Note the use of section headers using # and ## . Then there is a Supplementary Tables and Figures section. This contains four code chunks, each for a specific table or figure. Have a quick look at the code and see if you can figure out what it does, but don't worry if you can't understand everything. Finally, there is a Reproducibility section which describes how the results in the report can be reproduced. The session_info chunk prints information regarding R version and which packages and versions that are used. We highly encourage you to include this chunk in all your R Markdown reports: it's an effortless way to increase reproducibility.","title":"Overview"},{"location":"rmarkdown/r-markdown-6-the-mrsa-case-study/#rendering-options-and-paths","text":"Now that you have had a look at the R Markdown document, it is time to Knit! We will do this from the R terminal (rather than pressing Knit ). rmarkdown::render(\"code/supplementary_material.Rmd\", output_dir = \"results\") The reason for this is that we can then redirect the output html file to be saved in the results/ directory. Normally, while rendering, R code in the Rmd file will be executed using the directory of the Rmd file as working directory ( rmarkdown/code in this case). However, it is good practice to write all code as if it would be executed from the project root directory ( rmarkdown/ in this case). For instance, you can see that we have specified the files in params with relative paths from the project root directory. To set a different directory as working directory for all chunks one modifies the knit options like this: knitr::opts_knit$set(root.dir = '../') Here we set the working directory to the parent directory of the Rmd file ( ../ ), in other words, the project root. Use this rather than setwd() while working with Rmd files. Take a look at the output. You should find the html file in the results directory.","title":"Rendering options and paths"},{"location":"rmarkdown/r-markdown-6-the-mrsa-case-study/#formatting-tables-and-figures","text":"You will probably get a good idea of the contents of the file, but the tables look weird and the figures could be better formatted. Let's start by adjusting the figures! Locate the Setup chunk. Here, we have already set echo = FALSE . Let's add some default figure options: fig.height = 6, fig.width = 6, fig.align = 'center' . This will make the figures slightly smaller than default and center them. Knit again, using the same R command as above. Do you notice any difference? Better, but still not perfect! Let's improve the tables! We have not talked about tables before. There are several options to print tables, here we will use the kable function which is part of the knitr package. Go to the Sample info chunk. Replace the last line, sample_info , with: knitr::kable(sample_info) Knit again and look at the result. You should see a formatted table. The column names can be improved, and we could use a table legend. Change to use the following: knitr::kable(sample_info, caption = \"Sample info\", col.names = c(\"SRR\", \"GEO\", \"Strain\", \"Treatment\")) Knit and check the result. Try to fix the table in the QC statistics chunk in the same manner. The column names are fine here so no need to change them, but add a table legend: \"QC stats from FastQC\". Knit and check your results. Let's move on to the figures! Go to the Counts barplot chunk. To add a figure legend we have to use a chunk option (so not in the same way as for tables). Add the chunk option: fig.cap = \"Counting statistics per sample, in terms of read counts for genes and reads not counted for various reasons.\" Knit and check the outcome! Next, add a figure legend to the figure in the gene-heatmap chunk. Here we can try out the possibility to add R code to generate the legend: fig.cap = paste0(\"Expression (log-10 counts) of genes with at least \", max_cutoff, \" counts in one sample and a CV>\", cv_cutoff, \".\") This will use the cv_cutoff and max_cutoff variables to ensure that the figure legend gives the same information as was used to generate the plot. Note that figure legends are generated after the corresponding code chunk is evaluated. This means we can use objects defined in the code chunk in the legend. Knit and have a look at the results. The heatmap still looks a bit odd. Let's play with the fig.height and out.height options, like we did above, to scale the figure in a more appropriate way. Add this to the chunk options: fig.height = 10, out.height = \"22cm\" . Knit and check the results. Does it look better now? Now let's add a third figure! This time we will not plot a figure in R, but use an available image file showing the structure of the Snakemake workflow used to generate the inputs for this report. Add a new chunk at the end of the Supplementary Tables and Figures section containing this code: knitr::include_graphics(normalizePath(rulegraph_file)) Knitr paths Just like for R Markdown paths in general, Knitr needs everything to be relative to the directory in which the .Rmd file is situated. Just like setting the root.dir chunk option can help with this for the overall rendering, the normalizePath() function is needed when using include_graphics() for adding images with Knitr. Also, add the chunk options: fig.cap = \"A rule graph showing the different steps of the bioinformatic analysis that is included in the Snakemake workflow.\" and: out.height = \"11cm\" Knit and check the results. Note It is definitely possible to render R Markdown documents as part of a Snakemake or Nextflow workflow. This is something we do for the final version of the MRSA project (in the Containers tutorial). In such cases it is advisable to manage the installation of R and required R packages through your conda environment file and use the rmarkdown::render() command from the shell section of your Snakemake rule or Nexflow process. Quick recap In this section you learned some additional details for making nice R Markdown reports in a reproducible research project setting, including setting the root directory, adding tables as well as setting figure and table captions.","title":"Formatting tables and figures"},{"location":"rmarkdown/r-markdown-7-extra-material/","text":"While the tutorial teaches you all the basics of using R Markdown, there is much more you can do with it, if you want to! Here we cover some extra material if you're curious to learn more, but we don't consider this to be a main part of the course. If you want to read more about R Markdown in general, here are some useful resources: A nice \"Get Started\" section, as a complement to this tutorial, is available at RStudio.com . R Markdown cheat sheet (also available from Help --> Cheatsheets in RStudio) R Markdown reference guide (also available from Help --> Cheatsheets in RStudio) A nicer session info # While the default sessionInfo() command is highly useful for ending your report with all the packages and their respective versions, it can be a bit hard to read. There is, however, another version of the same command, which you can find in the devtools package. By combining this command with the markup result format you can get a more nicely formatted session information: ```{r Session info, echo = FALSE, results = \"markup\"} devtools::session_info() ``` R Markdown and workflows # Working with R Markdown in the context of a Snakemake or Nextflow workflow is something that is highly useful for reproducibility and quite easy to get going with. An important thing that you'll have to manage a bit more than usual is, however, the working directory of the R Markdown document, which is something you can do with parameters, the root_directory , output_dir and output_file special variables. The following is a simple example of how you can write a Snakemake rule for R Markdown: rule report: input: report = \"report.Rmd\" output: html = \"results/report.html\" params: outdir = \"results\" shell: \"\"\" Rscript -e 'parameters <- list(root_directory = getwd(), parameter_a = \"first\", parameter_b = 10); rmarkdown::render(\"{input.report}\", params = parameters, output_dir = \"{params.outdir}\", output_file = \"{output.html}\")' \"\"\" Doing it for Nextflow would look almost the same, except using Nextflow syntax and variables. R Markdown and other languages # While R is the default and original language for any R Markdown document it supports several others, including Python, bash, SQL, JavaScript, to name a few. This means that you can actually get the reproducibility of R Markdown documents when coding in languages other than just R, if your language is one of the supported ones . Two of the most important languages are Python and bash. While bash is supported out-of-the-box directly and only requires you to specify bash instead of r in the start of the code chunk, Python will additionally require you to have installed the reticulate package. Not only does this allow you to code in Python directly in your in R Markdown document, but the objects and variables you use in one language/chunk will actually be available for the other language! You can read more about the R Markdown Python engine here . R Markdown and LaTeX # This tutorial has been using HTML as the output format, as this is the most common format that many data analyses are using. Some reasons for this include not having to think about a page-layout (which is especially useful for documents with many figures/plots, which is common for data analysis documents), simplified creation ( i.e. not having to think about special LaTeX commands for PDF output) and fewer dependencies. The PDF format has a lot going for it as well, however, such as being an end-point for journal articles, books and similar documents, as well as being much more powerful (meaning a steeper learning curve) than just HTML rendering: you can customise almost anything you can think of. Not that HTML output is lacking in options, it's just that LaTeX is more feature-rich. Let's take an example: font sizes. This is something that is quite hard to do on a per-chunk basis in HTML, but easy in LaTeX. You can change the font size of all HTML chunks by using a custom CSS template, for instance, but in LaTeX you can just set the font size to something before and after a chunk, like so: \\footnotesize ```{r Count to 3} seq_len(3) ``` \\normalsize You could also do automatic figure captions using the LaTeX engine, meaning you won't have to add \"Figure X\" to each caption manually. You can even have separate groups of figure captions, such as one group for main article figures and one group for supplementary figures - the same goes for tables, of course. R Markdown uses LaTeX behind the scenes to render PDF documents, but you miss some of the features that are inherent in LaTeX by going this route. There is, thankfully, a different file format that you can use that more explicitly merges R Markdown and all the functionality of LaTeX, called Sweave . Sweave allows you to use any LaTeX command you want outside of R code chunks, which is awesome for those of you who are already using LaTeX and want to combine it with R Markdown. These files use the .Rnw extension rather than .Rmd , with some additional changes, such as code chunks starting with <<>>= and ending with @ . There is simply a plethora of tweaks, variables and parameters that you can use for PDF rendering, but it can be quite overwhelming if you're just starting out. We recommend using HTML for most things, especially data analysis, but PDF certainly has its strengths - you could, for example, write a whole paper (and its supplementary material) in R Markdown with LaTeX only , without using Microsoft Word or anything else - it doesn't get much more reproducible than that! In fact, there are several publication-ready templates on a per-journal basis in the package rticles , which can greatly facilitate this process! Presentations in R Markdown # R Markdown is not only useful for data analysis reports and papers, but you can also create presentations with it! In fact, most of the presentations created for this course were done using R Markdown. A major difference between presentations in R Markdown and e.g. Microsoft PowerPoint is the same as between any Markdown document (or LaTeX, for that matter) and the more common Microsoft Word: the more usual Microsoft software is \"what you see is what you get\", while Markdown/LaTeX doesn't show you the actual output until you've rendered it. This difference is more pronounced when it comes to presentations, as they are more visually heavy. In essence, a R Markdown presentation works the same way as for a R Markdown report, except some different formatting and output specifications. There are a number of output formats you can use, but the one we've used for this course (for no other reason than that we like it) is Xaringan . You can install it from Conda ( r-xaringan ) like any other package and then specify the output format as xaringan::moon_reader in your YAML header. Slides are separated using three dashes ( --- ) while two dashes ( -- ) signify slide elements that should appear on-click. Here is a bare-bones example of a R Markdown presentation using Xaringan: --- title: \"A R Markdown presentation\" output: xaringan::moon_reader --- # R Markdown presentations You can write text like normal, including all the normal Markdown formatting such as *italics* or **bold**. -- Anything can be separated into on-click appearance using double dashes. ## Sub-headers work fine as well Just remember to separate your slides with three dashes! --- # Use code chunks just like normal This is especially useful for presentations of data analyses, since you don't have to have a separate R Markdown or script to create the tables/figures and then copy/paste them into a PowerPoint! ```{r, fig.height = 5, fig.width = 5, fig.align = \"center\"} data(cars) plot(cars) ``` Having said that, presentations is R Markdown can do most things that PowerPoint can do, but it'll take some more effort. Getting something to look like you want in a WYSIWYG-editor like PowerPoint is easier, since you're seeing the output as you're making it, but it'll take more experimentation in R Markdown. You can, however, automate a lot of things, such as by using CSS templates that apply to each slide (including things such as font styles, header, footers, and more) or like the above mentioned benefit of having both code and its results already in your presentation without having to muck about with copying and pasting figures and tables to a separate presentation. For inspiration, we suggest you go to the lectures/ directory of the course Git repository. You should also have a look at the official documentation of Xaringan (which is itself a R Markdown-based presentation), as well as its several alternatives . We find that using R Markdown for presentations does take about the same time or slightly more compared to PowerPoint once you're used to it, but there's a learning curve - as with everything else. Anything related to actual code and presenting results can be much quicker, however! A good exercise is to take one of the presentations you have given in the past (such as for a lab meeting, a journal club, etc. ) and try to recreate that with R Markdown. Which method of creating presentations you prefer is, ultimately, up to you and what the your current end-goal is for the presentation.","title":"Extra materiel"},{"location":"rmarkdown/r-markdown-7-extra-material/#a-nicer-session-info","text":"While the default sessionInfo() command is highly useful for ending your report with all the packages and their respective versions, it can be a bit hard to read. There is, however, another version of the same command, which you can find in the devtools package. By combining this command with the markup result format you can get a more nicely formatted session information: ```{r Session info, echo = FALSE, results = \"markup\"} devtools::session_info() ```","title":"A nicer session info"},{"location":"rmarkdown/r-markdown-7-extra-material/#r-markdown-and-workflows","text":"Working with R Markdown in the context of a Snakemake or Nextflow workflow is something that is highly useful for reproducibility and quite easy to get going with. An important thing that you'll have to manage a bit more than usual is, however, the working directory of the R Markdown document, which is something you can do with parameters, the root_directory , output_dir and output_file special variables. The following is a simple example of how you can write a Snakemake rule for R Markdown: rule report: input: report = \"report.Rmd\" output: html = \"results/report.html\" params: outdir = \"results\" shell: \"\"\" Rscript -e 'parameters <- list(root_directory = getwd(), parameter_a = \"first\", parameter_b = 10); rmarkdown::render(\"{input.report}\", params = parameters, output_dir = \"{params.outdir}\", output_file = \"{output.html}\")' \"\"\" Doing it for Nextflow would look almost the same, except using Nextflow syntax and variables.","title":"R Markdown and workflows"},{"location":"rmarkdown/r-markdown-7-extra-material/#r-markdown-and-other-languages","text":"While R is the default and original language for any R Markdown document it supports several others, including Python, bash, SQL, JavaScript, to name a few. This means that you can actually get the reproducibility of R Markdown documents when coding in languages other than just R, if your language is one of the supported ones . Two of the most important languages are Python and bash. While bash is supported out-of-the-box directly and only requires you to specify bash instead of r in the start of the code chunk, Python will additionally require you to have installed the reticulate package. Not only does this allow you to code in Python directly in your in R Markdown document, but the objects and variables you use in one language/chunk will actually be available for the other language! You can read more about the R Markdown Python engine here .","title":"R Markdown and other languages"},{"location":"rmarkdown/r-markdown-7-extra-material/#r-markdown-and-latex","text":"This tutorial has been using HTML as the output format, as this is the most common format that many data analyses are using. Some reasons for this include not having to think about a page-layout (which is especially useful for documents with many figures/plots, which is common for data analysis documents), simplified creation ( i.e. not having to think about special LaTeX commands for PDF output) and fewer dependencies. The PDF format has a lot going for it as well, however, such as being an end-point for journal articles, books and similar documents, as well as being much more powerful (meaning a steeper learning curve) than just HTML rendering: you can customise almost anything you can think of. Not that HTML output is lacking in options, it's just that LaTeX is more feature-rich. Let's take an example: font sizes. This is something that is quite hard to do on a per-chunk basis in HTML, but easy in LaTeX. You can change the font size of all HTML chunks by using a custom CSS template, for instance, but in LaTeX you can just set the font size to something before and after a chunk, like so: \\footnotesize ```{r Count to 3} seq_len(3) ``` \\normalsize You could also do automatic figure captions using the LaTeX engine, meaning you won't have to add \"Figure X\" to each caption manually. You can even have separate groups of figure captions, such as one group for main article figures and one group for supplementary figures - the same goes for tables, of course. R Markdown uses LaTeX behind the scenes to render PDF documents, but you miss some of the features that are inherent in LaTeX by going this route. There is, thankfully, a different file format that you can use that more explicitly merges R Markdown and all the functionality of LaTeX, called Sweave . Sweave allows you to use any LaTeX command you want outside of R code chunks, which is awesome for those of you who are already using LaTeX and want to combine it with R Markdown. These files use the .Rnw extension rather than .Rmd , with some additional changes, such as code chunks starting with <<>>= and ending with @ . There is simply a plethora of tweaks, variables and parameters that you can use for PDF rendering, but it can be quite overwhelming if you're just starting out. We recommend using HTML for most things, especially data analysis, but PDF certainly has its strengths - you could, for example, write a whole paper (and its supplementary material) in R Markdown with LaTeX only , without using Microsoft Word or anything else - it doesn't get much more reproducible than that! In fact, there are several publication-ready templates on a per-journal basis in the package rticles , which can greatly facilitate this process!","title":"R Markdown and LaTeX"},{"location":"rmarkdown/r-markdown-7-extra-material/#presentations-in-r-markdown","text":"R Markdown is not only useful for data analysis reports and papers, but you can also create presentations with it! In fact, most of the presentations created for this course were done using R Markdown. A major difference between presentations in R Markdown and e.g. Microsoft PowerPoint is the same as between any Markdown document (or LaTeX, for that matter) and the more common Microsoft Word: the more usual Microsoft software is \"what you see is what you get\", while Markdown/LaTeX doesn't show you the actual output until you've rendered it. This difference is more pronounced when it comes to presentations, as they are more visually heavy. In essence, a R Markdown presentation works the same way as for a R Markdown report, except some different formatting and output specifications. There are a number of output formats you can use, but the one we've used for this course (for no other reason than that we like it) is Xaringan . You can install it from Conda ( r-xaringan ) like any other package and then specify the output format as xaringan::moon_reader in your YAML header. Slides are separated using three dashes ( --- ) while two dashes ( -- ) signify slide elements that should appear on-click. Here is a bare-bones example of a R Markdown presentation using Xaringan: --- title: \"A R Markdown presentation\" output: xaringan::moon_reader --- # R Markdown presentations You can write text like normal, including all the normal Markdown formatting such as *italics* or **bold**. -- Anything can be separated into on-click appearance using double dashes. ## Sub-headers work fine as well Just remember to separate your slides with three dashes! --- # Use code chunks just like normal This is especially useful for presentations of data analyses, since you don't have to have a separate R Markdown or script to create the tables/figures and then copy/paste them into a PowerPoint! ```{r, fig.height = 5, fig.width = 5, fig.align = \"center\"} data(cars) plot(cars) ``` Having said that, presentations is R Markdown can do most things that PowerPoint can do, but it'll take some more effort. Getting something to look like you want in a WYSIWYG-editor like PowerPoint is easier, since you're seeing the output as you're making it, but it'll take more experimentation in R Markdown. You can, however, automate a lot of things, such as by using CSS templates that apply to each slide (including things such as font styles, header, footers, and more) or like the above mentioned benefit of having both code and its results already in your presentation without having to muck about with copying and pasting figures and tables to a separate presentation. For inspiration, we suggest you go to the lectures/ directory of the course Git repository. You should also have a look at the official documentation of Xaringan (which is itself a R Markdown-based presentation), as well as its several alternatives . We find that using R Markdown for presentations does take about the same time or slightly more compared to PowerPoint once you're used to it, but there's a learning curve - as with everything else. Anything related to actual code and presenting results can be much quicker, however! A good exercise is to take one of the presentations you have given in the past (such as for a lab meeting, a journal club, etc. ) and try to recreate that with R Markdown. Which method of creating presentations you prefer is, ultimately, up to you and what the your current end-goal is for the presentation.","title":"Presentations in R Markdown"},{"location":"snakemake/snakemake-1-introduction/","text":"A workflow management system (WfMS) is a piece of software that sets up, performs and monitors a defined sequence of computational tasks ( i.e. \"a workflow\"). Snakemake is a WfMS that was developed in the bioinformatics community, and as such it has a number of features that make it particularly well-suited for creating reproducible and scalable data analyses. First of all the language you use to formulate your workflows is based on Python, which is a language with strong standing in academia. However, users are not required to know how to code in Python to work efficiently with Snakemake. Workflows can easily be scaled from your desktop to server, cluster, grid or cloud environments. This makes it possible to develop a workflow on your laptop, maybe using only a small subset of your data, and then run the real analysis on a cluster. Snakemake also has several features for defining the environment with which each task is carried out. This is important in bioinformatics, where workflows often involve running a large number of small third-party tools. Snakemake is primarily intended to work on files (rather than for example streams, reading/writing from databases or passing variables in memory). This fits well with many fields of bioinformatics, notably next-generation sequencing, that often involve computationally expensive operations on large files. It's also a good fit for a scientific research setting, where the exact specifications of the final workflow aren't always known at the beginning of a project. Lastly, a WfMS is a very important tool for making your analyses reproducible. By keeping track of when each file was generated, and by which operation, it is possible to ensure that there is a consistent \"paper trail\" from raw data to final results. Snakemake also has features that allow you to package and distribute the workflow, and any files it involves, once it's done. This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already, then open up a terminal and go to workshop-reproducible-research/tutorials/snakemake and activate your snakemake-env Conda environment.","title":"Introduction"},{"location":"snakemake/snakemake-10-generalizing-workflows/","text":"It's generally a good idea to separate project-specific parameters from the actual implementation of the workflow. If we want to move all project-specific information to config.yml , and let the Snakefile be a more general RNA-seq analysis workflow, we need the config file to: Specify which samples to run. Specify which genome to align to and where to download its sequence and annotation files. (Any other parameters we might need to make it into a general workflow, e.g. to support both paired-end and single-read sequencing) Note Putting all configuration in config.yml will break the generate_rulegraph rule. You can fix it either by replacing --config max_reads=0 with --configfile=config.yml in the shell command of that rule in the Snakefile, or by adding configfile: \"config.yml\" to the top of the Snakefile (as mentioned in a previous tip). The first point is straightforward; rather than using SAMPLES = [\"...\"] in the Snakefile we define it as a parameter in config.yml . You can either add it as a list similar to the way it was expressed before by adding SAMPLES: [\"...\"] to config.yml , or you can use this yaml notation: sample_ids: - SRR935090 - SRR935091 - SRR935092 You also have to change the workflow to reference config[\"sample_ids\"] (if using the latter example) instead of SAMPLES , as in: expand(\"intermediate/{sample_id}_fastqc.zip\", sample_id = config[\"sample_ids\"]) Do a dry-run afterwards to make sure that everything works as expected. The second point is trickier. Writing workflows in Snakemake is quite straightforward when the logic of the workflow is reflected in the file names, i.e. my_sample.trimmed.deduplicated.sorted.fastq , but that isn't always the case. In our case we have the FTP paths to the genome sequence and annotation where the naming doesn't quite fit with the rest of the workflow. The easiest solution is probably to make three parameters to hold these values, say genome_id , genome_fasta_path and genome_gff_path , but we will go for a somewhat more complex but very useful alternative. We want to construct a dictionary where something that will be a wildcard in the workflow is the key and the troublesome name is the value. An example might make this clearer (this is also in config.yml in the finished version of the workflow under tutorials/git/ ). This is a nested dictionary where \"genomes\" is a key with another dictionary as value, which in turn has genome ids as keys and so on. The idea is that we have a wildcard in the workflow that takes the id of a genome as value (either \"NCTC8325\" or \"ST398\" in this case). The fasta and gff3 paths can then be retrieved based on the value of the wildcard. genomes: NCTC8325: fasta: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz gff3: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/gff3/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.37.gff3.gz ST398: fasta: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection//staphylococcus_aureus_subsp_aureus_st398/dna/Staphylococcus_aureus_subsp_aureus_st398.ASM958v1.dna.toplevel.fa.gz gff3: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/gff3/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_st398//Staphylococcus_aureus_subsp_aureus_st398.ASM958v1.37.gff3.gz Go ahead and add the section above to config.yml . Let's now look at how to do the mapping from genome id to fasta path in the rule get_genome_fasta . This is how the rule currently looks (if you have added the log section as previously described). rule get_genome_fasta: \"\"\" Retrieve the sequence in fasta format for a genome. \"\"\" output: \"data/raw_external/NCTC8325.fa.gz\" log: \"results/logs/get_genome_fasta/NCTC8325.log\" shell: \"\"\" wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz -O {output} -o {log} \"\"\" We don't want the hardcoded genome id NCTC8325 , so replace that with a wildcard, say {genome_id} (remember to add the wildcard to the log: directive as well). Also change in index_genome to use a wildcard rather than a hardcoded genome id. Here you will run into a complication if you have followed the previous instructions and use the expand() expression. We want the list to expand to [\"intermediate/{genome_id}.1.bt2\", \"intermediate/{genome_id}.2.bt2\", ...] , i.e. only expanding the wildcard referring to the bowtie2 index. To keep the genome_id wildcard from being expanded we have to \"mask\" it with double curly brackets: {{genome_id}} . In addition, we need to replace the hardcoded intermediate/NCTC8325 in the shell directive of the rule with the genome id wildcard. Inside the shell directive the wildcard object is accessed with this syntax: {wildcards.genome_id} , so the bowtie2-build command should be: bowtie2-build tempfile intermediate/{wildcards.genome_id} > {log} We now need to supply the remote paths to the fasta and gff files for a given genome id. Because we've added this information to the config file we just need to pass it to the rule in some way. Take a look at the code and get_genome_fasta rule below. Here we have defined a function called get_fasta_path which takes the wildcards object as its only argument. This object allows access to the wildcards values via attributes (here wildcards.genome_id ). The function will then look in the nested config dictionary and return the value of the fasta path for the key wildcards.genome_id . In the rule this path is stored in the fasta_path param value and is made available to wget in the shell directive. def get_fasta_path(wildcards): return config[\"genomes\"][wildcards.genome_id][\"fasta\"] rule get_genome_fasta: \"\"\" Retrieve the sequence in fasta format for a genome. \"\"\" output: \"data/raw_external/{genome_id}.fa.gz\" log: \"results/logs/get_genome_fasta/{genome_id}.log\" params: fasta_path = get_fasta_path shell: \"\"\" wget {params.fasta_path} -O {output} -o {log} \"\"\" Note that this will only work if the {genome_id} wildcard can be resolved to something defined in the config (currently NCTC8325 or ST398 ). If you try to generate a fasta file for a genome id not defined in the config Snakemake will complain, even at the dry-run stage. Now change the get_genome_gff3 rule in a similar manner. The rules get_genome_fasta , get_genome_gff3 and index_genome can now download and index any genome as long as we provide valid links in the config file. However, we need to define somewhere which genome id we actually want to use when running the workflow. This needs to be done both in align_to_genome and generate_count_table . Do this by introducing a parameter in config.yml called \"genome_id\" (you can set it to either NCTC8325 or ST398 ). Now we can resolve the genome_id wildcard from the config. See below for an example for align_to_genome . Here the substr wildcard gets expanded from a list while genome_id gets expanded from the config file. input: index = expand(\"intermediate/{genome_id}.{substr}.bt2\", genome_id = config[\"genome_id\"], substr = [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"]) Also change the hardcoded genome id in the generate_count_table input in a similar manner. In general, we want the rules as far downstream as possible in the workflow to be the ones that determine what the wildcards should resolve to. In our case this is align_to_genome and generate_count_table . You can think of it like the rule that really \"needs\" the file asks for it, and then it's up to Snakemake to determine how it can use all the available rules to generate it. Here the align_to_genome rule says \"I need this genome index to align my sample to\" and then it's up to Snakemake to determine how to download and build the index. One last thing is to change the hardcoded NCTC8325 in the shell: directive of align_to_genome . Bowtie2 expects the index name supplied with the -x flag to be without the \".*.bt2\" suffix so we can't use -x {input.index} . Instead we'll insert the genome_id directly from the config like this: shell: \"\"\" bowtie2 -x intermediate/{config[genome_id]} -U {input[0]} > {output} 2>{log} \"\"\" Summary Well done! You now have a complete Snakemake workflow with a number of excellent features: A general RNA-seq pipeline which can easily be reused between projects, thanks to clear separation between code and settings. Great traceability due to logs and summary tables. Clearly defined the environment for the workflow using Conda. The workflow is neat and free from temporary files due to using temp() and shadow . A logical directory structure which makes it easy to separate raw data, intermediate files, and results. A project set up in a way that makes it very easy to distribute and reproduce either via Git, Snakemake's --archive option or a Docker image. Quick recap In this section we've learned: How to generalize a Snakemake workflow.","title":"Generalizing workflows"},{"location":"snakemake/snakemake-11-extra-material/","text":"If you want to read more about Snakemake in general you can find several resources here: The Snakemake documentation is available on readthedocs . Here is another (quite in-depth) tutorial . If you have questions, check out stack overflow . Using containers in Snakemake # Snakemake also supports defining a Singularity or Docker container for each rule (you will have time to work on the Containers tutorial later during the course). Analogous to using a rule-specific Conda environment, specify container: \"docker://some-account/rule-specific-image\" in the rule definition. Instead of a link to a container image, it is also possible to provide the path to a *.sif file (= a Singularity file). When executing Snakemake, add the --use-singularity flag to the command line. For the given rule, a Singularity container will then be created from the image or Singularity file that is provided in the rule definition on the fly by Snakemake and the rule will be run in this container. You can find pre-made Singularity or Docker images for many tools on https://biocontainers.pro/ (bioinformatics-specific) or on https://hub.docker.com/ . Here is an example for a rule and its execution: rule align_to_genome: \"\"\" Align a fastq file to a genome index using Bowtie 2. \"\"\" input: \"data/raw_internal/{sample_id}.fastq.gz\", \"intermediate/NCTC8325.1.bt2\", \"intermediate/NCTC8325.2.bt2\", \"intermediate/NCTC8325.3.bt2\", \"intermediate/NCTC8325.4.bt2\", \"intermediate/NCTC8325.rev.1.bt2\", \"intermediate/NCTC8325.rev.2.bt2\" output: \"intermediate/{sample_id,\\w+}.bam\" container: \"docker://quay.io/biocontainers/bowtie2:2.3.4.1--py35h2d50403_1\" shell: \"\"\" bowtie2 -x intermediate/NCTC8325 -U {input[0]} > {output} \"\"\" Start your Snakemake workflow with the following command: snakemake --use-singularity Feel free to modify the MRSA workflow according to this example. As Singularity is a container software that was developed for HPC clusters, and for example the Mac version is still a beta version, it might not work to run your updated Snakemake workflow with Singularity locally on your computer. In the next section we explain how you can run Snakemake workflows on UPPMAX where Singularity is pre-installed. Running Snakemake workflows on UPPMAX # There are several options to execute Snakemake workflows on UPPMAX (a HPC cluster with the SLURM workload manager). In any case, we highly recommend to use a session manager like tmux or screen so that you can run your workflow in a session in the background while doing other things on the cluster or even logging out of the cluster. Run your workflow in an interactive job # For short workflows with only a few rules that need the same compute resources in terms of CPU (cores), you can start an interactive job (in your tmux or screen session) and run your Snakemake workflow as you would do that on your local machine. Make sure to give your interactive job enough time to finish running all rules of your Snakemake workflow. Cluster configuration # For workflows with long run times and/or where each rule requires different compute resources, Snakemake can be configured to automatically send each rule as a job to the SLURM queue and to track the status of each job. The relevant parameters for such a cluster configuration are --cluster and --cluster-config , in combination with a cluster.yaml file that specifies default and rule-specific compute resources and your compute account details. Here is an example for a cluster.yaml file: # cluster.yaml - cluster configuration file __default__: account: # fill in your project compute account ID partition: core time: 01:00:00 ntasks: 1 cpus-per-task: 1 ### rule-specific resources trimming: time: 01-00:00:00 mapping: time: 01-00:00:00 cpus-per-task: 16 Start your Snakemake workflow in a tmux or screen session with the following command: snakemake -j 10 \\ --cluster-config cluster.yaml \\ --cluster \"sbatch \\ -A {cluster.account} \\ -p {cluster.partition} \\ -t {cluster.time} \\ --ntasks {cluster.ntasks} \\ --cpus-per-task {cluster.cpus-per-task}\" The additional parameter -j specifies the number of jobs that Snakemake is allowed to send to SLURM at the same time. SLURM Profile # The cluster configuration is actually marked as \"deprecated\" but still exists side-by-side with the thought to be replacement: profiles . Snakemake profiles can be used to define several options, allowing you to quickly adapt a workflow to different use-cases or to different environments. One such convenient profile is the SLURM profile developed to make a workflow make efficient use of the SLURM workload manager that is used e.g. on Uppmax. The SLURM Profile needs to be set up with the software cookiecutter which you can install with conda: conda install -c conda-forge cookiecutter . During the setup of the profile you will be asked for several values for your Profile. To configure the profile to use your account id see Example 1: project setup to use specific slurm account at the profile repository. Rule-specific resources can be defined in each rule via the resources: directive, for example: rule align_to_genome: input: \"{genome_id}.bt2\", \"{sample}.fastq.gz\" output: \"{sample}.bam\" resources: runtime = 360 threads: 10 shell: \"\"\" aligner -t {threads} -i {input[1]} -x {input[0]} > {output} \"\"\" Any rule for which runtime is specified in the resources directive will be submitted as one job to the SLURM queue with runtime as the allocated time. Similarly, the number specified in the threads directive will be used as the number of allocated cores. With this setup you can start the workflow with your SLURM Profile as follows from within a tmux or screen session: snakemake -j 10 --profile your_profile_name","title":"Extra materiel"},{"location":"snakemake/snakemake-11-extra-material/#using-containers-in-snakemake","text":"Snakemake also supports defining a Singularity or Docker container for each rule (you will have time to work on the Containers tutorial later during the course). Analogous to using a rule-specific Conda environment, specify container: \"docker://some-account/rule-specific-image\" in the rule definition. Instead of a link to a container image, it is also possible to provide the path to a *.sif file (= a Singularity file). When executing Snakemake, add the --use-singularity flag to the command line. For the given rule, a Singularity container will then be created from the image or Singularity file that is provided in the rule definition on the fly by Snakemake and the rule will be run in this container. You can find pre-made Singularity or Docker images for many tools on https://biocontainers.pro/ (bioinformatics-specific) or on https://hub.docker.com/ . Here is an example for a rule and its execution: rule align_to_genome: \"\"\" Align a fastq file to a genome index using Bowtie 2. \"\"\" input: \"data/raw_internal/{sample_id}.fastq.gz\", \"intermediate/NCTC8325.1.bt2\", \"intermediate/NCTC8325.2.bt2\", \"intermediate/NCTC8325.3.bt2\", \"intermediate/NCTC8325.4.bt2\", \"intermediate/NCTC8325.rev.1.bt2\", \"intermediate/NCTC8325.rev.2.bt2\" output: \"intermediate/{sample_id,\\w+}.bam\" container: \"docker://quay.io/biocontainers/bowtie2:2.3.4.1--py35h2d50403_1\" shell: \"\"\" bowtie2 -x intermediate/NCTC8325 -U {input[0]} > {output} \"\"\" Start your Snakemake workflow with the following command: snakemake --use-singularity Feel free to modify the MRSA workflow according to this example. As Singularity is a container software that was developed for HPC clusters, and for example the Mac version is still a beta version, it might not work to run your updated Snakemake workflow with Singularity locally on your computer. In the next section we explain how you can run Snakemake workflows on UPPMAX where Singularity is pre-installed.","title":"Using containers in Snakemake"},{"location":"snakemake/snakemake-11-extra-material/#running-snakemake-workflows-on-uppmax","text":"There are several options to execute Snakemake workflows on UPPMAX (a HPC cluster with the SLURM workload manager). In any case, we highly recommend to use a session manager like tmux or screen so that you can run your workflow in a session in the background while doing other things on the cluster or even logging out of the cluster.","title":"Running Snakemake workflows on UPPMAX"},{"location":"snakemake/snakemake-11-extra-material/#run-your-workflow-in-an-interactive-job","text":"For short workflows with only a few rules that need the same compute resources in terms of CPU (cores), you can start an interactive job (in your tmux or screen session) and run your Snakemake workflow as you would do that on your local machine. Make sure to give your interactive job enough time to finish running all rules of your Snakemake workflow.","title":"Run your workflow in an interactive job"},{"location":"snakemake/snakemake-11-extra-material/#cluster-configuration","text":"For workflows with long run times and/or where each rule requires different compute resources, Snakemake can be configured to automatically send each rule as a job to the SLURM queue and to track the status of each job. The relevant parameters for such a cluster configuration are --cluster and --cluster-config , in combination with a cluster.yaml file that specifies default and rule-specific compute resources and your compute account details. Here is an example for a cluster.yaml file: # cluster.yaml - cluster configuration file __default__: account: # fill in your project compute account ID partition: core time: 01:00:00 ntasks: 1 cpus-per-task: 1 ### rule-specific resources trimming: time: 01-00:00:00 mapping: time: 01-00:00:00 cpus-per-task: 16 Start your Snakemake workflow in a tmux or screen session with the following command: snakemake -j 10 \\ --cluster-config cluster.yaml \\ --cluster \"sbatch \\ -A {cluster.account} \\ -p {cluster.partition} \\ -t {cluster.time} \\ --ntasks {cluster.ntasks} \\ --cpus-per-task {cluster.cpus-per-task}\" The additional parameter -j specifies the number of jobs that Snakemake is allowed to send to SLURM at the same time.","title":"Cluster configuration"},{"location":"snakemake/snakemake-11-extra-material/#slurm-profile","text":"The cluster configuration is actually marked as \"deprecated\" but still exists side-by-side with the thought to be replacement: profiles . Snakemake profiles can be used to define several options, allowing you to quickly adapt a workflow to different use-cases or to different environments. One such convenient profile is the SLURM profile developed to make a workflow make efficient use of the SLURM workload manager that is used e.g. on Uppmax. The SLURM Profile needs to be set up with the software cookiecutter which you can install with conda: conda install -c conda-forge cookiecutter . During the setup of the profile you will be asked for several values for your Profile. To configure the profile to use your account id see Example 1: project setup to use specific slurm account at the profile repository. Rule-specific resources can be defined in each rule via the resources: directive, for example: rule align_to_genome: input: \"{genome_id}.bt2\", \"{sample}.fastq.gz\" output: \"{sample}.bam\" resources: runtime = 360 threads: 10 shell: \"\"\" aligner -t {threads} -i {input[1]} -x {input[0]} > {output} \"\"\" Any rule for which runtime is specified in the resources directive will be submitted as one job to the SLURM queue with runtime as the allocated time. Similarly, the number specified in the threads directive will be used as the number of allocated cores. With this setup you can start the workflow with your SLURM Profile as follows from within a tmux or screen session: snakemake -j 10 --profile your_profile_name","title":"SLURM Profile"},{"location":"snakemake/snakemake-2-the-basics/","text":"In this part of the tutorial we will create a very simple workflow from scratch, in order to show the fundamentals of how Snakemake works. The workflow will take two files as inputs, a.txt and b.txt , and the purpose is to convert the text in the files to upper case and then to concatenate them. Run the following shell commands. The first one will make an empty file named Snakefile , which will later contain the workflow. The second and third commands generate two files containing some arbitrary text. touch Snakefile echo \"This is a.txt\" > a.txt echo \"This is b.txt\" > b.txt Then open Snakefile in your favorite text editor. A Snakemake workflow is based on rules which take some file(s) as input, performs some type of operation on them, and generate some file(s) as outputs. Here is a very simple rule that produces a.upper.txt as an output, using a.txt as input. Copy this rule to your Snakefile and save it. rule convert_to_upper_case: output: \"a.upper.txt\" input: \"a.txt\" shell: \"\"\" tr [a-z] [A-Z] < {input} > {output} \"\"\" Attention! Indentation is important in Snakefiles, so make sure that you have the correct number of spaces before input / output / shell and their respective subsections. The number of spaces per level doesn't matter as long as you're consistent. Here we use four, but you could just as well use two for a more compact look. Don't use tabs (unless your editor automatically converts them to spaces). Rules can be given names, here it's convert_to_upper_case . While rule names are not strictly necessary we encourage you to use them and to make an effort to name your rules in a way that makes it easy to understand the purpose of the rule, as rule names are one of the main ways to interact with the workflow. The shell section (or directive) contains the shell commands that will convert the text in the input file to upper case and send it to the output file. In the shell command string, we can refer to elements of the rule via curly brackets. Here, we refer to the output file by specifying {output} and to the input file by specifying {input} . If you're not very familiar with Bash, this particular command can be read like \"send the contents of a.txt to the program tr , which will convert all characters in the set [a-z] to the corresponding character in the set [A-Z] , and then send the output to a.upper.txt \". Now let's run our first Snakemake workflow. When a workflow is executed Snakemake tries to generate a set of target files. Target files can be specified via the command line (or, as you will see later, in several other ways). Here we ask Snakemake to make the file a.upper.txt . It's good practice to first run with the flag -n (or --dry-run ), which will show what Snakemake plans to do without actually running anything, and you also need to specify how many cores to be used for the workflow with --cores or -c . For now, you only need 1 so set -c 1 . You can also use the flag -p , for showing the shell commands that it will execute, and the flag -r for showing the reason for running a specific rule. snakemake --help will show you all available flags. $ snakemake -n -c 1 -r -p a.upper.txt Building DAG of jobs... Job stats: job count min threads max threads --------------------- ------- ------------- ------------- convert_to_upper_case 1 1 1 total 1 1 1 [Mon Oct 25 16:48:43 2021] rule convert_to_upper_case: input: a.txt output: a.upper.txt jobid: 0 reason: Missing output files: a.upper.txt resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T tr [a-z] [A-Z] < a.txt > a.upper.txt Job stats: job count min threads max threads --------------------- ------- ------------- ------------- convert_to_upper_case 1 1 1 total 1 1 1 This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. You can see that Snakemake plans to run one job: the rule convert_to_upper_case with a.txt as input and a.upper.txt as output. The reason for doing this is that it's missing the file a.upper.txt . Now execute the workflow without the -n flag and check that the contents of a.upper.txt is as expected. Then try running the same command again. What do you see? It turns out that Snakemake only reruns jobs if there have been changes to either the input files, or the workflow itself . This is how Snakemake ensures that everything in the workflow is up to date. We will get back to this shortly. What if we ask Snakemake to generate the file b.upper.txt ? $ snakemake -n -c 1 -r -p b.upper.txt Building DAG of jobs... MissingRuleException: No rule to produce b.upper.txt (if you use input functions make sure that they don't raise unexpected exceptions). That didn't work well. We could copy the rule to make a similar one for b.txt , but that would be a bit cumbersome. Here is where named wildcards come in; one of the most powerful features of Snakemake. Simply change the input from input: \"a.txt\" to input: \"{some_name}.txt\" and the output to output: \"{some_name}.upper.txt\" . Now try asking for b.upper.txt again. Tada! What happens here is that Snakemake looks at all the rules it has available (actually only one in this case) and tries to assign values to all wildcards so that the targeted files can be generated. In this case it was quite simple, you can see that it says that wildcards: some_name=b , but for large workflows and multiple wildcards it can get much more complex. Named wildcards is what enables a workflow (or single rules) to be efficiently generalized and reused between projects or shared between people. It seems we have the first part of our workflow working, now it's time to make the second rule for concatenating the outputs from convert_to_upper_case . The rule structure will be similar; the only difference is that here we have two inputs instead of one. This can be expressed in two ways, either with named inputs like this: input: firstFile=\"...\", secondFile=\"...\" shell: \"\"\" some_function {input.firstFile} {input.secondFile} \"\"\" Or with indexes like this: input: \"...\", \"...\" shell: \"\"\" some_function {input[0]} {input[1]} \"\"\" Attention! If you have multiple inputs or outputs they need to be delimited with a comma (as seen above). This is a very common mistake when writing Snakemake workflows. The parser will complain, but sometimes the error message can be difficult to interpret. Now try to construct this rule yourself and name it concatenate_a_and_b . The syntax for concatenating two files in Bash is cat first_file.txt second_file.txt > output_file.txt . Call the output c.txt . Run the workflow in Snakemake and validate that the output looks as expected. Wouldn't it be nice if our workflow could be used for any files, not just a.txt and b.txt ? We can achieve this by using named wildcards (or in other ways as we will discuss later). As we've mentioned, Snakemake looks at all the rules it has available and tries to assign values to all wildcards so that the targeted files can be generated. We therefore have to name the output file in a way so that it also contains information about which input files it should be based on. Try to figure out how to do this yourself. If you're stuck you can look at the spoiler below, but spend some time on it before you look. Also rename the rule to concatenate_files to reflect its new more general use. Click to show rule concatenate_files: output: \"{first}_{second}.txt\" input: \"{first}.upper.txt\", \"{second}.upper.txt\" shell: \"\"\" cat {input[0]} {input[1]} > {output} \"\"\" We can now control which input files to use by the name of the file we ask Snakemake to generate. Run the workflow without the flag -n (or --dry-run ) to execute both rules, providing one core with -c 1 (or --cores 1 ): $ snakemake a_b.txt -c 1 Building DAG of jobs... Using shell: /bin/bash Provided cores: 1 (use --cores to define parallelism) Rules claiming more threads will be scaled down. Job stats: job count min threads max threads --------------------- ------- ------------- ------------- concatenate_files 1 1 1 convert_to_upper_case 2 1 1 total 3 1 1 Select jobs to execute... [Mon Oct 25 16:51:52 2021] rule convert_to_upper_case: input: b.txt output: b.upper.txt jobid: 2 wildcards: some_name=b resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T [Mon Oct 25 16:51:53 2021] Finished job 2. 1 of 3 steps (33%) done Select jobs to execute... [Mon Oct 25 16:51:53 2021] rule convert_to_upper_case: input: a.txt output: a.upper.txt jobid: 1 wildcards: some_name=a resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T [Mon Oct 25 16:51:53 2021] Finished job 1. 2 of 3 steps (67%) done Select jobs to execute... [Mon Oct 25 16:51:53 2021] rule concatenate_files: input: a.upper.txt, b.upper.txt output: a_b.txt jobid: 0 wildcards: first=a, second=b resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T [Mon Oct 25 16:51:53 2021] Finished job 0. 3 of 3 steps (100%) done Neat! Tip You can name a file whatever you want in a Snakemake workflow, but you will find that everything falls into place much nicer if the filename reflects the file's path through the workflow, e.g. sample_a.trimmed.deduplicated.sorted.bam . Quick recap In this section we've learned: How a simple Snakemake rule looks. How to define target files when executing a workflow. How to use named wildcards for writing generic and flexible rules.","title":"The basics"},{"location":"snakemake/snakemake-3-visualising-workflows/","text":"All that we've done so far could quite easily be done in a simple shell script that takes the input files as parameters. Let's now take a look at some of the features where a WfMS like Snakemake really adds value compared to a more straightforward approach. One such feature is the possibility to visualize your workflow. Snakemake can generate three types of graphs, one that shows how the rules are connected, one that shows how the jobs ( i.e. an execution of a rule with some given inputs/outputs/settings) are connected, and finally one that shows rules with their respective input/output files. First we look at the rule graph. The following command will generate a rule graph in the dot language and pipe it to the program dot , which in turn will save a visualization of the graph as a PNG file (if you're having troubles displaying PNG files you could use SVG or JPG instead). snakemake --rulegraph a_b.txt | dot -Tpng > rulegraph.png This looks simple enough, the output from the rule convert_to_upper_case will be used as input to the rule concatenate_files . For a more typical bioinformatics project it can look something like this when you include all the rules from processing of the raw data to generating figures for the paper. While saying that it's easy to read might be a bit of a stretch, it definitely gives you a better overview of the project than you would have without a WfMS. The second type of graph is based on the jobs, and looks like this for our little workflow (use --dag instead of --rulegraph ). snakemake --dag a_b.txt | dot -Tpng > jobgraph.png The main difference here is that now each node is a job instead of a rule. You can see that the wildcards used in each job are also displayed. Another difference is the dotted lines around the nodes. A dotted line is Snakemake's way of indicating that this rule doesn't need to be rerun in order to generate a_b.txt . Validate this by running snakemake -n -r a_b.txt and it should say that there is nothing to be done. We've discussed before that one of the main purposes of using a WfMS is that it automatically makes sure that everything is up to date. This is done by recursively checking that outputs are always newer than inputs for all the rules involved in the generation of your target files. Now try to change the contents of a.txt to some other text and save it. What do you think will happen if you run snakemake -n -r a_b.txt again? Click to show $ snakemake -n -r a_b.txt Building DAG of jobs... Job stats: job count min threads max threads --------------------- ------- ------------- ------------- concatenate_files 1 1 1 convert_to_upper_case 1 1 1 total 2 1 1 [Mon Oct 25 17:00:02 2021] rule convert_to_upper_case: input: a.txt output: a.upper.txt jobid: 1 reason: Updated input files: a.txt wildcards: some_name=a resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T [Mon Oct 25 17:00:02 2021] rule concatenate_files: input: a.upper.txt, b.upper.txt output: a_b.txt jobid: 0 reason: Input files updated by another job: a.upper.txt wildcards: first=a, second=b resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T Job stats: job count min threads max threads --------------------- ------- ------------- ------------- concatenate_files 1 1 1 convert_to_upper_case 1 1 1 total 2 1 1 This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. Were you correct? Also generate the job graph and compare to the one generated above. What's the difference? Now rerun without -n and validate that a_b.txt contains the new text (don't forget to specify -c 1 ). Note that Snakemake doesn't look at the contents of files when trying to determine what has changed, only at the timestamp for when they were last modified. We've seen that Snakemake keeps track of if files in the workflow have changed, and automatically makes sure that any results depending on such files are regenerated. What about if the rules themselves are changed? It turns out that since version 7.8.0 Snakemake keeps track of this automatically. Let's say that we want to modify the rule concatenate_files to also include which files were concatenated. rule concatenate_files: output: \"{first}_{second}.txt\" input: \"{first}.upper.txt\", \"{second}.upper.txt\" shell: \"\"\" echo 'Concatenating {input}' | cat - {input[0]} {input[1]} > {output} \"\"\" Note It's not really important for the tutorial, but the shell command used here first outputs \"Concatenating \" followed by a space delimited list of the files in input . This string is then sent to the program cat where it's concatenated with input[0] and input[1] (the parameter - means that it should read from standard input). Lastly, the output from cat is sent to {output} . If you now run the workflow as before you should see: rule concatenate_files: input: a.upper.txt, b.upper.txt output: a_b.txt jobid: 0 reason: Code has changed since last execution wildcards: first=a, second=b because although no files involved in the workflow have been changed, Snakemake recognizes that the workflow code itself has been modified and this triggers a re-run. Snakemake is aware of changes to four categories of such \"rerun-triggers\": \"input\" (changes to rule input files), \"params\" (changes to the rule params section), \"software-env\" (changes to conda environment files specified by the conda: directive) and \"code\" (changes to code in the shell: , run: , script: and notebook: directives). Prior to version 7.8.0, only changes to the modification time of input files would trigger automatic re-runs. To run Snakemake with this previous behaviour you can use the setting --rerun-triggers mtime at the command line. Change the shell: section of the concatenate_files rule back to the previous version, then try running: snakemake -n -r a_b.txt --rerun-triggers mtime and you should again see Nothing to be done (all requested files are present and up to date). You can also export information on how all files were generated (when, by which rule, which version of the rule, and by which commands) to a tab-delimited file like this: snakemake a_b.txt -c 1 -D > summary.tsv The content of summary.tsv is shown in the table below: output_file date rule version log-file(s) input-file(s) shellcmd status plan a_b.txt Mon Oct 25 17:01:46 2021 concatenate_files - a.upper.txt,b.upper.txt cat a.upper.txt b.upper.txt > a_b.txt rule implementation changed update pending a.upper.txt Mon Oct 25 17:01:46 2021 convert_to_upper_case - a.txt tr [a-z] [A-Z] < a.txt > a.upper.txt ok no update b.upper.txt Mon Oct 25 17:01:46 2021 convert_to_upper_case - b.txt tr [a-z] [A-Z] < b.txt > b.upper.txt ok no update You can see in the second last column that the rule implementation for a_b.txt has changed. The last column shows if Snakemake plans to regenerate the files when it's next executed. You can see that for the concatenate_files the plan is update pending because we generated the summary with the default behaviour of using all rerun-triggers. You might wonder where Snakemake keeps track of all these things? It stores all information in a hidden subdirectory called .snakemake . This is convenient since it's easy to delete if you don't need it anymore and everything is contained in the project directory. Just be sure to add it to .gitignore so that you don't end up tracking it with git. By now you should be familiar with the basic functionality of Snakemake, and you can build advanced workflows with only the features we have discussed here. There's a lot we haven't covered though, in particular when it comes to making your workflow more reusable. In the following section we will start with a workflow that is fully functional but not very flexible. We will then gradually improve it, and at the same time showcase some Snakemake features we haven't discussed yet. Note that this can get a little complex at times, so if you felt that this section was a struggle then you could move on to one of the other tutorials instead. Quick recap In this section we've learned: How to use --dag and --rulegraph for visualizing the job and rule graphs, respectively. How Snakemake reruns relevant parts of the workflow after there have been changes. How Snakemake tracks changes to files and code in a workflow","title":"Visualising workflow"},{"location":"snakemake/snakemake-4-the-mrsa-workflow/","text":"As you might remember from the intro , we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus ). In order to do this we have performed RNA-seq of three strains, one test and two controls. We have already set up a draft Snakemake workflow for the RNA-seq analysis and it seems to be running nicely. It's now up to you to modify this workflow to make it more flexible and reproducible! Tip This section will leave a little more up to you compared to the previous one. If you get stuck at some point the final workflow after all the modifications is available in tutorials/git/Snakefile . You are probably already in your snakemake-env environment, otherwise activate it (use conda info --envs if you are unsure). Tip Here we have one Conda environment for executing the whole Snakemake workflow. Snakemake also supports using explicit Conda environments on a per-rule basis, by specifying something like conda: rule-specific-env.yml in the rule definition and running Snakemake with the --use-conda flag. The given rule will then be run in the Conda environment specified in rule-specific-env.yml that will be created and activated on the fly by Snakemake. Let's start by generating the rule graph so that we get an overview of the workflow. snakemake -s snakefile_mrsa.smk --rulegraph | dot -T png > rulegraph_mrsa.png There are two differences in this command compared to the one we've used before. The first is that we're using the -s flag to specify which Snakemake workflow to run. We didn't need to do that before since Snakefile is the default name. The second is that we don't define a target. In the toy example we used a_b.txt as a target, and the wildcards were resolved based on that. How come that we don't need to do that here? It turns out that by default Snakemake targets the first rule in a workflow. By convention, we call this rule all and let it serve as a rule for aggregating the main outputs of the workflow. Now take some time and look through the workflow file and try to understand how the rules fit together. Use the rule graph as aid. The rules represent a quite standard, although somewhat simplified, workflow for RNA-seq analysis. If you are unfamiliar with the purpose of the different operations (index genome, FastQC and so on), then take a look at the intro . Also generate the job graph in the same manner. Here you can see that three samples will be downloaded from SRA (Sequence Read Archive); SRR935090, SRR935091, and SRR935092. Those will then be quality controlled with FastQC and aligned to a genome. The QC output will be aggregated with MultiQC and the alignments will be used to generate a count table, i.e. a table that shows how many reads map to each gene for each sample. This count table is then what the downstream analysis will be based on. Now try to run the whole workflow. Hopefully you see something like this. Building DAG of jobs... Using shell: /bin/bash Provided cores: 1 (use --cores to define parallelism) Rules claiming more threads will be scaled down. Job stats: job count min threads max threads -------------------- ------- ------------- ------------- align_to_genome 3 1 1 all 1 1 1 fastqc 3 1 1 generate_count_table 1 1 1 generate_rulegraph 1 1 1 get_SRA_by_accession 3 1 1 get_genome_fasta 1 1 1 get_genome_gff3 1 1 1 index_genome 1 1 1 multiqc 1 1 1 sort_bam 3 1 1 total 19 1 1 Select jobs to execute... [Mon Oct 25 17:13:47 2021] rule get_genome_fasta: output: data/raw_external/NCTC8325.fa.gz jobid: 6 resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T --2021-10-25 17:13:48-- ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz => \u2018data/raw_external/NCTC8325.fa.gz\u2019 Resolving ftp.ensemblgenomes.org (ftp.ensemblgenomes.org)... 193.62.197.75 Connecting to ftp.ensemblgenomes.org (ftp.ensemblgenomes.org)|193.62.197.75|:21... connected. Logging in as anonymous ... Logged in! ==> SYST ... done. ==> PWD ... done. . . [lots of stuff] . . localrule all: input: results/tables/counts.tsv, results/multiqc.html, results/rulegraph.png jobid: 0 resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T [Mon Oct 25 17:14:38 2021] Finished job 0. 19 of 19 steps (100%) done After everything is done, the workflow will have resulted in a bunch of files in the directories data , intermediate and results . Take some time to look through the structure, in particular the quality control reports in results and the count table in results/tables . Quick recap In this section we've learned: How the MRSA workflow looks. How to run the MRSA workflow. Which output files the MRSA workflow produces.","title":"The MRSA Workflow"},{"location":"snakemake/snakemake-5-parameters/","text":"In a typical bioinformatics project, considerable efforts are spent on tweaking parameters for the various programs involved. It would be inconvenient if you had to change in the shell scripts themselves every time you wanted to run with a new setting. Luckily, there is a better option for this: the params keyword. rule some_rule: output: \"...\" input: \"...\" params: cutoff=2.5 shell: \"\"\" some_program --cutoff {params.cutoff} {input} {output} \"\"\" We run most of the programs with default settings in our workflow. However, there is one parameter in the rule get_SRA_by_accession that we use for determining how many reads we want to retrieve from SRA for each sample ( -X 25000 ). Change in this rule to use the parameter max_reads instead and set the value to 20000. If you need help, click to show the solution below. Click to show rule get_SRA_by_accession: \"\"\" Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run accession number. \"\"\" output: \"data/raw_internal/{sample_id}.fastq.gz\" params: max_reads = 20000 shell: \"\"\" fastq-dump {wildcards.sample_id} -X {params.max_reads} --readids \\ --dumpbase --skip-technical --gzip -Z > {output} \"\"\" Now run through the workflow. Because there's been changes to the get_SRA_by_accession rule this will trigger a re-run of the rule for all three accessions. In addition all downstream rules that depend on output from get_SRA_by_accession are re-run. The parameter values we set in the params section don't have to be static, they can be any Python expression. In particular, Snakemake provides a global dictionary of configuration parameters called config . Let's modify get_SRA_by_accession to look something like this in order to make use of this dictionary: rule get_SRA_by_accession: \"\"\" Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run accession number. \"\"\" output: \"data/raw_internal/{sample_id}.fastq.gz\" params: max_reads = config[\"max_reads\"] shell: \"\"\" fastq-dump {wildcards.sample_id} -X {params.max_reads} --readids \\ --dumpbase --skip-technical --gzip -Z > {output} \"\"\" Note that Snakemake now expects there to be a key named max_reads in the config dictionary. If we don't populate the dictionary somehow the dictionary will be empty so if you were to run the workflow now it would trigger a KeyError (try running snakemake -s snakefile_mrsa.smk -n to see for yourself). In order to populate the config dictionary with data for the workflow we could use the snakemake --config KEY=VALUE syntax directly from the command line. However, from a reproducibility perspective, it's not optimal to set parameters from the command line, since it's difficult to keep track of which parameter values that were used. A much better alternative is to use the --configfile FILE option to supply a configuration file to Snakemake. In this file we can collect all the project-specific settings, sample ids and so on. This also enables us to write the Snakefile in a more general manner so that it can be better reused between projects. Like several other files used in these tutorials, this file should be in yaml format . Create the file below and save it as config.yml . max_reads: 25000 If we now run Snakemake with --configfile config.yml , it will parse this file to form the config dictionary. If you want to overwrite a parameter value, e.g. for testing, you can still use the --config KEY=VALUE flag, as in --config max_reads=1000 . Tip Rather than supplying the config file from the command line you could also add the line configfile: \"config.yml\" to the top of your Snakefile. Keep in mind that with such a setup Snakemake will complain if the file config.yml is not present. Quick recap In this section we've learned: How to set parameter values with the params directive. How to run Snakemake with the config variable and with a configuration file.","title":"Parameters"},{"location":"snakemake/snakemake-6-logs/","text":"As you probably noticed it was difficult to follow how the workflow progressed since some rules printed a lot of output to the terminal. In some cases this also contained important information, such as statistics on the sequence alignments or genome indexing. This could be valuable for example if you later in the project get weird results and want to debug. It's also important from a reproducibility perspective that the \"paper trail\" describing how the outputs were generated is saved. Luckily, Snakemake has a feature that can help with this. Just as we define input and output in a rule we can also define log . rule some_rule: output: \"...\" input: \"...\" log: \"...\" shell: \"\"\" echo 'Converting {input} to {output}' > {log} \"\"\" A log file is not different from any other output file, but it's dealt with a little differently by Snakemake. For example, it's shown in the file summary when using -D . It's also a good way to clarify the purpose of the file. We probably don't need to save logs for all the rules, only the ones with interesting output. get_genome_fasta and get_genome_gff3 would be good to log since they are dependent on downloading files from an external server. multiqc aggregates quality control data for all the samples into one html report, and the log contains information about which samples were aggregated. index_genome outputs some statistics about the genome indexing. align_to_genome outputs important statistics about the alignments. This is probably the most important log to save. Now add a log file to some or all of the rules above. A good place to save them to would be results/logs/rule_name/ . In order to avoid that multiple jobs write to the same files Snakemake requires that all output and log files contain the same wildcards, so be sure to include any wildcards used in the rule in the log name as well, e.g. {some_wildcard}.log . You also have to specify in the shell section of each rule what you want the log to contain. Some of the programs we use send their log information to standard out, some to standard error and some let us specify a log file via a flag. For example, in the align_to_genome rule, it could look like this (bowtie2 writes log info to standard error): rule align_to_genome: \"\"\" Align a fastq file to a genome index using Bowtie 2. \"\"\" output: \"intermediate/{sample_id,\\w+}.bam\" input: \"data/raw_internal/{sample_id}.fastq.gz\", \"intermediate/NCTC8325.1.bt2\", \"intermediate/NCTC8325.2.bt2\", \"intermediate/NCTC8325.3.bt2\", \"intermediate/NCTC8325.4.bt2\", \"intermediate/NCTC8325.rev.1.bt2\", \"intermediate/NCTC8325.rev.2.bt2\" log: \"results/logs/align_to_genome/{sample_id}.log\" shell: \"\"\" bowtie2 -x intermediate/NCTC8325 -U {input[0]} > {output} 2>{log} \"\"\" To save some time you can use the info below. # Wget has a -o flag for specifying the log file wget remote_file -O output_file -o {log} # MultiQC writes to standard error so we redirect with \"2>\" multiqc -n output_file input_files 2> {log} # Bowtie2-build redirects to standard out so we use \">\" bowtie2-build input_file index_dir > {log} Now rerun the whole workflow. Do the logs contain what they should? Note how much easier it is to follow the progression of the workflow when the rules write to logs instead of to the terminal. Tip If you have a rule with a shell directive in which several commands are run and you want to save stdout and stderr for all commands into the same log file you can add exec &>{log} as the first line of the shell directive. If you run with -D (or -S for a simpler version) you will see that the summary table now also contains the log file for each of the files in the workflow. Quick recap In this section we've learned: How to redirect output to log files with the log directive.","title":"Logs"},{"location":"snakemake/snakemake-7-temporary-files/","text":"It's not uncommon that workflows contain temporary files that should be kept for some time and then deleted once they are no longer needed. A typical case could be that some operation generates a file, which is then compressed to save space or indexed to make searching faster. There is then no need to save the original output file. Take a look at the job graph for our workflow again. The output from align_to_genome is a bam file, which contains information about all the reads for a sample and where they map in the genome. For downstream processing we need this file to be sorted by genome coordinates. This is what the rule sort_bam is for. We therefore end up with both intermediate/{sample_id}.bam and intermediate/{sample_id}.sorted.bam . In Snakemake we can mark an output file as temporary like this: output: temp(\"...\") The file will then be deleted as soon as all jobs where it's an input have finished. Now do this for the output of align_to_genome . We have to rerun the rule for it to trigger, so use -R align_to_genome . It should look something like this: . . rule sort_bam: input: intermediate/SRR935090.bam output: intermediate/SRR935090.sorted.bam jobid: 2 wildcards: sample_id=SRR935090 Removing temporary output file intermediate/SRR935090.bam. Finished job 2. . . Tip Sometimes you may want to trigger removal of temporary files without actually rerunning the jobs. You can then use the --delete-temp-output flag. In some cases you may instead want to run only parts of a workflow and therefore want to prevent files marked as temporary from being deleted (because the files are needed for other parts of the workflow). In such cases you can use the --notemp flag. Snakemake has a number of options for marking files: temp(\"...\") : The output file should be deleted once it's no longer needed by any rules. protected(\"...\") : The output file should be write-protected. Typically used to protect files that require a huge amount of computational resources from being accidentally deleted. ancient(\"...\") : The timestamp of the input file is ignored and it's always assumed to be older than any of the output files. touch(\"...\") : The output file should be \"touched\", i.e. created or updated, when the rule has finished. Typically used as \"flag files\" to enforce some rule execution order without real file dependencies. directory(\"...\") : The output is a directory rather than a file. Quick recap In this section we've learned: How to mark an output file as temporary for automatic removal.","title":"Temporary files"},{"location":"snakemake/snakemake-8-targets/","text":"So far we have only defined the inputs/outputs of a rule as strings, or in some case a list of strings, but Snakemake allows us to be much more flexible than that. Actually, we can use any Python expression or even functions, as long as they return a string or list of strings. Consider the rule align_to_genome below. rule align_to_genome: \"\"\" Align a fastq file to a genome index using Bowtie 2. \"\"\" output: \"intermediate/{sample_id,\\w+}.bam\" input: \"data/raw_internal/{sample_id}.fastq.gz\", \"intermediate/NCTC8325.1.bt2\", \"intermediate/NCTC8325.2.bt2\", \"intermediate/NCTC8325.3.bt2\", \"intermediate/NCTC8325.4.bt2\", \"intermediate/NCTC8325.rev.1.bt2\", \"intermediate/NCTC8325.rev.2.bt2\" shell: \"\"\" bowtie2 -x intermediate/NCTC8325 -U {input[0]} > {output} \"\"\" Here we have seven inputs; the fastq file with the reads and six files with similar file names from the Bowtie 2 genome indexing. We can try to tidy this up by using a Python expression to generate a list of these files instead. If you're familiar with Python you could do this with list comprehensions like this: input: fastq = \"data/raw_internal/{sample_id}.fastq.gz\", index = [f\"intermediate/NCTC8325.{substr}.bt2\" for substr in [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"]] This will take the elements of the list of substrings one by one, and insert that element in the place of {substr} . Since this type of aggregating rules are quite common, Snakemake also has a more compact way of achieving the same thing. input: fastq = \"data/raw_internal/{sample_id}.fastq.gz\", index = expand(\"intermediate/NCTC8325.{substr}.bt2\", substr = [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"]) Now change in the rules index_genome and align_to_genome to use the expand() expression. In the workflow we decide which samples to run by including the SRR ids in the names of the inputs to the rules multiqc and generate_count_table . This is a potential source of errors since it's easy to change in one place and forget to change in the other. As we've mentioned before, but not really used so far, Snakemake allows us to use Python code \"everywhere\". Let's therefore define a list of sample ids and put at the very top of the Snakefile, just before the rule all . SAMPLES = [\"SRR935090\", \"SRR935091\", \"SRR935092\"] Now use expand() in multiqc and generate_count_table to use SAMPLES for the sample ids. For the multiqc rule it could look like this: input: expand(\"intermediate/{sample_id}_fastqc.zip\", sample_id = SAMPLES) See if you can update the generate_count_table rule in the same manner! Quick recap In this section we've learned: How to use the expand() expression to create a list with file names, inserting all provided wildcard values.","title":"Targets"},{"location":"snakemake/snakemake-9-shadow-rules/","text":"Take a look at the index_genome rule below: rule index_genome: \"\"\" Index a genome using Bowtie 2. \"\"\" output: index = expand(\"intermediate/NCTC8325.{substr}.bt2\", substr = [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"]) input: \"data/raw_external/NCTC8325.fa.gz\" log: \"results/logs/index_genome/NCTC8325.log\" shell: \"\"\" # Bowtie2 cannot use .gz, so unzip to a temporary file first gunzip -c {input} > tempfile bowtie2-build tempfile intermediate/NCTC8325 >{log} # Remove the temporary file rm tempfile \"\"\" There is a temporary file here called tempfile which is the uncompressed version of the input, since Bowtie 2 cannot use compressed files. There are a number of drawbacks with having files that aren't explicitly part of the workflow as input/output files to rules: Snakemake cannot clean up these files if the job fails, as it would do for normal output files. If several jobs are run in parallel there is a risk that they write to tempfile at the same time. This can lead to very scary results. Sometimes we don't know the names of all the files that a program can generate. It is, for example, not unusual that programs leave some kind of error log behind if something goes wrong. All of these issues can be dealt with by using the shadow option for a rule. The shadow option results in that each execution of the rule is run in an isolated temporary directory (located in .snakemake/shadow/ by default). There are a few options for shadow (for the full list of these options see the Snakemake docs ). The most simple is shadow: \"minimal\" , which means that the rule is executed in an empty directory that the input files to the rule have been symlinked into. For the rule below, that means that the only file available would be input.txt . The shell commands would generate the files some_other_junk_file and output.txt . Lastly, Snakemake will move the output file ( output.txt ) to its \"real\" location and remove the whole shadow directory. We therefore never have to think about manually removing some_other_junk_file . rule some_rule: output: \"output.txt\" input: \"input.txt\" shadow: \"minimal\" shell: \"\"\" touch some_other_junk_file cp {input} {output} \"\"\" Try this out for the rules where we have to \"manually\" deal with files that aren't tracked by Snakemake ( multiqc , index_genome ). Also remove the shell commands that remove temporary files from those rules, as they are no longer needed. Now rerun the workflow and validate that the temporary files don't show up in your working directory. Tip Some people use the shadow option for almost every rule and some never use it at all. One thing to keep in mind is that it leads to some extra file operations when the outputs are moved to their final location. This is no issue when the shadow directory is on the same disk as the output directory, but if you're running on a distributed file system and generate very many or very large files it might be worth considering other options (see e.g. the --shadow-prefix flag). Quick recap In this section we've learned: How to use the shadow option to handle files that are not tracked by Snakemake.","title":"Shadow rules"}]}